<languages />

= Introduction =

To request one or more GPUs for a [[Running jobs|Slurm]] job, use this form:
  --gpus-per-node=<model_specifier>:<number>

For example:
  --gpus-per-node=a100:1

This requests a single A100 GPU (unless you also use <code>--nodes</code> to specify more than a single node).
See the following section, <i>Available GPUs,</i> for valid model specifiers.

The following form can also be used:
  --gres=gpu:<model_specifier>:<number>
This form may not be supported in the future.  We recommend that you replace it in your scripts with <code>--gpus-per-node</code>.

Slurm supports a variety of other directives that you can use to request GPU resources: <code>--gpus</code>, <code>--gpus-per-socket</code>, <code>--gpus-per-task</code>, <code>--mem-per-gpu</code>, and <code>--ntasks-per-gpu</code>.  Please see the Slurm documentation for [https://slurm.schedmd.com/sbatch.html sbatch] for more about these.  Our staff do not test all of these; if you try one but don't get the result you expect, [[Technical support|contact technical support]].

For general advice on job scheduling, see [[Running jobs]].

= Available GPUs =
The following table summarizes the available GPU models and their corresponding specifiers:

{| class="wikitable"
|-
! Cluster !! GPU model !! Model specifiers<br>for Slurm !! Notes
|- 
| rowspan=4|[[Fir#Node_characteristics|Fir]] || rowspan=4|H100-80gb || h100 || 
|- 
|   nvidia_h100_80gb_hbm3_1g.10gb || MIG
|- 
|   nvidia_h100_80gb_hbm3_2g.20gb || MIG
|- 
|   nvidia_h100_80gb_hbm3_3g.40gb || MIG
|-
| rowspan=5|[[Narval/en#Node_characteristics|Narval]] || rowspan=5|A100-40gb || a100 || 
|-
|  a100_1g.5gb  || MIG 
|-
|  a100_2g.10gb || MIG 
|-
|  a100_3g.20gb || MIG 
|-
|  a100_4g.20gb || MIG 
|- 
| rowspan=5|[[Nibi#Node_characteristics|Nibi]] || rowspan=4|H100-80gb || h100 || 
|-
|   nvidia_h100_80gb_hbm3_1g.10gb || MIG
|- 
|   nvidia_h100_80gb_hbm3_2g.20gb || MIG
|- 
|   nvidia_h100_80gb_hbm3_3g.40gb || MIG
|-
| MI300A-128gb  || (none; see [[Nibi#Node_characteristics|Nibi]]) || 
|- 
| rowspan=4|[[Rorqual/en#Node_characteristics|Rorqual]] || rowspan=4|H100-80gb || h100 || 
|-
|   nvidia_h100_80gb_hbm3_1g.10gb || MIG; synonyms h100_1g.10gb, h100_1.10, h100_10gb
|- 
|   nvidia_h100_80gb_hbm3_2g.20gb || MIG; synonyms h100_2g.20gb, h100_2.20, h100_20gb
|- 
|   nvidia_h100_80gb_hbm3_3g.40gb || MIG; synonyms h100_3g.40gb, h100_3.40, h100_40gb
|-
| [[Trillium#Node_characteristics|Trillium]] || H100-80gb || h100 || 
|-
| rowspan=2|[[Killarney#Killarney_hardware_specifications|Killarney]] || H100-80gb || h100 || &nbsp; 
|-
|  L40S-48gb || l40s || &nbsp; 
|-
| rowspan=2|[[TamIA/en#Node_characteristics|tamIA]] || H100-80gb || h100 || &nbsp; 
|-
|  H200 || h200 || &nbsp; 
|-
| rowspan=2|[[Vulcan#Vulcan_hardware_specifications|Vulcan]] || L40S-48gb || l40s || &nbsp; 
|}

GPU model specifiers (including MIG specifiers) available on any given cluster can be obtained from Slurm with the following command.
This may be useful if the table above has not been updated with the latest changes.

{{Command|sinfo -o "%G"{{!}}grep gpu{{!}}sed 's/gpu://g'{{!}}sed 's/),/\n/g'{{!}}cut -d: -f1{{!}}sort{{!}}uniq}}

There are short synonyms available for some of the MIG specifiers at certain sites; this command will not provide those synonyms.
Also, the presence of a GPU model does not guarantee that you will be able to use one of the corresponding specifiers in your jobs; there may be 
further restrictions on what model specifiers are available based on (for example) which research group you belong.
For further information see the site-specific page by clicking on the cluster name in the above table, or [[Technical support|contact support]].

If you do not supply a model specifier your job may be rejected or it may be sent to an arbitrary GPU instance.
There are very few programs which can use an arbitrary GPU efficiently,
so we strongly recommend that you always provide a specific GPU model specifier in your job scripts. 

There are GPUs available at Arbutus, but like other cloud resources they cannot be scheduled via Slurm.
See [[Cloud resources]] for more details.

== Multi-Instance GPUs (MIGs) ==
MIG is a technology that partitions a GPU into multiple instances.
Your jobs might be able to use a MIG instance instead of a whole GPU.
Please see [[Multi-Instance_GPU]] for more about this.

= Requesting CPU cores and system memory =

Along with each GPU instance, your job should have a number of CPU cores (default is <code>1</code>) and some amount of system memory. The recommended maximum numbers of CPU cores and gigabytes of system memory per GPU instance are listed in the [[Allocations_and_compute_scheduling#Ratios_in_bundles|table of bundle characteristics]].

= Examples =

== Single-core job ==
If you need only a single CPU core and one GPU:
{{File
  |name=gpu_serial_job.sh
  |lang="sh"
  |contents=
#!/bin/bash
#SBATCH --account=def-someuser
#SBATCH --gpus-per-node=a100:1
#SBATCH --mem=4000M               # memory per node
#SBATCH --time=0-03:00
./program                         # you can use 'nvidia-smi' for a test
}}

== Multi-threaded job ==
For a GPU job which needs multiple CPUs in a single node:
{{File
  |name=gpu_threaded_job.sh
  |lang="sh"
  |contents=
#!/bin/bash
#SBATCH --account=def-someuser
#SBATCH --gpus-per-node=a100:1 
#SBATCH --cpus-per-task=6         # CPU cores or threads
#SBATCH --mem=4000M               # memory per node
#SBATCH --time=0-03:00
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
./program
}}

For each GPU requested, we recommend
* on Fir, no more than 12 CPU cores;
* on Narval, no more than 12 CPU cores
* on Nibi, no more than 14 CPU cores, 
* on Rorqual, no more than 16 CPU cores

== MPI job ==
{{File
  |name=gpu_mpi_job.sh
  |lang="sh"
  |contents=
#!/bin/bash
#SBATCH --account=def-someuser
#SBATCH --gpus=a100:8             # total number of GPUs
#SBATCH --ntasks-per-gpu=1        # total of 8 MPI processes
#SBATCH --cpus-per-task=6         # CPU cores per MPI process
#SBATCH --mem-per-cpu=5G          # host memory per CPU core
#SBATCH --time=0-03:00            # time (DD-HH:MM)
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
srun --cpus-per-task=$SLURM_CPUS_PER_TASK ./program
}}

== Whole nodes ==
If your application can efficiently use an entire node and its associated GPUs, you will probably experience shorter wait times if you ask Slurm for a whole node. Use one of the following job scripts as a template. 

===Packing single-GPU jobs within one SLURM job===

If you need to run four single-GPU programs or two 2-GPU programs for longer than 24 hours, [[GNU Parallel]] is recommended. A simple example is:
<pre>
cat params.input | parallel -j4 'CUDA_VISIBLE_DEVICES=$(({%} - 1)) python {} &> {#}.out'
</pre>
In this example, the GPU ID is calculated by subtracting 1 from the slot ID {%} and {#} is the job ID, starting from 1.

A <code>params.input</code> file should include input parameters in each line, like this:
<pre>
code1.py
code2.py
code3.py
code4.py
...
</pre>
With this method, you can run multiple tasks in one submission. The <code>-j4</code> parameter means that GNU Parallel can run a maximum of four concurrent tasks, launching another as soon as one ends. CUDA_VISIBLE_DEVICES is used to ensure that two tasks do not try to use the same GPU at the same time.

== Profiling GPU tasks ==

On [[Narval/en|Narval]] and [[Rorqual/en|Rorqual]], profiling is possible but requires disabling the
[https://developer.nvidia.com/dcgm NVIDIA Data Center GPU Manager (DCGM)]. This must be done during job submission by setting the <code>DISABLE_DCGM</code> environment variable:

{{Command|DISABLE_DCGM{{=}}1 salloc --account{{=}}def-someuser --gpus-per-node{{=}}a100:1 --mem{{=}}4000M --time{{=}}03:00}}

Then, in your interactive job, wait until DCGM is disabled on the node: 
{{Command|while [ ! -z "$(dcgmi -v {{!}} grep 'Hostengine build info:')" ]; do  sleep 5; done}}

Finally, launch your profiler. For more details on profilers, see [[Debugging and profiling]].

On Fir and Nibi, GPU profiling like the above technique is not available yet.

= See also =
[[CUDA]]<br>
[[Multi-Instance GPU]]<br>
[[Running jobs]]

[[Category:SLURM]]