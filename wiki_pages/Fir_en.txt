<languages />

{| class="wikitable"
|-
| Availability date: <i>August 11, 2025</i>
|-
| Login node: <i>fir.alliancecan.ca</i>
|-
| Automation node: <i>robot.fir.alliancecan.ca</i>
|-
| Globus collection: <i>alliancecan#fir-globus</i>
|-
| Data transfer node (rsync, scp, sftp ...): <i>to be determined</i>
|-
| Portal: <i>to be determined</i>
|}

Fir is a versatile, heterogeneous computing cluster built in partnership with Lenovo Canada and Data Direct Networks (DDN) and is designed to support a wide range of scientific computations. It is hosted at Simon Fraser University (SFU) in Burnaby, British Columbia, and is named after the Red Creek Fir—the largest known Douglas fir tree on Earth by volume.

=About Fir=

SFU remains committed to environmentally sustainable high-performance computing. With Fir, the university is transitioning from traditional air cooling to advanced direct-to-chip liquid cooling, significantly improving energy efficiency and reducing power consumption associated with cooling.

The new high-speed InfiniBand network in Fir delivers more than twice the performance of the previous-generation Cedar cluster.

Fir is ranked #78 on the June 2025 [https://top500.org/lists/top500/list/2025/06/ TOP500 list] of the world’s most powerful supercomputers.

=Access=

Each researcher must request access in CCDB, via Resources--> Access Systems.

Select Fir from the list on the left.

Select I request access.

It can take up to one hour for your access to be enabled.

=Site-specific policies=

Fir's compute nodes have full access to the internet.

The crontab tool is not supported.

Each job should have a duration of at least one hour (at least five minutes for test jobs) and the maximum job duration is 7 days (168 hours). 

For transferring data via Globus, use the endpoint specified at the top of this page; for tools like rsync and scp, please use the login node.

=Storage=

51PB high-performance DDN Lustre storage (2PB NVME / 49 SAS).
{{Note|All mounts share the available storage|}}
{| class="wikitable"
! Storage Area !!  Access Path !! Quotas !! Backup !! Notes
|-
| '''HOME''' || Default <code>$HOME</code> || Small per-user quota || Daily automatic backup || Cannot be increased; use `/project</code> for larger storage
|-
| '''SCRATCH''' || <code>$HOME/scratch</code> || Large per-user quota || No backup || For temporary files; old files are purged automatically
|-
| '''PROJECT''' || <code>$HOME/project/${def-project-id}</code> || Large and adjustable per-project quota || Daily backup || For group data sharing and large datasets
|}

=High-performance interconnect=

* InfiniBand NDR interconnect
* CPU node island size, is 27:5 blocking factor over 216 nodes of 192 cores
* GPU nodes are 2:1 blocking factor
* Storage access is fully non-blocking

=Node characteristics=

{| class="wikitable sortable"
! nodes !! cores !! available memory !! CPU !! Storage !! GPU
|-
| 864 || rowspan="2"| 192 || 750G DDR5 || 2 x AMD EPYC 9655 (Zen 5) @ 2.7 GHz, 384MB cache L3 || 7.84TB NVMe 
|-
| 8 || 6000G DDR5 || 2 x AMD EPYC 9654 (Zen 4) @ 2.4 GHz, 384MB cache L3 || 7.84TB NVMe 
|- 
|  160 || 48 || 1125G DDR5 || 1 x AMD EPYC 9454 (Zen 4) @ 2.75 GHz, 256MB cache L3 || 7.84TB NVMe || 4 x NVidia H100 SXM5 (80 GB memory)
|}

==CPU nodes==

===Architecture===
Each node features 2 × AMD EPYC 9655 (Zen 5) @ 2.7 GHz processors, totaling 192 physical cores. The system is built on a chiplet-based NUMA architecture, where each chiplet (CCD) operates as a separate NUMA node. The memory and cache hierarchy is non-uniform, and performance is sensitive to data locality.

===Layout===

* 2 sockets, each with:
** 96 cores
** 12 CCDs (chiplets), each with:
*** 8 cores
*** 32 MiB shared L3 cache

Each core with:
* 1 MiB L2 cache
* 32+32 KiB L1 instruction/data cache
* 12 DDR5 memory channels (shared via the I/O die)

Total:
* 24 NUMA nodes per node (12 per socket × 2)
* 192 cores total
* 768 MiB L3 cache total

===Performance tuning recommendations===

To make best use of the EPYC 9655's architecture:

1. Align tasks to CCDs (NUMA Domains)
Each CCD contains 8 tightly-coupled cores with shared L3 cache. Keeping threads within a CCD avoids inter-chiplet communication latency.

Use:<code>#SBATCH --cpus-per-task=8</code>

This ensures that threads of each task stay within a single CCD.

2. Distribute tasks across NUMA nodes

With 24 NUMA domains per node, launch 24 tasks per node to fully utilize all CCDs without overloading any single NUMA node.

Use:<code>#SBATCH --ntasks-per-node=24</code>

Together with <code>--cpus-per-task=8</code>, this fills the full 192-core node cleanly.

==GPU nodes==

===Architecture===
Each GPU node contains 1 × AMD EPYC 9454 (Zen 4) @ 2.75 GHz processor with 48 physical cores. This processor uses AMD’s chiplet-based NUMA architecture, with memory access times that vary depending on core and memory locality. GPU nodes use the NPS=4 mode (NUMA Per Socket), dividing the socket into four NUMA nodes for better memory locality.

===Layout===

* 1 socket, configured as:
** 6 CCDs (Core Complex Dies)

Each CCD contains:
* 8 cores
* 32 MiB of shared L3 cache

Each core has:
* 1 MiB L2 cache
* 32 KiB L1 instruction cache
* 32 KiB L1 data cache
* 12 DDR5 memory channels

NPS=4 (on GPU nodes):
* Socket is split into 4 NUMA nodes

* Each NUMA node has:
** 12 cores (1.5 CCDs per node)
** 3 memory channels

* 2 NVidia H100 80GB accelerators
** The 4 node accelerators are interconnected by SXM5.

===Performance tuning recommendations===

To fully utilize the architecture of the EPYC 9454 CPU and ensure optimal CPU-GPU data locality:

1. Bind threads to CCDs

Each CCD has 8 closely coupled cores sharing a 32 MiB L3 cache. To keep threads within a CCD: <code>#SBATCH --cpus-per-task=8</code>

This confines threads to one CCD, reducing cross-CCD latency and improving cache usage.

2. Match Tasks to NUMA Nodes
With 4 NUMA nodes per socket (NPS=4), launch 4 tasks per node (or a multiple thereof) for best performance:
<syntaxhighlight lang="bash">
#SBATCH --ntasks-per-node=4
#SBATCH --cpus-per-task=12
</syntaxhighlight>

This keeps each task within a NUMA domain and ensures local access to memory and the GPU.

===GPU instances===

To request one or more full H100 GPUs, you need to use one of the following Slurm options:

'''One H100-80gb''' : <code>--gpus=h100:1</code>

'''Multiple H100-80gb per node''' :
* <code>--gpus-per-node=h100:2</code>
* <code>--gpus-per-node=h100:3</code>
* <code>--gpus-per-node=h100:4</code>

'''For multiple full H100 GPUs spread anywhere''': <code>--gpus=h100:n</code> (replace n with the number of GPUs you want)

Approximately half of the GPU nodes are configured with MIG technology, and only 3 GPU instance sizes are available:

* '''1g.10gb''': 1/8th of the computing power with 10GB GPU memory
* '''2g.20gb''': 2/8th of the computing power with 20GB GPU memory
* '''3g.40gb''': 3/8th of the computing power with 40GB GPU memory

To request one and only one GPU instance for your compute job, use the corresponding option:

* '''1g.10gb''' : <code>--gpus=nvidia_h100_80gb_hbm3_1g.10gb:1</code>
* '''2g.20gb''' : <code>--gpus=nvidia_h100_80gb_hbm3_2g.20gb:1</code>
* '''3g.40gb''' : <code>--gpus=nvidia_h100_80gb_hbm3_3g.40gb:1</code>