<languages />
[[Category:Software]][[Category:BiomolecularSimulation]]


[http://www.ks.uiuc.edu/Research/namd/ NAMD] est un logiciel de dynamique moléculaire orienté-objet conçu pour la simulation de gros systèmes biomoléculaires.
Les simulations sont préparées et analysées dans le logiciel de visualisation [[VMD]].


= Installation =
Le logiciel est installé par notre équipe technique et est disponible via des modules. Si vous avez besoin d'une version plus récente, que vous devez faire vous-même l'installation ou que vous avez des questions, contactez le [[Technical support/fr|soutien technique]].

<span id="Environment_modules"></span>
= Modules d'environnement =

<div class="mw-translate-fuzzy">
La plus récente version 3.0.1 est installée sur toutes nos grappes. La version précédente 2.14 est aussi disponible, mais elle ne peut pas être utilisée avec des GPU H100. Vous pouvez utiliser la version GPU seulement sur Narval.
</div>

Pour les tâches exécutées sur plusieurs nœuds, utilisez UCX.

<span id="Submission_scripts"></span>
= Scripts =

L'information sur l'ordonnanceur Slurm se trouve à la page [[Running jobs/fr|Exécuter des tâches]].

<div class="mw-translate-fuzzy">
== Tâches séquentielles et tâches multifils == 
Le script suivant est pour une simulation séquentielle sur un seul cœur. Pour utiliser plus de cœurs, augmentez la valeur de <tt>--cpus-per-task</tt>, sans dépasser le nombre de cœurs disponible sur le nœud.
</div>

{{File
  |name=serial_namd_job.sh
  |lang="sh"
  |contents=
#!/bin/bash
#
#SBATCH --cpus-per-task=8
#SBATCH --mem 10g            # memory in Mb, increase as needed    
#SBATCH -o slurm.%N.%j.out    # STDOUT file
#SBATCH -t 0:05:00            # time (D-HH:MM), increase as needed
#SBATCH --account=def-specifyaccount

module load StdEnv/2023  gcc/12.3 namd-multicore/3.0.2
namd3 +p$SLURM_CPUS_PER_TASK  +idlepoll +setcpuaffinity stmv.namd
}}

<span id="Multi-node_CPU_jobs"></span>
<div class="mw-translate-fuzzy">
== Tâches parallèles sur CPU ==
</div>



<div class="mw-translate-fuzzy">
===Tâches UCX===
L'exemple suivant utilise 80 processus à raison de 40 processus par nœud sur 2 nœuds et utilise ainsi pleinement les 80 cœurs. Le script  suppose que des nœuds entiers sont utilisés; ainsi, sur Béluga, <code>ntasks-per-node</code> devrait être 40. Les tâches NAMD qui utilisent des nœuds entiers offrent la meilleure performance.
</div>

{{File
  |name=ucx_namd_job.sh
  |lang="sh"
  |contents=
#!/bin/bash
#
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=192
#SBATCH --mem=0            # memory per node, 0 means all memory
#SBATCH -o slurm.%N.%j.out    # STDOUT
#SBATCH -t 0:05:00            # time (D-HH:MM)
#SBATCH --account=def-specifyaccount

module load StdEnv/2023 gcc/13.3 namd-ucx/3.0.2
srun --mpi=pmi2 namd3 stmv.namd

}}



<div class="mw-translate-fuzzy">
=== Tâches utilisant 1 GPU ===
Le prochain exemple utilise huit cœurs CPU et un GPU P100 sur un seul nœud.
</div>

'''Important''' : NAMD 3 offre un nouveau paramètre d’entrée, ce qui dirige plus de calculs vers le GPU. Cela peut améliorer considérablement la performance.

Pour l'utiliser, ajoutez la ligne suivante à votre fichier en entrée&nbsp;:

 GPUresident on;

{{File
  |name=multicore_gpu_namd_job.sh
  |lang="sh"
  |contents=
#!/bin/bash

#SBATCH --cpus-per-task=16 
#SBATCH --mem=10g    
#SBATCH --time=0:15:00
#SBATCH --gpus-per-node=h100:1
#SBATCH --account=def-specifyaccount

module load StdEnv/2023  gcc/12.3  cuda/12.6 namd-multicore/3.0.2
namd3 +p$SLURM_CPUS_PER_TASK  +idlepoll stmv.namd

}}


<div class="mw-translate-fuzzy">
== Tâches utilisant plusieurs GPU ==
=== Tâches UCX avec GPU ===
Cet exemple est pour Béluga et suppose que des nœuds entiers sont utilisés, ce qui offre une meilleure performance pour les tâches NAMD. L'exemple utilise 8 processus sur 2 nœuds, chaque processus utilisant 10 fils et 1 GPU. Ceci utilise pleinement les nœuds GPU de Béluga qui ont 40 cœurs et 4 GPU par nœud. Remarquez que 1 cœur par tâche est réservé à un fil de communication; il est donc normal que NAMD ne rapporte que 72 cœurs utilisés.
</div>

This example is for narval and it assumes that full nodes are used, which gives best performance for NAMD jobs. It uses 8 processes in total on 2 nodes, each process(task) using 12 threads and 1 GPU.   This fully utilizes Narval GPU nodes which have 48 cores and 4 GPUs per node.  Note that 1 core per task has to be reserved for a communications thread, so NAMD will report that only 88 cores are being used but this is normal.  

Pour utiliser ce script sur une autre grappe, voyez les caractéristiques des nœuds disponibles sur la grappe et ajustez les options --cpus-per-task et --gpus-per-node en conséquence.

<div class="mw-translate-fuzzy">
'''NOTE''' : Les versions UCX peuvent être utilisées sur toutes les grappes.
{{File
  |name=ucx_namd_job.sh
  |lang="sh"
  |contents=
#!/bin/bash
</div>

{{File
  |name=ucx_namd_job.sh
  |lang="sh"
  |contents=
#!/bin/bash

<div class="mw-translate-fuzzy">
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=4
#SBATCH --cpus-per-task=10 # number of threads per task (process)
#SBATCH --gpus-per-node=v100:4
#SBATCH --mem=0            # memory per node, 0 means all memory
#SBATCH --time=0:15:00
#SBATCH --account=def-specifyaccount
</div>

<div class="mw-translate-fuzzy">
module load StdEnv/2020  intel/2020.1.217  cuda/11.0 namd-ucx-smp/2.14
NUM_PES=$(expr $SLURM_CPUS_PER_TASK - 1 )
srun --cpus-per-task=$SLURM_CPUS_PER_TASK --mpi=pmi2 namd2 ++ppn $NUM_PES apoa1.namd
}}
</div>



<span id="Performance_and_benchmarking"></span>
==Performance et étalonnage <i>benchmarking</i>==

Le guide <i>[https://mdbench.ace-net.ca/mdbench/ Molecular Dynamics Performance Guide]</i> a été créé par une équipe [https://www.ace-net.ca/ d'ACENET]. Le guide décrit les conditions optimales pour exécuter aussi des tâches sur nos grappes avec AMBER, GROMACS et OpenMM.

Voici un exemple d'étalonnage. La performance de NAMD varie selon les systèmes simulés, en rapport particulièrement avec la quantité d'atomes. Il serait donc très utile d'effectuer le type d'étalonnage montré ici dans les cas où la simulation d'un système particulier serait de longue durée. Les données ainsi collectées peuvent aussi servir à documenter vos demandes aux concours d'allocation de ressources.

Pour obtenir des résultats pertinents, nous vous suggérons de varier le nombre d'étapes pour que la simulation du système se fasse sur quelques minutes et que la collecte des données de durée se fasse à des intervalles d'au moins quelques secondes. Vous pourriez remarquer des variations dans vos résultats de durée si le temps d'exécution est trop court.  

Les données ci-dessous proviennent d'un étalonnage standard apoa1 effectué sur la grappe Graham qui possède des nœuds CPU de 32 cœurs et des nœuds GPU de 32 cœurs et 2 GPU. Pour faire le même exercice avec une autre grappe, vous devrez tenir compte de la structure de ses nœuds.

Dans le premier tableau, nous utilisons NAMD 2.12 chargé du module verbs. L'efficacité est calculée avec (durée avec 1 cœur) / (N * (durée avec N cœurs) ).

{| class="wikitable sortable"
|-
! # cœurs !! Durée réelle par étape !! Efficacité
|-
| 1 ||  0.8313||100%
|-
| 2 ||  0.4151||100%
|-
| 4 ||  0.1945|| 107%
|-
| 8 ||   0.0987|| 105%
|-
| 16 ||   0.0501|| 104%
|-
| 32   ||    0.0257|| 101%
|-
| 64 ||  0.0133|| 98%
|-
| 128 || 0.0074|| 88%
|-
| 256 || 0.0036|| 90%
|-
| 512 || 0.0021|| 77%
|-
|}

Dans ce cas, nous constatons qu'il est acceptable d'utiliser 256 cœurs pour simuler le système. Si vous demandez plus de cœurs qu'il n'en faut, vos tâches seront plus longtemps en attente et votre résultat d'ensemble en serait affecté.

Dans le prochain cas, l'étalonnage est fait avec l'utilisation de GPU. Le module multicœur de NAMD est utilisé pour les simulations pouvant être effectuées avec un (1) nœud alors que le module verbs-smp sert dans les cas de tâches nécessitant plusieurs nœuds.

{| class="wikitable sortable"
|-
! # cœurs  !! #GPU !! Durée réelle par étape !! Notes
|-
| 4 || 1   ||  0.0165 || 1 nœud, multicœur
|-
| 8 || 1   || 0.0088 || 1 nœud, multicœur
|-
| 16 || 1   || 0.0071 || 1 nœud, multicœur
|-
| 32   || 2   ||  0.0045  || 1 nœud, multicœur
|-
| 64 || 4 || 0.0058  || 2 nœuds, verbs-smp
|-
| 128 || 8 ||  0.0051 || 2 nœuds, verbs-smp
|-
|}

Les données du tableau indiquent clairement qu'il est absolument inutile d'utiliser plus d'un nœud puisque la performance décroît avec 2 nœuds ou plus. Avec 1 nœud, il est préférable d'utiliser 1 GPU/16 cœurs puisque l'efficacité est maximale; l'utilisation de 2 GPU/32 cœurs est acceptable si vos résultats doivent être produits rapidement. Puisque les nœuds GPU de Graham l'ordre de priorité est le même pour toutes les tâches 1 GPU/16 cœurs, il n'y a aucun avantage à utiliser 4 ou 8 cœurs.

On peut se demander si la simulation peut ou non utiliser un GPU. Les résultats de l'étalonnage indiquent que l'utilisation d'un nœud GPU (2 GPU/32 cœurs) sur Graham traite la tâche plus rapidement que sur 4 nœuds qui ne sont pas des GPU. Puisque le coût d'un nœud GPU sur Graham est près de deux fois plus que celui d'un nœud qui n'est pas un GPU, il est plus économique d'utiliser les GPU. C'est ce que vous devriez faire tant que c'est possible, mais puisque les nœuds CPU sont en plus grand nombre, vous devriez considérer aussi ne pas utiliser de GPU si le temps d'attente est trop long.

= NAMD 3 =

NAMD 3 est disponible dans un module. Dans le cas de certaines configurations de système, la performance pourrait être meilleure comparée à celle de NAMD 2.14.

Pour l'essayer immédiatement, vous pouvez télécharger le binaire à partir du site web NAMD website et le modifier pour nos systèmes avec, par exemple (indiquez la version au besoin)

 tar xvfz NAMD_3.0alpha11_Linux-x86_64-multicore-CUDA-SingleNode.tar.gz 
 cd NAMD_3.0alpha11_Linux-x86_64-multicore-CUDA
 setrpaths.sh  --path .

Par la suite, l'exécutable namd3 dans ce répertoire sera lié aux librairies appropriées.  Vous pouvez alors soumettre une tâche qui utilise cet exécutable.

Pour obtenir une meilleure performance avec NAMD 3 sur un GPU, nous vous recommandons fortement d'ajouter au fichier de configuration le mot-clé suivant, pourvu que la configuration en entrée que vous utilisez le permette.

 GPUresident on;

Pour plus d'information sur ce paramètre et ses changements, voir [https://www.ks.uiuc.edu/Research/namd/alpha/3.0alpha/ cette page].

= Références =
* À télécharger : http://www.ks.uiuc.edu/Development/Download/download.cgi?PackageName=NAMD (on vous demandera de vous inscrire)
*[http://www.ks.uiuc.edu/Research/namd/2.14/ug/ NAMD User's guide for version 2.14]
*[https://www.ks.uiuc.edu/Research/namd/3.0.1/ug/ NAMD User's guide for version 3.0.1]
*[http://www.ks.uiuc.edu/Research/namd/3.0.1/notes.html NAMD version 3.0.1 release notes]
*[http://www.ks.uiuc.edu/Research/namd/2.14/notes.html NAMD version 2.14 release notes]
* Tutoriels : http://www.ks.uiuc.edu/Training/Tutorials/