<languages />

= Introduction =

Pour demander un ou plusieurs GPU pour une tâche [[Running jobs/fr|Slurm]], utilisez
  --gpus-per-node=<type>:<number>

Par exemple
  --gpus-per-node=a100:1

Ceci est pour un seul GPU A100, à moins que vous ajoutiez <code>--nodes</code> pour demander plusieurs nœuds.
Pour les types valides, voir [[Using GPUs with Slurm/fr|GPU disponibles</i>]].

Vous pouvez aussi utiliser
  --gres=gpu:<type>:<number>
Cependant, il est possible que ce format ne soit plus pris en charge. Nous vous recommandons de le remplacer par  <code>--gpus-per-node</code>.

Slurm prend en charge plusieurs autres directives que vous pouvez utiliser pour demander des GPU, par exemple <code>--gpus</code>, <code>--gpus-per-socket</code>, <code>--gpus-per-task</code>, <code>--mem-per-gpu</code> et <code>--ntasks-per-gpu</code>. Voyez la documentation de Slurm sur [https://slurm.schedmd.com/sbatch.html sbatch].  Notre équipe ne teste pas toutes les combinaisons; si vous n'obtenez pas le résultat voulu, contactez le [[Technical support/fr|soutien technique]].

Pour l'information générale sur l'ordonnancement des tâches, consultez [[Running jobs/fr|Exécuter des tâches]].

= GPU disponibles =
The following table summarizes the available GPU types:

<div class="mw-translate-fuzzy">
{| class="wikitable"
|-
! Grappe !! Caractéristiques !! Type <br>de GPU !! Modèle  !! Compute<br>Capability(*) !! Remarques
|- 
| Fir   || [[Fir/fr#Caractéristiques_des_nœuds|<b>Détails</b>]] || [[Multi-Instance_GPU/fr#Configurations_disponibles|Configurations disponibles|<b>options</b>]]
|| H100-80gb    || 90   ||deux GPU connectés par socket CPU; tous les GPU sont connectés via NVLink
|-

| Narval           || [[Narval#Caractéristiques_des_nœuds|<b>Détails</b>]]
|| [[Narval/en#GPU_instances|<b>options</b>]] || A100-40gb || 80 || deux GPU connectés par socket CPU; tous les GPU sont connectés via NVLink
|- 
| rowspan=2|Nibi   || rowspan=2|[[Nibi/fr#Caractéristiques_des_nœuds|<b>Détails</b>]] || rowspan=2|[[Multi-Instance GPU/fr#Configurations_disponibles|<b>Options</b>]]
|| H100-80gb    || 90   || deux GPU connectés par socket CPU; tous les GPU sont connectés via NVLink
|-
|| MI300A-128gb || s.o. || mémoire unifiée entre CPU et GPU
|- 
| Rorqual          || [[Rorqual#Caractéristiques_des_nœuds|<b>Détails</b>]]
|| [[Rorqual#Instances_GPU|<b>options</b>]] || H100-80gb || 90 || deux GPU connectés par socket CPU; tous les GPU sont connectés via NVLink
|-
| Trillium          || [[Trillium/fr#Caractéristiques_des_nœuds|<b>détails</b>]]
|| [[Trillium/fr#Instances_GPU|Instances GPU|<b>options</b>]] || H100-80gb || 90 || deux GPU connectés par socket CPU; tous les GPU sont connectés via NVLink
|-
| Arbutus || colspan=4|Les ressources infonuagiques ne sont pas ordonnancées. Voir [[Cloud resources/fr|Ressources infonuagiques]] pour l'information sur le matériel.
|}
</div>

La commande ci-dessous présente les types de GPU (et MIG) disponibles sur chacune des grappes. Cette commande est utile si le tableau plus haut n'a pas été mis à jour récemment.

{{Command|sinfo -o "%G"{{!}}grep gpu{{!}}sed 's/gpu://g'{{!}}sed 's/),/\n/g'{{!}}cut -d: -f1{{!}}sort{{!}}uniq}}

There are short synonyms available for some of the MIG types at certain sites; this command will not provide those synonyms.
Also, the presence of a GPU type does not guarantee that you will be able to use that type in your jobs; there may be 
further restrictions on what types are available based on (for example) which research group you are in.
For further information see the site-specific page by clicking on the cluster name in the above table, or [[Technical support|contact support]].

If you do not supply a type specifier your job may be rejected or it may be sent to an arbitrary GPU type.
There are very few programs which can use an arbitrary GPU efficiently,
so we strongly recommend that you always request a specific GPU type. 

There are GPUs available at Arbutus, but like other cloud resources they cannot be scheduled via Slurm.
See [[Cloud resources]] for more details.

== GPU multi-instances (MIG) ==
La technologie MIG permet de partitionner un GPU en plusieurs instances. Vos tâches pourraient utiliser un MIG plutôt qu'un GPU entier. Pour plus d'information, voir [[Multi-Instance_GPU/fr|GPU multi-instances]].

<span id="Requesting_CPU_cores_and_system_memory"></span>
= Demander des cœurs CPU et la mémoire système =

Avec chaque instance GPU, une tâche doit avoir un nombre de cœurs CPU (<code>1</code> par défaut) et une certaine quantité de mémoire système. Pour le maximum de coeurs CPU et de mémoire système, voir [[Allocations_and_compute_scheduling/fr#Ratios_dans_les_bundles|le tableau des ratios]].

<span id="Examples"></span>
= Exemples =

== Tâches avec un seul cœur ==
Pour une tâche qui nécessite un seul cœur CPU et un GPU,
{{File
  |name=gpu_serial_job.sh
  |lang="sh"
  |contents=
#!/bin/bash
#SBATCH --account=def-someuser
#SBATCH --gpus-per-node=a100:1
#SBATCH --mem=4000M               # mémoire par nœud
#SBATCH --time=0-03:00
./program                         # pour tester, utilisez nvidia-smi
}}

== Tâches multifils ==
Pour une tâche GPU qui nécessite plusieurs CPU dans un seul nœud,
{{File
  |name=gpu_threaded_job.sh
  |lang="sh"
  |contents=
#!/bin/bash
#SBATCH --account=def-someuser
#SBATCH --gpus-per-node=a100:1         # nombre de GPU par nœud
#SBATCH --cpus-per-task=6         # cœurs ou fils CPU
#SBATCH --mem=4000M               # mémoire par nœud
#SBATCH --time=0-03:00
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
./program
}}

Pour chaque GPU demandé, nous recommandons
* sur Fir, un maximum de 12 cœurs CPU
* sur Narval, un maximum de 12 cœurs CPU
* sur Nibi, un maximum de 14 cœurs CPU
* sur Rorqual, un maximum de 16 cœurs CPU

== Tâches MPI ==
{{File
  |name=gpu_mpi_job.sh
  |lang="sh"
  |contents=
#!/bin/bash
#SBATCH --account=def-someuser
#SBATCH --gpus=8                  # nombre total de GPU
#SBATCH --ntasks-per-gpu=1        # 8 processus MPI au total
#SBATCH --cpus-per-task=6         # cœurs CPU par processus MPI
#SBATCH --mem-per-cpu=5G          # mémoire hôte par cœur CPU
#SBATCH --time=0-03:00            # temps de calcul (JJ-HH:MM)
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
srun --cpus-per-task=$SLURM_CPUS_PER_TASK ./program
}}

== Nœuds entiers  ==
Si votre application peut utiliser efficacement un nœud entier et ses GPU associés, vous pouvez probablement réduire le temps d'attente si vous demandez un nœud entier. Utilisez les scripts suivants comme modèle. 

<span id="Packing_single-GPU_jobs_within_one_SLURM_job"></span>
===Regroupement de tâches pour un seul GPU===

Pour exécuter pendant plus de 24 heures quatre programmes qui utilisent un seul GPU ou deux programmes qui utilisent deux GPU, nous recommandons [[GNU Parallel/fr|GNU Parallel]]. Voici un exemple simple&nbsp;:
<pre>
cat params.input | parallel -j4 'CUDA_VISIBLE_DEVICES=$(({%} - 1)) python {} &> {#}.out'
</pre>
L'identifiant du GPU est calculé en soustrayant 1 de l'identifiant de la fente (<i>slot</i>), représenté par {%}. L'identifiant de la tâche est représenté par {#}, avec des valeurs partant de 1.

Le fichier <code>params.input</code> devrait contenir les paramètres sur des lignes distinctes, comme suit&nbsp;:
<pre>
code1.py
code2.py
code3.py
code4.py
...
</pre>
Vous pouvez ainsi soumettre plusieurs tâches. Le paramètre <code>-j4</code> fait en sorte que GNU Parallel exécutera quatre tâches concurremment en lançant une tâche aussitôt que la précédente est terminée. Pour éviter que deux tâches se disputent le même GPU, utilisez CUDA_VISIBLE_DEVICES.

<span id="Profiling_GPU_tasks"></span>
== Profilage des tâches avec GPU ==

Sur Fir et Nibi, le profilage n'est pas disponible parce que les compteurs de performance ne sont pas accessibles.

Sur [[Narval]] et [[Rorqual]] le profilage est possible, mais 
[https://developer.nvidia.com/dcgm DCGM (NVIDIA Data Center GPU Manager)]
doit être désactivé. Ceci doit se faire lorsque vous soumettez la tâche en configurant la variable d'environnement <code>DISABLE_DCGM</code>.

{{Command|DISABLE_DCGM{{=}}1 salloc --account{{=}}def-someuser --gpus-per-node{{=}}1 --mem{{=}}4000M --time{{=}}03:00}}

Ensuite, dans votre tâche interactive, attendez que DCGM soit désactivé sur le nœud. 
{{Command|while [ ! -z "$(dcgmi -v {{!}} grep 'Hostengine build info:')" ]; do  sleep 5; done}}

Enfin, lancez le profileur (voir [[Debugging and profiling/fr|débogage et profilage]] pour les détails).

= Voir aussi =
[[CUDA/fr|Cuda]]<br>
[[Multi-Instance GPU/fr|GPU multi-instances]]<br>
[[Running jobs/fr|Exécuter des tâches]]

[[Category:SLURM]]