<languages />

=Description=

[https://rapids.ai/ RAPIDS] est une suite de bibliothèques de logiciels ''open source'' de NVIDIA qui sert principalement à l'exécution de pipelines de science des données et d'analyse en Python avec des GPU. La suite s'appuie sur CUDA pour l'optimisation des calculs de bas niveau et fournit des API Python conviviales semblables à celles de Pandas ou Scikit-learn.

Les principales composantes sont :


* '''cuDF''' : bibliothèque Python de DataFrames GPU (selon le format en colonne Apache Arrow) pour le chargement, la fusion, l'agrégation, la sélection et autres manipulations des données.

* '''cuML''' :  suite de bibliothèques pour l’implémentation d’algorithmes d’apprentissage machine et de fonctions primitives, qui permet le partage d’API compatibles avec d’autres projets RAPIDS.

* '''cuGraph''' : bibliothèque pour l’analyse de graphiques accélérée par GPU, qui offre une fonctionnalité semblable à NetworkX et est parfaitement intégrée à la plateforme [de science des données] RAPIDS.

* '''Cyber Log Accelerators (CLX ou ''clicks'')''' :  collection d’exemples RAPIDS dans les domaines de la sécurité, la science des données et le génie, qui permet d’appliquer rapidement RAPIDS et l’accélération GPU à des cas concrets de cybersécurité.

* '''cuxFilter''' : bibliothèque de connecteurs pour relier facilement des bibliothèques de visualisation et des DataFrames GPU et permet aussi d’utiliser interactivement des graphiques de différentes bibliothèques dans le même tableau de bord.

* '''cuSpatial''' : bibliothèque C++/Python avec accélération GPU pour les systèmes d’information géographique incluant la recherche de points à l’intérieur d’un polygone, la jointure spatiale, les systèmes de coordonnées, les primitives de forme, les distances et l’analyse de trajectoires.

* '''cuSignal''' : accélération GPU dans le traitement des signaux avec  CuPy, Numba et l’écosystème RAPID. Dans certains cas, cuSignal est un port direct de Scipy Signal pour utiliser des ressources de calcul via CuPy, mais qui contient aussi des kernels Numba CUDA pour plus d’accélération à des fonctions sélectionnées

* '''cuCIM''' : boîte à outils extensible pour l’accélération GPU des entrées/sorties, la vision par ordinateur et le traitement des primitives, principalement dans le domaine de l’ imagerie médicale.

* '''RAPIDS Memory Manager (RMM)''' : outil de gestion des allocations de mémoire pour cuDF (C++ et Python) et les autres bibliothèques RAPIDS. RMM gère aussi le remplacement des allocations de mémoire CUDA et de la mémoire des périphériques CUDA et effectue rapidement les allocations et désallocations de manière asynchrone en réservant une quantité définie de mémoire. 

= Images Apptainer = 

Pour créer une image Apptainer (auparavant [[Singularity/fr#Utilisez_plutôt_Apptainer|Singularity]]) pour RAPIDS, il faut d’abord trouver et sélectionner une image Docker fournie par NVIDIA.

==Trouver une image Docker== 
 
À partir de RAPIDS v23.08, les deux types d’images Docker pour RAPIDS sont ''base'' et ''notebooks''. Pour chaque type, plusieurs images sont fournies pour les différentes combinaisons des versions de RAPIDS et de CUDA avec Ubuntu ou CentOS. Pour une image en particulier, la commande <tt>pull</tt> se trouve sous l'onglet ''Tags'' de chacun des sites.   
 
* [https://catalog.ngc.nvidia.com/orgs/nvidia/teams/rapidsai/containers/base RAPIDS Base] : contient un environnement RAPIDS prêt à être utilisé pour soumettre une tâche à l'ordonnanceur.
* [https://catalog.ngc.nvidia.com/orgs/nvidia/teams/rapidsai/containers/notebooks RAPIDS Notebooks] : ajoute à l'image Base un serveur Jupyter notebook et des exemples de notebooks. Utilisez ce type d'image pour travailler en mode interactif avec des noteboooks et des exemples.

==Construire une image Apptainer==

Par exemple, si la commande Docker <tt>pull</tt> pour une image sélectionnée se lit 

<source lang="console">docker pull nvcr.io/nvidia/rapidsai/rapidsai:cuda11.0-runtime-centos7</source> 
 
avec un ordinateur qui supporte Apptainer, vous pouvez construire une image Apptainer (ici ''rapids.sif'') avec la commande suivante, basée sur l'étiquette <code>pull</code>
 
<source lang="console">[name@server ~]$ apptainer build rapids.sif docker://nvcr.io/nvidia/rapidsai/rapidsai:cuda11.0-runtime-centos7</source>

Le processus prend habituellement de 30 à 60 minutes. Puisque la taille de l’image est grande, assurez-vous que vous avez assez de mémoire et d’espace disque sur le serveur.

=Travailler sur une grappe= 

Une fois que vous avez une image Apptainer pour RAPIDS sur une de nos grappes, vous pouvez demander une session interactive sur un nœud GPU ou soumettre une tâche en lot à l 'ordonnanceur quand votre code RAPIDS est prêt.

==Travailler interactivement sur un nœud GPU== 

Si l’image Apptainer a été construite avec une image Docker de type notebooks, elle inclut un serveur Jupyter Notebook et peut être employée pour explorer RAPIDS interactivement sur un nœud de calcul GPU.<br>
Pour demander une session interactive sur un nœud de calcul GPU, par exemple un GPU de type T4 de Graham, utilisez

<source lang="console">[name@gra-login ~]$ salloc --ntasks=1 --cpus-per-task=2 --mem=10G --gres=gpu:t4:1 --time=1:0:0 --account=def-someuser</source>

Quand la ressource est allouée, lancez l’interpréteur RAPIDS sur le nœud
GPU avec

<source lang="console">[name@gra#### ~]$ module load apptainer
[name@gra#### ~]$ apptainer shell --nv -B /home -B /project -B /scratch  rapids.sif
</source>
* l'option <tt>--nv</tt> fait le ''bind mount'' du périphérique GPU de l’hôte sur le conteneur pour que l’accès au GPU puisse se faire de l’intérieur du conteneur Apptainer;
* l'option  <tt>-B</tt> fait le ''bind mount'' des systèmes de fichiers auxquels vous voulez avoir accès dans le conteneur.

Lorsque l’invite de l’interpréteur change pour <tt>Apptainer></tt>, vous pouvez consulter les statistiques pour le GPU dans l'interpréteur pour vous assurer que vous avez accès au GPU.

<source lang="console">Apptainer> nvidia-smi</source>

Lorsque l’invite change pour <tt>Apptainer></tt>, vous pouvez lancer le serveur Jupyter Notebook server dans l’environnement RAPIDS; ceci affichera l’URL du serveur.
<source lang="console">Apptainer> jupyter-lab --ip $(hostname -f) --no-browser 
</source>

'''NOTE :''' À partir de la version 23.08, RAPIDS n'a pas besoin d'être activé après que Conda ait démarré puisque tous les paquets sont inclus dans l'environnnement Conda de base qui est activé par défaut dans ; par exemple, vous pouvez lancer le serveur Jupyter Notebook dans l'interpréteur du conteneur. 

Puisqu’un nœud de calcul sur Graham n’est pas connecté directement à l’internet, il faut configurer un tunnel SSH pour faire la redirection de port entre votre ordinateur et le nœud GPU. Pour les détails, voyez [[Advanced Jupyter configuration/fr#Se_connecter_à_JupyterLab|comment se connecter à Jupyter Notebook]].

==Soumettre une tâche RAPIDS à l'ordonnanceur==  
 
Quand votre code RAPIDS est prêt, vous pouvez soumettre une tâche à l'ordonnanceur. La bonne pratique est [[Using node-local storage/fr|d'utiliser le disque local]] quand vous travaillez avec un conteneur sur un nœud de calcul.   

 
'''Script de soumission'''
{{File
  |name=submit.sh
  |lang="sh"
  |contents=
#!/bin/bash
#SBATCH --gres=gpu:t4:1
#SBATCH --cpus-per-task=2
#SBATCH --mem=10G
#SBATCH --time=dd:hh:mm
#SBATCH --account=def-someuser

module load apptainer

# copy your container image and your rapids code and data to the local disk on a compute node via $SLURM_TMPDIR

cd $SLURM_TMPDIR 
cp /path/to/rapids.sif ./
cp /path/to/your_rapids_code.py ./
cp -r /path/to/your_datasets ./
 
apptainer exec --nv rapids.sif python ./my_rapids_code.py 
 
# save any results to your /project before terminating the job
cp -r your_results ~/projects/def-someuser/username/

}}

=Références=

* [https://docs.rapids.ai/ RAPIDS Docs]: documentation complète pour RAPIDS, comment rester en contact et rapporter les problèmes;
* [https://github.com/rapidsai/notebooks RAPIDS Notebooks]: exemples sur GitHub que vous pouvez utiliser;
* [https://medium.com/rapids-ai RAPIDS on Medium]: cas d’usage et blogues.