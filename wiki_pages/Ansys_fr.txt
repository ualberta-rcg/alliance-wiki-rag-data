<languages />
[[Category:Software]]

[http://www.ansys.com/ Ansys] est une suite logicielle pour la conception 3D et la simulation. La suite comprend des applications comme [http://www.ansys.com/Products/Fluids/ANSYS-Fluent Ansys Fluent] et [http://www.ansys.com/products/fluids/ansys-cfx Ansys CFX].

= Licence=
La suite Ansys est hébergée sur nos grappes, mais nous n'avons pas une licence qui permet un accès généralisé. Toutefois, plusieurs établissements, facultés et départements possèdent des licences qui peuvent être utilisées sur nos grappes; vérifiez l'aspect légal de son utilisation. En ce qui a trait à l'aspect technique, nos nœuds de calcul doivent pouvoir communiquer avec votre serveur de licence. Si ce n'est pas déjà fait, notre équipe technique coordonnera ceci avec votre gestionnaire de licence. Quand tout sera en place, vous pourrez charger le module Ansys qui localisera de lui-même la licence. En cas de difficulté, communiquez avec le [[Technical support/fr| soutien technique]].

<div class="mw-translate-fuzzy">
== Configurez votre propre fichier de licence ==
Notre module Ansys cherche l'information sur la licence à différents endroits, dont votre répertoire /home.
Pour indiquer votre propre serveur de licence, créez un fichier nommé <code>$HOME/.licenses/ansys.lic</code> qui contient les deux lignes ci-dessous, où vous remplacez FLEXPORT, INTEPORT et LICSERVER par les valeurs de votre serveur.
</div> 

<div class="mw-translate-fuzzy">
{| class="wikitable" style="text-align:left; border:1px solid #BBB; background-color:#F9F9F9; width:50%;"
|+ style="text-align:left; background-color:#F2F2F2; font-size:110%" | FICHIER : ansys.lic
|-
| style="border-style: none none none none; font-size: 100%; padding-left:10%; padding-bottom:0;" | setenv("ANSYSLMD_LICENSE_FILE", "<b>FLEXPORT</span>@LICSERVER</b>")
|-
| style="border-style: none none none none; font-size: 100%; padding-left:10%; padding-top:0;" | setenv("ANSYSLI_SERVERS", "<b>INTEPORT@LICSERVER</b>") 
|}
</div>

Les valeurs correspondant aux serveurs de licence CMC et SHARCNET se trouvent dans le tableau ci-dessous. Pour utiliser un différent serveur, voir [[Ansys/fr#Serveurs_de_licence_locaux|Serveurs de licence locaux]] ci-dessous.

<div class="mw-translate-fuzzy">
{| class="wikitable"
|+ style="text-align:left; background-color:#F2F2F2; font-size:110%" | TABLEAU : Serveurs de licence préconfigurés
! Licence
! Grappe
! LICSERVER
! FLEXPORT
! INTEPORT
! VENDPORT
! NOTES
|-
| CMC
| beluga
| <code>10.20.73.21</code>
| <code>6624</code>
| <code>2325</code>
| s.o.
| aucune
|-
| CMC
| cedar
| <code>172.16.0.101</code>
| <code>6624</code>
| <code>2325</code>
| s.o.
| aucune
|-
| CMC
| graham
| <code>10.25.1.56</code>
| <code>6624</code>
| <code>2325</code>
| s.o.
| nouvelle IP le 21 février 2025
|-
| CMC
| narval
| <code>10.100.64.10</code>
| <code>6624</code>
| <code>2325</code>
| s.o.
| aucune
|-
| SHARCNET
| beluga/cedar/graham/gra-vdi/nibi/narval/rorqual
| <code>license3.sharcnet.ca</code>
| <code>1055</code>
| <code>2325</code>
| n/a
| None
|-
| SHARCNET
| niagara
| <code>localhost</code>
| <code>1055</code>
| <code>2325</code>
| <code>1793</code>
| aucune
|}
</div>

Si vous avez obtenu une licence de CMC, vous devez faire parvenir le nom d'utilisateur associé à votre compte avec l'Alliance à <cmcsupport@cmc.ca>, autrement la licence ne fonctionnera pas. Pour connaître le nombre de cœurs que vous pouvez utiliser avec une licence CMC, voyez les sections <I>Other Tricks and Tips</i> des  [https://www.cmc.ca/?s=Other+Tricks+and+Tips&lang=en/  guides Ansys Electronics Desktop et Ansys Mechanical/Fluids].

<span id="Local_license_servers"></span>
=== Serveurs de licence locaux ===

Avant que le serveur de licence de votre établissement puisse être utilisé, les coupe-feu des deux parties doivent être configurés. Dans plusieurs cas, ce travail est déjà fait; suivez les directives dans le paragraphe <i>Prêt à utiliser</i> ci-dessous. Autrement, référez-vous au paragraphe <i>Configuration requise</i> un peu plus bas.

<span id="Ready_to_use"></span>
==== Prêt à utiliser ====

<div class="mw-translate-fuzzy">
Pour utiliser un serveur de licence ANSYS déjà configuré pour être utilisé sur la grappe où vous allez soumettre des tâches, contactez votre administrateur de serveur de licences Ansys et obtenez les trois éléments d'information suivants&nbsp;:
 1) le nom d'hôte complet (LICSERVER) du serveur
 2) le port flex (FLEXPORT) pour Ansys, habituellement 1055
 3) le port d'interconnexion (INTEPORT), habituellement 2325
Une fois les trois éléments d'information collectés, configurez votre fichier <code>~/.licenses/ansys.lic</code> en entrant les valeurs de LICSERVER, FLEXPORT et INTEPORT dans le modèle <code>FILE: ansys.lic</code> ci-dessus.
</div>

<span id="Setup_required"></span>
==== Configuration requise ====

<div class="mw-translate-fuzzy">
Si votre serveur de licence Ansys local n'a jamais été configuré pour être utilisé sur la ou les grappes où vous allez soumettre des tâches, en plus des 3 éléments ci-dessus, vous devrez ÉGALEMENT obtenir les éléments suivants auprès de l'administrateur&nbsp;:
 4) le numéro de port statique du fournisseur (VENDPORT)
 5) confirmation que <servername> se résoudra à la même adresse IP que LICSERVER sur nos grappes
où <servername> peut être trouvé dans la première ligne du fichier de licence avec le format <i>SERVER <servername> <host id> <lmgrd port></i>. L'élément 5 est obligatoire sinon les extractions de licences Ansys ne fonctionneront sur aucune grappe distante. S'il s'avère que <servername> ne répond pas à cette exigence, demandez à votre administrateur de licence de remplacer <servername> par le même nom d'hôte complet que LICSERVER ou au moins par un nom d'hôte qui se résoudra à la même adresse IP que LICSERVER à distance.
</div>

<span id="Checking_license"></span>
== Vérifier la licence ==

<div class="mw-translate-fuzzy">
Pour vérifier si <code>ansys.lic</code> est bien configuré et fonctionne correctement, copiez et collez la séquence de commandes suivantes sur la grappe où vous voulez soumettre des tâches. La seule différence est de spécifier YOURUSERID. Si le logiciel n’est pas à jour sur le serveur de licence distant, un problème peut survenir si la dernière version du module Ansys est chargée pour effectuer des tests. Pour que la licence fonctionne quand des tâches sont soumises, assurez-vous que la même version du module Ansys qui est chargé par votre script est utilisée dans les commandes ci-dessous.
 [gra-login:~] cd /tmp
 [gra-login:~] salloc --time{{=}}1:0:0 --mem{{=}}1000M --account{{=}}def-YOURUSERID
 [gra-login:~] module load StdEnv/2023; module load ansys/2023R2
 [gra-login:~] $EBROOTANSYS/v$(echo ${EBVERSIONANSYS:2:2}${EBVERSIONANSYS:5:1})/licensingclient/linx64/lmutil lmstat -c $ANSYSLMD_LICENSE_FILE 1> /dev/null && echo Success {{!}}{{!}} echo Fail
</div>

<div class="mw-translate-fuzzy">
Si le résultat est <code>Success</code>, la licence devrait fonctionner quand des tâches sont soumises. <br>
Par contre si le résultat est <code>Fail</code>, écrivez au [[technical support/fr|soutien technique]].
</div>

<span id="Version_compatibility"></span>
= Compatibilité des versions =

<div class="mw-translate-fuzzy">
Les simulations Ansys sont typiquement compatibles avec des versions postérieures, mais <span style="color:red">ce n'est pas le cas</span> avec les versions antérieures. Ceci signifie que des simulations faites avec une moins récente version de Ansys devrait pouvoir être chargées et exécutées sans problème avec une version plus récente.  Par exemple, une simulation créée et sauvegardée avec ansys/2022R2 devrait fonctionner avec ansys/2023R2, mais  <span style="color:red">pas dans l'autre sens</span>.  Il est toujours possible de lancer une simulation créée avec une version antérieure, mais il est fort possible que la simulation plante ou que vous obteniez des messages d'erreur. Quant aux simulations Fluent, si vous ne vous souvenez pas du numéro de la version Ansys que vous avez utilisée  pour créer le fichier cas, vous trouverez des indices avec les lignes suivantes.
</div>

 $ grep -ia fluent combustor.cas
   (0 "fluent15.0.7  build-id: 596")

 $ grep -ia fluent cavity.cas.h5
   ANSYS_FLUENT 24.1 Build 1018

<span id="Platform_support"></span>
== Plateformes prises en charge ==

== ANSYS Fluent ==
Voici la procédure habituelle pour utiliser Fluent avec les grappes de Calcul Canada :

<span id="What&#039;s_new"></span>
== Nouveautés ==

Ansys publie régulièrement des <i>service packs</i> pour regrouper plusieurs mises-à-jour apportant différents correctifs et améliorations à ses versions majeures. Des informations similaires pour les versions précédentes peuvent généralement être trouvées sur [https://www.ansys.com/blog le blog Ansys], en utilisant la barre de recherche FILTERS. Par exemple, la recherche de <code>What’s New Fluent 2024 gpu</code> affichera le document <code>[https://www.ansys.com/blog/fluent-2024-r1 What’s New for Ansys Fluent in 2024 R1?]</code> qui contient une multitude d'informations sur la prise en charge des GPU. Spécifier un numéro de version dans le champ de recherche [https://www.ansys.com/news-center/press-releases Press Release] est également un bon moyen de trouver des informations sur les nouvelles versions. Le module <code>ansys/2025R1.02</code> pour la dernière version de Ansys a été installé récemment; pour l'utiliser cependant, vous avez besoin d'un serveur de licence comme celui de CMC. La mise à jour du serveur de licence de SHARCNET est en cours et tant que ce travail ne sera pas terminé, seules les versions <code>ansys/2024R2.04</code> ou moins récentes seront prises en charge. Si un module pose problème ou pour demander l'installation d'une nouvelle version, écrivez au [[Technical support/fr|soutien technique]].

<span id="Service_packs"></span>
== Correctifs ==

À partir d'Ansys 2024, un module Ansys distinct sera identifié avec une décimale et deux chiffres après le numéro de version, chaque fois qu'un <i>service pack</i> est installé pour la version initiale. Par exemple, la version initiale pour 2024 sans aucun <i>service pack</i> peut être chargée en exécutant <code>module load ansys/2024R1</code> tandis qu'un module avec le <i>service pack</i> 3 peut être chargé avec <code>module load ansys/2024R1.03</code>. Si un <i>service pack</i> est déjà disponible au moment où une nouvelle version doit être installée, il est fort probable que seulement un module pour ce numéro de <i>service pack</i> sera installé, à moins qu'une demande soit faite pour l'installation de la version initiale.

La plupart du temps, vous voudrez probablement charger la dernière version du module équipé du dernier <i>service pack</i> installé en exécutant simplement <code>module load ansys</code>. Bien qu'il ne soit pas prévu que les <i>service packs</i> aient un impact sur les résultats numériques, les modifications qu'ils apportent sont importantes et donc si des calculs ont déjà été effectués avec la version initiale ou un <i>service pack</i> antérieur, certains groupes préféreront peut-être continuer à l'utiliser. Le fait d'avoir des modules distincts pour chaque <i>service pack</i> rend cela possible. À partir d'Ansys 2024R1, une description détaillée de ce que fait chaque <i>service pack</i> se trouve dans [https://storage.ansys.com/staticfiles/cp/Readme/release2024R1/info_combined.pdf la documentation officielle] (les versions futures pourront probablement être consultées de la même manière en modifiant le numéro de version contenu dans le lien).

= Soumettre des tâches en lot sur nos grappes = 
Plusieurs implémentations MPI incluses dans la suite Ansys permettent le calcul parallèle, mais aucune n'est compatible avec l'ordonnanceur Slurm (voir [[Running_jobs/fr|Exécuter des tâches]]). Pour cette raison, il faut utiliser des directives particulières à chaque paquet Ansys pour lancer une tâche parallèle. Vous trouverez ci-dessous quelques scripts de soumission pour ce faire. Ils fonctionneront sur toutes les grappes, mais sur Niagara, vous devrez peut-être [https://docs.scinet.utoronto.ca/index.php faire certains ajustements].

<div class="mw-translate-fuzzy">
== Ansys Fluent ==
La procédure suivante est habituellement utilisée pour exécuter Fluent sur une de nos grappes&nbsp;:
</div>

# Sur votre ordinateur, préparez votre tâche avec Fluent du Ansys Workbench jusqu'au point où les calculs seraient exécutés.
# Exportez le fichier de cas avec <i>File > Export > Case…</i> ou localisez le répertoire dans lequel Fluent enregistre les fichiers pour votre projet. Le nom des fichiers de cas a souvent un format tel que <code>FFF-1.cas.gz</code>.
# Si vous voulez poursuivre avec des données d'un calcul effectué précédemment, exportez aussi un fichier de données avec <i>File > Export > Data…</i> ou trouvez-le dans le même répertoire /project (<code>FFF-1.dat.gz</code>). 
# [[Transferring_data/fr|Transférez]] le fichier de cas (et le fichier de données s'il y a lieu) dans le système de fichiers [[Project_layout/fr|/project]] ou [[Storage_and_file_management/fr#Types_de_stockage|/scratch]] de la grappe. Quand les fichiers sont exportés, sauvegardez-les avec des noms plus faciles à repérer que <code>FFF-1.*</code> ou renommez-les au téléversement.
# Créez un fichier de journalisation dont le but est de charger les fichiers de cas (et le fichier de données s'il y a lieu), lancez le solveur et enregistrez les résultats. Voyez les exemples ci-dessous et n'oubliez pas d'ajuster les noms des fichiers et le nombre d'itérations. 
# S'il arrive fréquemment que les tâches ne démarrent pas en raison d'un manque de licence (et que de les soumettre de nouveau manuellement ne convient pas), vous pouvez modifier votre script pour que votre tâche soit remise en file d'attente (au plus 4 fois) comme c'est le cas pour le script sous l'onglet <i>Plusieurs nœuds (par cœur + remise en attente)</i> plus loin. Cependant, ceci remet aussi en attente les simulations qui ont échoué pour d'autres raisons que l'absence de licence (par exemple la divergence), gaspillant ainsi du temps de calcul. Il est donc fortement recommandé de vérifier les fichiers de sortie de l'ordonnanceur pour savoir si chaque tentative de remise en attente est ou non due à un problème de licence. Si vous découvrez que la remise en attente est due à un problème avec la simulation, annulez immédiatement la tâche avec <code>scancel jobid</code> et corrigez le problème.
# Lorsque la  [[Running_jobs/fr|tâche est terminée]], vous pouvez télécharger le fichier de données et le retourner dans Fluent avec <i>File > Import > Data…</i>.

<span id="Slurm_scripts"></span>
=== Scripts pour l'ordonnanceur Slurm ===

<span id="General_purpose"></span>
==== Utilisation générale ====

<div class="mw-translate-fuzzy">
La plupart des tâches Fluent devraient utiliser le script <i>par nœud</i> ci-dessous pour minimiser le temps d'attente et maximiser la performance en utilisant le moins de nœuds possible. Les tâches demandant beaucoup de cœurs CPU pourraient attendre moins longtemps dans la queue avec le script <i>par cœur</i>, mais le démarrage d’une tâche utilisant plusieurs nœuds peut prendre beaucoup plus de temps, ce qui en diminue l'intérêt. Il faut aussi tenir compte du fait qu'exécuter des tâches intensives sur un nombre indéterminé de nœuds pouvant être très élevé fait en sorte que ces tâches seront beaucoup plus susceptibles de planter si un des nœuds de calcul fait défaut pendant la simulation. Les scripts suivants utilisent la mémoire partagée pour les tâches utilisant un seul nœud et la mémoire distribuée (avec MPI et l’interconnexion CHP appropriée) pour les tâches en utilisant plusieurs.

Les deux onglets pour Narval peuvent fournir une alternative plus robuste si Fluent plante pendant la phase initiale de partitionnement automatique du maillage lors de l'utilisation des scripts Intel standards avec le solveur parallèle. L'autre option serait d'effectuer manuellement le partitionnement du maillage dans l'interface graphique de Fluent, puis d'essayer d'exécuter à nouveau la tâche sur la grappe avec les scripts Intel. Ainsi, vous pouvez inspecter les statistiques de partitionnement et spécifier la méthode pour obtenir un résultat optimal. Le nombre de partitions de maillage doit être un multiple entier du nombre de cœurs; pour une efficacité optimale, assurez-vous d'avoir au moins 10&nbsp;000 cellules par cœur.
</div>

<tabs>

<tab name="Plusieurs nœuds (par nœud)">
{{File
|name=script-flu-bynode-intel.sh
|lang="bash"
|contents=
#!/bin/bash

<div class="mw-translate-fuzzy">
#SBATCH --account=def-group   # Specify account name
#SBATCH --time=00-03:00       # Specify time limit dd-hh:mm
#SBATCH --nodes=1             # Specify number of compute nodes (narval 1 node max)
#SBATCH --ntasks-per-node=32  # Specify number of cores per node (graham 32 or 44, cedar 48, beluga 40, narval 64, or less)
#SBATCH --mem=0               # Do not change (allocates all memory per compute node)
#SBATCH --cpus-per-task=1     # Do not change
</div>

<div class="mw-translate-fuzzy">
module load StdEnv/2023       # Do not change
module load ansys/2023R2      # or newer versions (beluga, cedar, graham, narval)
</div>

MYJOURNALFILE=sample.jou      # Specify your journal file name
MYVERSION=3d                  # Specify 2d, 2ddp, 3d or 3ddp

# ------- do not change any lines below --------

if [[ "$CC_CLUSTER" == narval ]]; then
 module load intel/2023 intelmpi
 export INTELMPI_ROOT=$I_MPI_ROOT
 unset I_MPI_ROOT
fi

if [[ ("${EBVERSIONANSYS//R*}" -ge 2025 && "${CC_CLUSTER}" == nibi) || "${CC_CLUSTER}" == narval ]]; then
 export I_MPI_HYDRA_BOOTSTRAP=ssh
 unset I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS
fi

slurm_hl2hl.py --format ANSYS-FLUENT > /tmp/machinefile-$SLURM_JOB_ID
NCORES=$SLURM_NTASKS

if [ "$SLURM_NNODES" == 1 ]; then
 fluent -g $MYVERSION -t $NCORES -mpi=intel -pshmem -i $MYJOURNALFILE
else
 if [[ "${CC_CLUSTER}" == nibi ]]; then
   fluent -g $MYVERSION -t $NCORES -mpi=intel -peth -cnf=/tmp/machinefile-$SLURM_JOB_ID -i $MYJOURNALFILE
 else
   fluent -g $MYVERSION -t $NCORES -mpi=intel -pib -cnf=/tmp/machinefile-$SLURM_JOB_ID -i $MYJOURNALFILE
 fi
fi
}}
</tab>

<tab name="Plusieurs nœuds (par cœur)">
{{File
|name=script-flu-bycore-intel.sh
|lang="bash"
|contents=
#!/bin/bash

<div class="mw-translate-fuzzy">
#SBATCH --account=def-group   # Specify account
#SBATCH --time=00-03:00       # Specify time limit dd-hh:mm
##SBATCH --nodes=1            # Uncomment to specify (narval 1 node max)
#SBATCH --ntasks=16           # Specify total number of cores for all nodes
#SBATCH --mem-per-cpu=4G      # Specify memory per core
#SBATCH --cpus-per-task=1     # Do not change
</div>

<div class="mw-translate-fuzzy">
module load StdEnv/2023       # Do not change
module load ansys/2023R2      # or newer versions (beluga, cedar, graham, narval)
</div>

MYJOURNALFILE=sample.jou      # Specify your journal file name
MYVERSION=3d                  # Specify 2d, 2ddp, 3d or 3ddp

# ------- do not change any lines below --------

if [[ "$CC_CLUSTER" == narval ]]; then
 module load intel/2023 intelmpi
 export INTELMPI_ROOT=$I_MPI_ROOT
 unset I_MPI_ROOT
fi

if [[ ("${EBVERSIONANSYS//R*}" -ge 2025 && "${CC_CLUSTER}" == nibi) || "${CC_CLUSTER}" == narval ]]; then
 export I_MPI_HYDRA_BOOTSTRAP=ssh
 unset I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS
fi

slurm_hl2hl.py --format ANSYS-FLUENT > /tmp/machinefile-$SLURM_JOB_ID
NCORES=$SLURM_NTASKS

if [ "$SLURM_NNODES" == 1 ]; then
 fluent -g $MYVERSION -t $NCORES -mpi=intel -pshmem -i $MYJOURNALFILE
else
 if [[ "${CC_CLUSTER}" == nibi ]]; then
   fluent -g $MYVERSION -t $NCORES -mpi=intel -peth -cnf=/tmp/machinefile-$SLURM_JOB_ID -i $MYJOURNALFILE
 else
   fluent -g $MYVERSION -t $NCORES -mpi=intel -pib -cnf=/tmp/machinefile-$SLURM_JOB_ID -i $MYJOURNALFILE
 fi
fi
}}
</tab>

<tab name="Plusieurs nœuds (par nœud, Narval)">
{{File
|name=script-flu-bynode-openmpi.sh
|lang="bash"
|contents=
#!/bin/bash

<div class="mw-translate-fuzzy">
#SBATCH --account=def-group   # Specify account name
#SBATCH --time=00-03:00       # Specify time limit dd-hh:mm
#SBATCH --nodes=1             # Specify number of compute nodes
#SBATCH --ntasks-per-node=64  # Specify number of cores per node (narval 64 or less)
#SBATCH --mem=0               # Do not change (allocates all memory per compute node)
#SBATCH --cpus-per-task=1     # Do not change
</div>

<div class="mw-translate-fuzzy">
module load StdEnv/2023       # Do not change
module load ansys/2023R2      # or newer versions (narval only)
</div>

MYJOURNALFILE=sample.jou      # Specify your journal file name
MYVERSION=3d                  # Specify 2d, 2ddp, 3d or 3ddp

# ------- do not change any lines below --------

export OPENMPI_ROOT=$EBROOTOPENMPI
slurm_hl2hl.py --format ANSYS-FLUENT > /tmp/mf-$SLURM_JOB_ID
for i in `cat /tmp/mf-$SLURM_JOB_ID {{!}} uniq`; do echo "${i}:$(cat /tmp/mf-$SLURM_JOB_ID {{!}} grep $i {{!}} wc -l)" >> /tmp/machinefile-$SLURM_JOB_ID; done
NCORES=$SLURM_NTASKS

if [ "$SLURM_NNODES" == 1 ]; then
 fluent -g $MYVERSION -t $NCORES -mpi=openmpi -pshmem -i $MYJOURNALFILE
else
 export FI_PROVIDER=verbs
 fluent -g $MYVERSION -t $NCORES -mpi=openmpi -pib -cnf=/tmp/machinefile-$SLURM_JOB_ID -i $MYJOURNALFILE
fi
}}
</tab>

<tab name="Plusieurs nœuds (par cœur, Narval)">
{{File
|name=script-flu-bycore-openmpi.sh
|lang="bash"
|contents=
#!/bin/bash

<div class="mw-translate-fuzzy">
#SBATCH --account=def-group   # Specify account name
#SBATCH --time=00-03:00       # Specify time limit dd-hh:mm
##SBATCH --nodes=1            # Uncomment to specify number of compute nodes (optional)
#SBATCH --ntasks=16           # Specify total number of cores
#SBATCH --mem-per-cpu=4G      # Specify memory per core
#SBATCH --cpus-per-task=1     # Do not change
</div>

<div class="mw-translate-fuzzy">
module load StdEnv/2023       # Do not change     
module load ansys/2023R2      # or newer versions (narval only)
</div>

MYJOURNALFILE=sample.jou      # Specify your journal file name
MYVERSION=3d                  # Specify 2d, 2ddp, 3d or 3ddp

# ------- do not change any lines below --------

export OPENMPI_ROOT=$EBROOTOPENMPI
slurm_hl2hl.py --format ANSYS-FLUENT > /tmp/mf-$SLURM_JOB_ID
for i in `cat /tmp/mf-$SLURM_JOB_ID {{!}} uniq`; do echo "${i}:$(cat /tmp/mf-$SLURM_JOB_ID {{!}} grep $i {{!}} wc -l)" >> /tmp/machinefile-$SLURM_JOB_ID; done
NCORES=$SLURM_NTASKS

if [ "$SLURM_NNODES" == 1 ]; then
 fluent -g $MYVERSION -t $NCORES -mpi=openmpi -pshmem -i $MYJOURNALFILE
else
 export FI_PROVIDER=verbs
 fluent -g $MYVERSION -t $NCORES -mpi=openmpi -pib -cnf=/tmp/machinefile-$SLURM_JOB_ID -i $MYJOURNALFILE
fi
}}
</tab>

<div class="mw-translate-fuzzy">
<tab name="Plusieurs nœuds (par nœud, Niagara)">
{{File
|name=script-flu-bynode-intel-nia.sh
|lang="bash"
|contents=
#!/bin/bash
</div>

<div class="mw-translate-fuzzy">
#SBATCH --account=def-group      # Specify account name
#SBATCH --time=00-03:00          # Specify time limit dd-hh:mm
#SBATCH --nodes=2                # Specify number of compute nodes
#SBATCH --ntasks-per-node=80     # Specify number cores per node (niagara 80 or less)
#SBATCH --mem=0                  # Do not change (allocate all memory per compute node)
#SBATCH --cpus-per-task=1        # Do not change (required parameter)
</div>

<div class="mw-translate-fuzzy">
module load CCEnv StdEnv/2023    # Do not change
module load arch/avx512
module load ansys/2023R2         # or newer versions (niagara only)
</div>

MYJOURNALFILE=sample.jou         # Specify your journal file name
MYVERSION=3d                     # Specify 2d, 2ddp, 3d or 3ddp

<div class="mw-translate-fuzzy">
# These settings are used instead of your ~/.licenses/ansys.lic
LICSERVER=license3.sharcnet.ca   # Specify license server hostname
FLEXPORT=1055                    # Specify server flex port
INTEPORT=2325                    # Specify server interconnect port
VENDPORT=1793                    # Specify server vendor port
</div>

# ------- do not change any lines below --------

ssh tri-gw -fNL $FLEXPORT:$LICSERVER:$FLEXPORT
ssh tri-gw -fNL $VENDPORT:$LICSERVER:$VENDPORT
export ANSYSLMD_LICENSE_FILE=$FLEXPORT@localhost
export ANSYSLI_SERVERS=$INTEPORT@localhost

slurm_hl2hl.py --format ANSYS-FLUENT > $SLURM_SUBMIT_DIR/machinefile-$SLURM_JOB_ID
NCORES=$SLURM_NTASKS

<div class="mw-translate-fuzzy">
if [ ! -L "$HOME/.ansys" ]; then
  echo "ERROR: A link to a writable .ansys directory does not exist."
  echo 'Remove ~/.ansys if one exists and then run: ln -s $SCRATCH/.ansys ~/.ansys'
  echo "Then try submitting your job again. Aborting the current job now!"
elif [ ! -L "$HOME/.fluentconf" ]; then
  echo "ERROR: A link to a writable .fluentconf directory does not exist."
  echo 'Remove ~/.fluentconf if one exists and run: ln -s $SCRATCH/.fluentconf ~/.fluentconf'
  echo "Then try submitting your job again. Aborting the current job now!"
elif [ ! -L "$HOME/.flrecent" ]; then
  echo "ERROR: A link to a writable .flrecent file does not exist."
  echo 'Remove ~/.flrecent if one exists and then run: ln -s $SCRATCH/.flrecent ~/.flrecent'
  echo "Then try submitting your job again. Aborting the current job now!"
else
  mkdir -pv $SCRATCH/.ansys
  mkdir -pv $SCRATCH/.fluentconf
  touch $SCRATCH/.flrecent
  if [ "$SLURM_NNODES" == 1 ]; then
   fluent -g $MYVERSION -t $NCORES -affinity=0 -mpi=intel -pshmem -i $MYJOURNALFILE
  else
   fluent -g $MYVERSION -t $NCORES -affinity=0 -mpi=intel -ssh -pib -cnf=$SLURM_SUBMIT_DIR/machinefile-$SLURM_JOB_ID -i $MYJOURNALFILE
  fi
fi
}}
</tab>
</div>

</tabs>

<span id="License_requeue"></span>
==== Remise en file d'attente pour obtenir la licence ====

Les scripts suivants ne doivent être utilisés qu'avec des tâches Fluent qui sont connues pour se terminer normalement sans générer d'erreurs en sortie, mais qui nécessitent généralement plusieurs tentatives de remise en file d'attente pour obtenir les licences. Ils ne sont pas recommandés pour les tâches Fluent qui peuvent 1) s'exécuter pendant une longue période avant de planter 2) s'exécuter jusqu'à la fin mais contenir des avertissements de journalisation; dans les deux cas, les simulations seront répétées depuis le début jusqu'à ce que le nombre maximal de tentatives de remise en file d'attente spécifié par la valeur <code>array</code> soit atteint. Pour ces types de tâches, les scripts à usage général (ci-dessus) doivent être utilisés.

<tabs>
<tab name="Plusieurs nœuds (par nœud + remise en attente)">
{{File
|name=script-flu-bynode+requeue.sh
|lang="bash"
|contents=
#!/bin/bash

<div class="mw-translate-fuzzy">
#SBATCH --account=def-group   # Specify account
#SBATCH --time=00-03:00       # Specify time limit dd-hh:mm
#SBATCH --nodes=1             # Specify number of compute nodes (narval 1 node max)
#SBATCH --ntasks-per-node=32  # Specify number of cores per node (graham 32 or 44, cedar 48, beluga 40, or less)
#SBATCH --mem=0               # Do not change (allocates all memory per compute node)
#SBATCH --cpus-per-task=1     # Do not change
#SBATCH --array=1-5%1         # Specify number of requeue attempts (2 or more, 5 is shown)
</div>

<div class="mw-translate-fuzzy">
module load StdEnv/2023       # Do not change
module load ansys/2023R2      # Specify version (beluga, cedar, graham, narval)
</div>

MYJOURNALFILE=sample.jou      # Specify your journal file name
MYVERSION=3d                  # Specify 2d, 2ddp, 3d or 3ddp

# ------- do not change any lines below --------

if [[ "$CC_CLUSTER" == narval ]]; then
 module load intel/2023 intelmpi
 export INTELMPI_ROOT=$I_MPI_ROOT
 unset I_MPI_ROOT
fi

if [[ ("${EBVERSIONANSYS//R*}" -ge 2025 && "${CC_CLUSTER}" == nibi) || "${CC_CLUSTER}" == narval ]]; then
 export I_MPI_HYDRA_BOOTSTRAP=ssh
 unset I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS
fi

slurm_hl2hl.py --format ANSYS-FLUENT > /tmp/machinefile-$SLURM_JOB_ID
NCORES=$SLURM_NTASKS

<div class="mw-translate-fuzzy">
if [ "$SLURM_NNODES" == 1 ]; then
 #export I_MPI_HYDRA_BOOTSTRAP=ssh    # uncomment on beluga or cedar
 fluent -g $MYVERSION -t $NCORES -affinity=0 -mpi=intel -pshmem -i $MYJOURNALFILE
else
 fluent -g $MYVERSION -t $NCORES -affinity=0 -mpi=intel -pib -cnf=/tmp/machinefile-$SLURM_JOB_ID -i $MYJOURNALFILE
fi
if [ $? -eq 0 ]; then
    echo "Job completed successfully! Exiting now."
    scancel $SLURM_ARRAY_JOB_ID
else
    echo "Job attempt $SLURM_ARRAY_TASK_ID of $SLURM_ARRAY_TASK_COUNT failed due to license or simulation issue!"
    if [ $SLURM_ARRAY_TASK_ID -lt $SLURM_ARRAY_TASK_COUNT ]; then
       echo "Resubmitting job now …"
    else
       echo "All job attempts failed exiting now."
    fi
fi
}}
</tab>
</div>

<tab name="Plusieurs nœuds (par cœur + remise en attente)">
{{File
|name=script-flu-bycore+requeue.sh
|lang="bash"
|contents=
#!/bin/bash

#SBATCH --account=def-group   # Specify account
#SBATCH --time=00-03:00       # Specify time limit dd-hh:mm
##SBATCH --nodes=1            # Uncomment to specify (narval 1 node max) 
#SBATCH --ntasks=16           # Specify total number of cores
#SBATCH --mem-per-cpu=4G      # Specify memory per core
#SBATCH --cpus-per-task=1     # Do not change
#SBATCH --array=1-5%1         # Specify number of requeue attempts (2 or more, 5 is shown)

<div class="mw-translate-fuzzy">
module load StdEnv/2023       # Do not change
module load ansys/2023R2      # Specify version (beluga, cedar, graham, narval)
</div>

MYJOURNALFILE=sample.jou      # Specify your journal file name
MYVERSION=3d                  # Specify 2d, 2ddp, 3d or 3ddp

# ------- do not change any lines below --------

if [[ "$CC_CLUSTER" == narval ]]; then
 module load intel/2023 intelmpi
 export INTELMPI_ROOT=$I_MPI_ROOT
 unset I_MPI_ROOT
fi

if [[ ("${EBVERSIONANSYS//R*}" -ge 2025 && "${CC_CLUSTER}" == nibi) || "${CC_CLUSTER}" == narval ]]; then
 export I_MPI_HYDRA_BOOTSTRAP=ssh
 unset I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS
fi

slurm_hl2hl.py --format ANSYS-FLUENT > /tmp/machinefile-$SLURM_JOB_ID
NCORES=$SLURM_NTASKS

<div class="mw-translate-fuzzy">
if [ "$SLURM_NNODES" == 1 ]; then
 #export I_MPI_HYDRA_BOOTSTRAP=ssh    # uncomment on beluga or cedar
 fluent -g $MYVERSION -t $NCORES -affinity=0 -mpi=intel -pshmem -i $MYJOURNALFILE
else
 fluent -g $MYVERSION -t $NCORES -affinity=0 -mpi=intel -pib -cnf=/tmp/machinefile-$SLURM_JOB_ID -i $MYJOURNALFILE
fi
if [ $? -eq 0 ]; then
    echo "Job completed successfully! Exiting now."
    scancel $SLURM_ARRAY_JOB_ID
else
    echo "Job attempt $SLURM_ARRAY_TASK_ID of $SLURM_ARRAY_TASK_COUNT failed due to license or simulation issue!"
    if [ $SLURM_ARRAY_TASK_ID -lt $SLURM_ARRAY_TASK_COUNT ]; then
       echo "Resubmitting job now …"
    else
       echo "All job attempts failed exiting now."
    fi
fi
}}
</tab>
</tabs>
</div>

<span id="Solution_restart"></span>
==== Redémarrage ====

Les deux scripts suivants automatisent le redémarrage de tâches intensives qui exigent plus que le maximum de sept jours d'exécution permis sur la plupart des grappes. Le redémarrage se fait à partir des fichiers de valeur de pas de temps les plus récemment sauvegardés. Une exigence de base est que le premier pas puisse être terminé avant la fin du temps demandé dans le vecteur de tâches (défini dans le haut du script) quand une simulation est lancée à partir d'un champ initialisé. Nous supposons que la valeur du pas est fixe. Pour commencer, un groupe de <i>sample.cas</i>, <i>sample.dat</i> et <i>sample.jou</i> doit être présent. Modifiez le fichier <i>sample.jou</i> pour qu'il contienne <code>/solve/dual-time-iterate 1</code> et <code>/file/auto-save/data-frequency 1</code>. Créez ensuite un fichier de journalisation avec <code>cp sample.jou sample-restart.jou</code> et modifiez le fichier <i>sample-restart.jou</i> pour qu'il contienne <code>/file/read-cas-data sample-restart</code> plutôt que <code>/file/read-cas-data sample</code> et mettez en commentaire la ligne pour l'initialisation en la précédant d’un point-virgule, par exemple <code>;/solve/initialize/initialize-flow</code>. Si votre deuxième pas et les pas qui suivent sont exécutés deux fois plus vite que le pas initial, modifiez <i>sample-restart.jou</i> en spécifiant <code>/solve/dual-time-iterate 2</code>. De cette façon, la solution ne sera redémarrée qu'après que les deux pas suivant le pas initial soient terminés. Un fichier de résultats pour chaque pas sera enregistré dans le sous-répertoire de sortie. La valeur 2 est arbitraire, mais elle devrait être utilisée pour que la durée de deux pas soit moindre que la durée allouée au vecteur de tâches. Ceci limitera le nombre de redémarrages, ce qui consomme beaucoup de ressources. Si le premier pas de <i>sample.jou</i> est fait à partir d'une solution précédente, choisissez 1 plutôt que 2 puisque tous les pas auront probablement besoin du même temps d'exécution. En supposant que 2 est choisi, la durée totale de la simulation sera 1*Dt+2*Nrestart*Dt où Nrestart est le nombre de redémarrages défini dans le script Slurm. Le nombre total de pas (de même que le nombre de fichiers de résultats générés) sera ainsi 1+2*Nrestart. La valeur pour le temps demandé devrait être choisie afin que le pas initial et les pas suivants se terminent dans la fenêtre de temps de Slurm, qui peut aller jusqu'à <code>#SBATCH --time=07-00:00</code> jours.

<tabs>
<tab name="Plusieurs nœuds (par nœud + redémarrage)">
{{File
|name=script-flu-bynode+restart.sh
|lang="bash"
|contents=
#!/bin/bash

<div class="mw-translate-fuzzy">
#SBATCH --account=def-group   # Specify account
#SBATCH --time=07-00:00       # Specify time limit dd-hh:mm
#SBATCH --nodes=1             # Specify number of compute nodes (narval 1 node max)
#SBATCH --ntasks-per-node=32  # Specify number of cores per node (graham 32 or 44, cedar 48, beluga 40, narval 64, or less)
#SBATCH --mem=0               # Do not change (allocates all memory per compute node)
#SBATCH --cpus-per-task=1     # Do not change
#SBATCH --array=1-5%1         # Specify number of solution restarts (2 or more, 5 is shown)
</div>

<div class="mw-translate-fuzzy">
module load StdEnv/2023       # Do not change
module load ansys/2023R2      # Specify version (beluga, cedar, graham, narval)
</div>

MYVERSION=3d                        # Specify 2d, 2ddp, 3d or 3ddp
MYJOUFILE=sample.jou                # Specify your journal filename
MYJOUFILERES=sample-restart.jou     # Specify journal restart filename
MYCASFILERES=sample-restart.cas.h5  # Specify cas restart filename
MYDATFILERES=sample-restart.dat.h5  # Specify dat restart filename

# ------- do not change any lines below --------

if [[ "$CC_CLUSTER" == narval ]]; then
 module load intel/2023 intelmpi
 export INTELMPI_ROOT=$I_MPI_ROOT
 unset I_MPI_ROOT
fi

if [[ ("${EBVERSIONANSYS//R*}" -ge 2025 && "${CC_CLUSTER}" == nibi) || "${CC_CLUSTER}" == narval ]]; then
 export I_MPI_HYDRA_BOOTSTRAP=ssh
 unset I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS
fi

slurm_hl2hl.py --format ANSYS-FLUENT > /tmp/machinefile-$SLURM_JOB_ID
NCORES=$SLURM_NTASKS

<div class="mw-translate-fuzzy">
# Specify 2d, 2ddp, 3d or 3ddp and replace sample with your journal filename …
if [ "$SLURM_NNODES" == 1 ]; then
  #export I_MPI_HYDRA_BOOTSTRAP=ssh    # uncomment on beluga or cedar
  if [ "$SLURM_ARRAY_TASK_ID" == 1 ]; then
    fluent -g 2ddp -t $NCORES -affinity=0 -i $MYJOUFILE
  else
    fluent -g 2ddp -t $NCORES -affinity=0 -i $MYJOUFILERES
  fi
else 
  if [ "$SLURM_ARRAY_TASK_ID" == 1 ]; then
    fluent -g 2ddp -t $NCORES -affinity=0 -cnf=/tmp/machinefile-$SLURM_JOB_ID -mpi=intel -ssh -i $MYJOUFILE
  else
    fluent -g 2ddp -t $NCORES -affinity=0 -cnf=/tmp/machinefile-$SLURM_JOB_ID -mpi=intel -ssh -i $MYJOUFILERES
  fi
fi
if [ $? -eq 0 ]; then
    echo
    echo "SLURM_ARRAY_TASK_ID  = $SLURM_ARRAY_TASK_ID"
    echo "SLURM_ARRAY_TASK_COUNT = $SLURM_ARRAY_TASK_COUNT"
    echo
    if [ $SLURM_ARRAY_TASK_ID -lt $SLURM_ARRAY_TASK_COUNT ]; then
      echo "Restarting job with the most recent output dat file …"
      ln -sfv output/$(ls -ltr output {{!}} grep .cas {{!}} tail -n1 {{!}} awk '{print $9}') $MYCASFILERES
      ln -sfv output/$(ls -ltr output {{!}} grep .dat {{!}} tail -n1 {{!}} awk '{print $9}') $MYDATFILERES
      ls -lh cavity* output/*
    else
      echo "Job completed successfully! Exiting now."
      scancel $SLURM_ARRAY_JOB_ID
     fi
else
     echo "Simulation failed. Exiting …"
fi
}}
</tab>
</div>

<tab name="Plusieurs nœuds (par cœur + redémarrage)">
{{File
|name=script-flu-bycore+restart.sh
|lang="bash"
|contents=
#!/bin/bash

#SBATCH --account=def-group   # Specify account
#SBATCH --time=00-03:00       # Specify time limit dd-hh:mm
##SBATCH --nodes=1            # Uncomment to specify (narval 1 node max)
#SBATCH --ntasks=16           # Specify total number of cores
#SBATCH --mem-per-cpu=4G      # Specify memory per core
#SBATCH --cpus-per-task=1     # Do not change
#SBATCH --array=1-5%1         # Specify number of restart aka time steps (2 or more, 5 is shown)

<div class="mw-translate-fuzzy">
module load StdEnv/2023       # Do not change
module load ansys/2023R2      # Specify version (beluga, cedar, graham, narval)
</div>

MYVERSION=3d                        # Specify 2d, 2ddp, 3d or 3ddp
MYJOUFILE=sample.jou                # Specify your journal filename
MYJOUFILERES=sample-restart.jou     # Specify journal restart filename
MYCASFILERES=sample-restart.cas.h5  # Specify cas restart filename
MYDATFILERES=sample-restart.dat.h5  # Specify dat restart filename

# ------- do not change any lines below --------

if [[ "$CC_CLUSTER" == narval ]]; then
 module load intel/2023 intelmpi
 export INTELMPI_ROOT=$I_MPI_ROOT
 unset I_MPI_ROOT
fi

if [[ ("${EBVERSIONANSYS//R*}" -ge 2025 && "${CC_CLUSTER}" == nibi) || "${CC_CLUSTER}" == narval ]]; then
 export I_MPI_HYDRA_BOOTSTRAP=ssh
 unset I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS
fi

slurm_hl2hl.py --format ANSYS-FLUENT > /tmp/machinefile-$SLURM_JOB_ID
NCORES=$SLURM_NTASKS

<div class="mw-translate-fuzzy">
if [ "$SLURM_NNODES" == 1 ]; then
  #export I_MPI_HYDRA_BOOTSTRAP=ssh    # uncomment on beluga or cedar
  if [ "$SLURM_ARRAY_TASK_ID" == 1 ]; then
    fluent -g $MYVERSION -t $NCORES -affinity=0 -mpi=intel -pshmem -I $MYFILEJOU
  else
    fluent -g $MYVERSION -t $NCORES -affinity=0 -mpi=intel -pshmem -I $MYFILEJOURES
  fi
else 
  if [ "$SLURM_ARRAY_TASK_ID" == 1 ]; then
    fluent -g $MYVERSION -t $NCORES -affinity=0 -mpi=intel -pib -cnf=/tmp/machinefile-$SLURM_JOB_ID -i $MYJOUFILE
  else
    fluent -g $MYVERSION -t $NCORES -affinity=0 -mpi=intel -pib -cnf=/tmp/machinefile-$SLURM_JOB_ID -i $MYJOUFILERES
  fi
fi
if [ $? -eq 0 ]; then
    echo
    echo "SLURM_ARRAY_TASK_ID  = $SLURM_ARRAY_TASK_ID"
    echo "SLURM_ARRAY_TASK_COUNT = $SLURM_ARRAY_TASK_COUNT"
    echo
    if [ $SLURM_ARRAY_TASK_ID -lt $SLURM_ARRAY_TASK_COUNT ]; then
      echo "Restarting job with the most recent output dat file"
      ln -sfv output/$(ls -ltr output {{!}} grep .cas {{!}} tail -n1 {{!}} awk '{print $9}') $MYCASFILERES
      ln -sfv output/$(ls -ltr output {{!}} grep .dat {{!}} tail -n1 {{!}} awk '{print $9}') $MYDATFILERES
      ls -lh cavity* output/*
    else
      echo "Job completed successfully! Exiting now."
      scancel $SLURM_ARRAY_JOB_ID
     fi
else
     echo "Simulation failed. Exiting now."
fi
}}
</tab>
</tabs>
</div>

<span id="Journal_files"></span>
=== Fichiers de journalisation ===

Les fichiers de journalisation peuvent contenir toutes les commandes de l'interface TUI (''Text User Interface'') de Fluent; elles peuvent être utilisées pour modifier des paramètres de simulation comme la température, la pression ou la vitesse du flux. Vous pouvez ainsi effectuer une série de simulations sous différentes conditions simplement en modifiant les paramètres du fichier de journalisation. Consultez le guide d'utilisation de Fluent pour plus d'information ainsi que pour connaître la liste des commandes. Les fichiers qui suivent sont configurés avec <code>/file/cff-file no</code> pour utiliser les formats de fichiers .cas/.dat qui sont les formats par défaut pour les modules jusqu'à 2019R3. Pour utiliser les formats .cas.h5/.dat.h5 plus efficaces des versions à partir de 2020R1, la configuration est 
<code>/file/cff-files yes</code>.

<tabs>
<tab name="Fichier de journalisation (stable, cas)">
{{File
|name=sample1.jou
|contents=
; SAMPLE FLUENT JOURNAL FILE - STEADY SIMULATION
; ----------------------------------------------
; lines beginning with a semicolon are comments

; Overwrite files by default
/file/confirm-overwrite no

; Preferentially read/write files in legacy format
/file/cff-files no

; Read input case and data files
/file/read-case-data FFF-in

; Run the solver for this many iterations
/solve/iterate 1000

; Overwrite output files by default
/file/confirm-overwrite n

; Write final output data file
/file/write-case-data FFF-out

; Write simulation report to file (optional)
/report/summary y "My_Simulation_Report.txt"

; Cleanly shutdown fluent
/exit
}}
</tab>

<tab name="Fichier de journalisation (stable, cas + données)">
{{File
|name=sample2.jou
|contents=
; EXEMPLE DE FICHIER DE JOURNALISATION - SIMULATION STABLE
; ----------------------------------------------
; le point-virgule en début de ligne signale un commentaire

; Overwrite files by default
/file/confirm-overwrite no

; Preferentially read/write files in legacy format
/file/cff-files no

; Read input files
/file/read-case-data FFF-in

; Write a data file every 100 iterations
/file/auto-save/data-frequency 100

; Retain data files from 5 most recent iterations
/file/auto-save/retain-most-recent-files y

; Write data files to output sub-directory (appends iteration)
/file/auto-save/root-name output/FFF-out

; Run the solver for this many iterations
/solve/iterate 1000

; écrire le dernier fichier de cas et de données en sortie
/file/write-case-data FFF-out

; enregistrer le rapport de la simulation (optionnel)
/report/summary y "My_Simulation_Report.txt"

; fermez correctement Fluent
exit
}}
</tab>

<tab name="Fichier de journalisation (temporaire)">
{{File
|name=sample3.jou
|contents=
; EXEMPLE DE FICHIER DE JOURNALISATION - SIMULATION TEMPORAIRE
; ----------------------------------------------
; le point-virgule en début de ligne signale un commentaire

; Overwrite files by default
/file/confirm-overwrite no

; Preferentially read/write files in legacy format
/file/cff-files no

; Read the input case file
/file/read-case FFF-transient-inp

; For continuation (restart) read in both case and data input files
;/file/read-case-data FFF-transient-inp

; Write a data (and maybe case) file every 100 time steps
/file/auto-save/data-frequency 100
/file/auto-save/case-frequency if-case-is-modified

; Retain only the most recent 5 data (and maybe case) files
/file/auto-save/retain-most-recent-files y

; Write to output sub-directory (appends flowtime and timestep)
/file/auto-save/root-name output/FFF-transient-out-%10.6f

; ##### Settings for Transient simulation :  #####

; Set the physical time step size
/solve/set/time-step 0.0001

; Set the number of iterations for which convergence monitors are reported
/solve/set/reporting-interval 1

; ##### Fin des paramètres #####

; initialiser avec la méthode hybride
/solve/initialize/hyb-initialization

; indiquer le nombre maximal d'itérations par pas et le nombre de pas
;/solve/set/max-iterations-per-time-step 75
;/solve/dual-time-iterate 1000 ,
/solve/dual-time-iterate 1000 75

; enregistrer les derniers fichiers en sortie pour les cas et les données
/file/write-case-data FFF-transient-out

; enregistrer le rapport de la simulation (optionnel)
/report/summary y Report_Transient_Simulation.txt

; Cleanly shutdown fluent
/exit
}}
</tab>

</tabs>

<span id="UDFs"></span>
=== Fonctions UDF ===

La première étape est de transférer vers la grappe votre UDF (<i> User-Defined Function</i>), soit le fichier source sampleudf.c et tous les fichiers de dépendance supplémentaires. Lors du téléchargement à partir d'une machine Windows, assurez-vous que le mode texte de votre client de transfert est utilisé, sinon Fluent ne pourra pas lire correctement le fichier sur la grappe qui elle exécute Linux. L'UDF doit être placée dans le répertoire où résident vos fichiers de journalisation, cas et dat. Ajoutez ensuite l'une des commandes suivantes dans votre fichier de journalisation avant les commandes qui lisent vos fichiers de simulation cas/dat. Que vous utilisiez l'approche UDF interprétée ou compilée, avant de télécharger votre fichier de cas, vérifiez que les boîtes de dialogue <i>Interpreted UDFs</i> et <i>UDF Library Manager</i> ne sont pas configurées pour utiliser un UDF; ceci garantira que lorsque les tâches sont soumises, seules les commandes du fichier de journalisation auront le contrôle.

<span id="Interpreted"></span>
==== Interpreté ====

Pour indiquer à Fluent d'interpréter votre UDF au moment de l'exécution, ajoutez la ligne de commande suivante dans votre fichier journal avant que les fichiers cas/dat ne soient lus ou initialisés. Remplacez le nom de fichier sampleudf.c par le nom de votre fichier source. La commande reste la même, que la simulation soit exécutée séquentiellement ou en parallèle. Pour vous assurer que l'UDF se trouve dans le même répertoire que le fichier de journalisation, ouvrez votre fichier cas dans l'interface graphique Fluent, supprimez toutes les définitions gérées et réenregistrez-le. Cela garantira que seule la commande/méthode suivante est en contrôle lors de l'exécution de Fluent. Pour utiliser une UDF interprétée avec des tâches parallèles, elle devra être parallélisée comme décrit dans la section ci-dessous.

 define/user-defined/interpreted-functions "sampleudf.c" "cpp" 10000 no

<span id="Compiled"></span>
==== Compilé ====

Pour utiliser cette approche, votre UDF doit être compilée sur une de nos grappes au moins une fois. Cela créera une structure de sous-répertoire libudf contenant la bibliothèque partagée <code>libudf.so</code> requise. Le répertoire libudf ne peut pas être simplement copié d'un système distant (comme votre ordinateur portable) vers l'Alliance car les dépendances de la bibliothèque partagée ne seront pas satisfaites, ce qui fera planter Fluent au démarrage. Cela dit, une fois que vous avez compilé votre UDF sur une de nos grappes, vous pouvez transférer la libudf nouvellement créée vers n'importe quel autre de nos grappes, à condition que votre compte charge la même version du module d'environnement StdEnv. Une fois copiée, l'UDF peut être utilisée en supprimant le commentaire de la deuxième ligne (load) libudf ci-dessous dans votre fichier de journalisation quand une tâche est soumise. Les deux lignes libudf (compile et load) ne doivent pas être laissées sans commentaire lors de la soumission de tâches, sinon votre UDF sera automatiquement (re)compilée pour chaque tâche. Non seulement cette méthode est très inefficace, mais elle peut également entraîner des conflits de build de type « racetime » si plusieurs tâches sont exécutées à partir du même répertoire. Outre la configuration de votre fichier de journalisation pour construire votre UDF, l'interface graphique de Fluent (exécutée sur n'importe quel nœud de calcul ou sur gra-vdi) peut également être utilisée. Pour ce faire, ajoutez le fichier source UDF dans la boîte de dialogue <i>Compiled UDFs</i>, et cliquez sur <i>Build</i>. Lorsque vous utilisez une UDF compilée avec des tâches parallèles, votre fichier source doit être parallélisé comme indiqué dans la section ci-dessous.

 define/user-defined/compiled-functions compile libudf yes sampleudf.c "" ""

et/ou

 define/user-defined/compiled-functions load libudf

<span id="Parallel"></span>
==== Parallèle ====

Avant qu'une UDF puisse être utilisée avec une tâche parallèle Fluent (SMP à nœud unique et MPI à nœuds multiples), elle doit être parallélisée. En procédant ainsi, nous contrôlons comment/quels processus (hôte et/ou calcul) exécutent des parties spécifiques du code UDF lorsque Fluent est exécuté en parallèle sur la grappe. La procédure d'instrumentation consiste à ajouter des directives de compilation, des prédicats et des macros de réduction dans votre UDF séquentielle. Si vous ne le faites pas, Fluent fonctionnera lentement au mieux ou plantera immédiatement au pire. Le résultat final sera une UDF unique qui s'exécute efficacement lorsque Fluent est utilisé à la fois en mode séquentiel et en mode parallèle. Le sujet est décrit en détail dans <I>Fluent Customization Manual, Part I: Chapter 7: Parallel Considerations</I> qui se trouve dans la [[Ansys/fr#Documentation_en_ligne|Documentation en ligne]].

==== DPM ====
Les UDF peuvent être utilisées pour personnaliser les modèles de phase discrète (DPM pour <i>Discrete Phase Models</i>) comme décrit dans <I>2024R2 Fluent Users Guide, Part III: Solution Mode, Chapter 24: Modeling Discrete Phase, 24.2 Steps for Using the Discrete Phase Models,</i> et dans <i>2024R2 Fluent Customization Manual, Part I: Creating and Using User Defined Functions, Chapter 2: DEFINE Macros, 2.5 Discrete Phase Model (DPM) DEFINE Macros</I>. Avant qu'une UDF basée sur DMP puisse être utilisée dans une simulation, l'injection d'un ensemble de particules doit être définie en spécifiant des <i>Point Properties</i> avec des variables telles que la position de la source, la trajectoire initiale, le débit massique, la durée, la température, etc., en fonction du type d'injection. Cela peut être fait dans l'interface graphique en cliquant sur le panneau <I>Physics--> Discrete Phase</I>, puis en cliquant sur le bouton <I>Injections</I>. Cela ouvrira la boîte de dialogue <I>Injections</I> dans laquelle une ou plusieurs injections peuvent être créées en cliquant sur le bouton <I>Create</I>. La boîte de dialogue <i>Set Injection Properties</i> contient le menu déroulant <i>Injection Type</i> avec les quatre premiers types disponibles (<i>single, group, surface, flat-fan-atomizer</i>). Si vous sélectionnez l'un de ces types, vous pouvez alors sélectionner l'onglet <i>Point Properties</i> pour saisir les champs de valeurs correspondants. Une autre façon de spécifier les <i>Point Properties</i> serait de lire un fichier texte d'injection. Pour ce faire, sélectionnez <i>File</i> dans le menu déroulant <i>Injection Type</i>, spécifiez le nom de l'injection à créer, puis cliquez sur le bouton <i>File</i> (situé à côté du bouton <I>OK</I> en bas de la boîte de dialogue <i>Set Injection Properties</i>). Ici, vous pouvez sélectionner un fichier d'échantillon d'injection (avec l'extension .dpm) ou un fichier texte d'injection créé manuellement. Pour ce faire, dans la boîte de dialogue <i>Select File</i>, sélectionnez <i>All Files (*)</i>, puis mettez en surbrillance le fichier qui pourrait avoir n'importe quel nom arbitraire mais qui a généralement une extension .inj; cliquez sur le bouton OK. En supposant qu'il n'y ait aucun problème avec le fichier, aucun message d'erreur ou d'avertissement de la console n'apparaîtra dans Fluent. Lorsque vous serez retourné à la boîte de dialogue  <i>Injection</i>, vous devriez voir le même nom d'injection que celui que vous avez spécifié dans la boîte de dialogue <i>Set Injection Properties</i> et pouvoir répertorier ses particules et propriétés dans la console. Ouvrez ensuite la boîte de dialogue <i>Discrete Phase Model</i> et sélectionnez <i>Interaction with Continuous Phase</i> qui permettra de mettre à jour les termes sources DPM à chaque itération de flux. Ce paramètre peut être enregistré dans votre fichier cas ou ajouté via le fichier de journalisation comme indiqué. Une fois que l'injection est confirmée comme fonctionnant dans l'interface graphique, les étapes peuvent être automatisées en ajoutant des commandes au fichier de journalisation après l'initialisation de la solution, par exemple :
/define/models/dpm/interaction/coupled-calculations yes
/define/models/dpm/injections/delete-injection injection-0:1
/define/models/dpm/injections/create injection-0:1 no yes file no zinjection01.inj no no no no
/define/models/dpm/injections/list-particles injection-0:1
/define/models/dpm/injections/list-injection-properties injection-0:1
où un format de fichier stable d'injection de base créé manuellement pourrait ressembler à :
$ cat zinjection01.inj
(z=4 12)
( x y z u v w diamètre t débit massique fréquence massique temps nom )
(( 2.90e-02 5.00e-03 0.0 -1,00e-03 0,0 0,0 1,00e-04 2,93e+02 1,00e-06 0,0 0,0 0,0 ) injection-0:1 )
notant que les fichiers d'injection pour les simulations DPM sont généralement configurés pour un suivi stationnaire ou instable de particules, le format du premier étant décrit dans <I>2024R2 Fluent Customization Manual, Part III: Solution Mode | Chapter 24: Modeling Discrete Phase | 24.3. Setting Initial Conditions for the Discrete Phase | 24.3.13 Point Properties for File Injections | 24.3.13.1 Steady File Format</I>.

<span id="CFX"></span>
<div class="mw-translate-fuzzy">
== Ansys CFX ==
</div>

<span id="Slurm_scripts"></span>
=== Scripts pour l'ordonnanceur Slurm ===

Le résumé des options de ligne de commande peut être affiché avec <b>cfx5solve -help</b>. La version du module chargée dans votre script pour l'ordonnanceur doit d'abord être chargée manuellement. Par défaut, cfx5solve s'exécute en simple précision (''-single''). Pour exécuter cfx5solve en double précision, ajoutez l'option <code>-double</code>, sachant que cela doublera également les besoins en mémoire. Par défaut, cfx5solve prend en charge les maillages jusqu'à 80 millions d'éléments structurés ou 200 millions d'éléments non structurés. Pour les maillages plus grands (jusqu'à 2 milliards d'éléments), ajoutez l'option <code>-large</code>. Différentes combinaisons de ces options peuvent être uitilisées pour le partitionneur, l'interpolateur ou le solveur. Consultez le guide d'ANSYS CFX-Solver Manager pour plus de détails.

<tabs>
<tab name="Nœud simple">
{{File
|name=script-local.sh
|lang="bash"
|contents=
#!/bin/bash

#SBATCH --account=def-group   # Specify account name
#SBATCH --time=00-03:00       # Specify time limit dd-hh:mm
#SBATCH --nodes=1             # Specify single compute node (do not change)
#SBATCH --ntasks-per-node=4   # Specify number cores (maximum: graham 44, cedar 32 or 48, beluga 40, narval 64)
#SBATCH --mem=16G             # Specify node memory (optionally set to 0 to allocate all node memory)
#SBATCH --cpus-per-task=1     # Do not change

#module load StdEnv/2020      # Uncomment to use (deprecated)     
#module load 2021R2           # Specify 2021R2 only

module load StdEnv/2023
module load ansys/2023R2      # Specify 2022R2 or newer module versions

# append additional cfx5solve command line options as required
if [[ "$CC_CLUSTER" = narval ]]; then
  cfx5solve -def YOURFILE.def -start-method "Open MPI Local Parallel" -part $SLURM_CPUS_ON_NODE
else
  cfx5solve -def YOURFILE.def -start-method "Intel MPI Local Parallel" -part $SLURM_CPUS_ON_NODE
fi
}}</tab>

<tab name="Plusieurs nœuds">
{{File
|name=script-cfx-multiple.sh
|lang="bash"
|contents=
#!/bin/bash

#SBATCH --account=def-group   # Specify account name
#SBATCH --time=00-03:00       # Specify time limit dd-hh:mm
#SBATCH --nodes=2             # Specify multiple compute nodes (2 or more)
#SBATCH --ntasks-per-node=64  # Specify all cores per node (maximum: graham 44, 48, beluga 40, narval 64)
#SBATCH --mem=0               # Use all memory per compute node (do not change)
#SBATCH --cpus-per-task=1     # Do not change

#module load StdEnv/2020      # Uncomment to use (deprecated)     
#module load 2021R2           # Specify 2021R2 only

module load StdEnv/2023
module load ansys/2023R2      # Specify 2022R2 or newer module versions

NNODES=$(slurm_hl2hl.py --format ANSYS-CFX)

# append additional cfx5solve command line options as required
if [[ "$CC_CLUSTER" = narval ]]; then
  cfx5solve -def YOURFILE.def -start-method "Open MPI Distributed Parallel" -par-dist $NNODES
else
  export I_MPI_HYDRA_BOOTSTRAP=ssh
  unset I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS
  cfx5solve -def YOURFILE.def -start-method "Intel MPI Distributed Parallel" -par-dist $NNODES
fi

}}</tab>

</tabs>

== Workbench ==

Initialisez le fichier de projet avant de le soumettre pour la première fois.<br>
# Connectez-vous à la grappe avec [[VNC/fr#Nœuds_de_calcul|TigerVNC]].
# Dans le même répertoire où se trouve le fichier de projet (YOURPROJECT.wbpj), [[Ansys/fr#Workbench_3|lancez Workbench]] avec la même version du module Ansys qui a servi à créer le projet. 
# Dans Workbench, ouvrez le projet avec <I>File -> Open</I>.
# Dans la fenêtre principale, faites un clic droit sur <i>Setup</i> et sélectionnez <I>Clear All Generated Data</I>.
# Dans la liste déroulante de la barre de menus du haut, cliquez sur <I>File -> Exit</I> pour sortir de Workbench.
# Dans la fenêtre contextuelle Ansys Workbench qui affiche <I>The current project has been modified. Do you want to save it ?</I> cliquez sur le bouton <i>No</i>. 
# Quittez Workbench et soumettez la tâche avec un des scripts ci-dessous.

Pour ne pas écrire la solution quand une tâche en cours se termine avec succès sur une grappe,  enlevez <code>;Save(Overwrite=True)</code> de la dernière ligne du script Slurm. Il sera ainsi plus facile d'effectuer plusieurs tâches de test (pour une mise à l'échelle quand vous changez ntasks) puisque la solution initialisée ne sera pas écrasée à chaque fois et donc n'aura pas besoin d'être réinitialisée entre les tâches. Ou encore, vous pouvez sauvegarder une copie du fichier initialisé YOURPROJECT.wbpj et du sous-répertoire YOURPROJECT_files avant que chaque tâche soit soumise et les restaurer ensuite une fois que la solution est écrite.

<span id="Slurm_scripts"></span>
=== Scripts pour l'ordonnanceur Slurm ===

Pour soumettre un fichier de projet à la queue, personnaliser les scripts suivants et lancer la commande  <code>sbatch script-wbpj-202X.sh</code>.

<tabs>
<tab name="Single node (StdEnv/2023)">
{{File
|name=script-wbpj-2023.sh
|lang="bash"
|contents=
#!/bin/bash

#SBATCH --account=def-account
#SBATCH --time=00-03:00                # Time (DD-HH:MM)
#SBATCH --mem=16G                      # Total Memory (set to 0 for all node memory)
#SBATCH --ntasks=4                     # Number of cores
#SBATCH --nodes=1                      # Do not change (multi-node not supported)
##SBATCH --exclusive                   # Uncomment for scaling testing
##SBATCH --constraint=broadwell        # Applicable to graham or cedar

module load StdEnv/2023 ansys/2023R2   # OR newer Ansys module versions (untested)

if [ "$SLURM_NNODES" == 1 ]; then
  MEMPAR=0                             # Set to 0 for SMP (shared memory parallel)
else
  MEMPAR=1                             # Set to 1 for DMP (distributed memory parallel)
fi

rm -fv *_files/.lock
MWFILE=~/.mw/Application\ Data/Ansys/`basename $(find $EBROOTANSYS/v* -maxdepth 0 -type d)`/SolveHandlers.xml
sed -re "s/(.AnsysSolution>+)[a-zA-Z0-9]*(<\/Distribute.)/\1$MEMPAR\2/" -i "$MWFILE"
sed -re "s/(.Processors>+)[a-zA-Z0-9]*(<\/MaxNumber.)/\1$SLURM_NTASKS\2/" -i "$MWFILE"
sed -i "s!UserConfigured=\"0\"!UserConfigured=\"1\"!g" "$MWFILE"

export KMP_AFFINITY=disabled
export I_MPI_HYDRA_BOOTSTRAP=ssh

runwb2 -B -E "Update()" -F YOURPROJECT.wbpj
#runwb2 -B -E "Update();Save(Overwrite=True)" -F YOURPROJECT.wbpj
}}
</tab>
<tab name="Single node (StdEnv/2020)">
{{File
|name=script-wbpj-2020.sh
|lang="bash"
|contents=
#!/bin/bash

#SBATCH --account=def-account
#SBATCH --time=00-03:00                # Time (DD-HH:MM)
#SBATCH --mem=16G                      # Total Memory (set to 0 for all node memory)
#SBATCH --ntasks=4                     # Number of cores
#SBATCH --nodes=1                      # Do not change (multi-node not supported)
##SBATCH --exclusive                   # Uncomment for scaling testing (optional)
##SBATCH --constraint=broadwell        # Uncomment on graham or cedar (optional)

module load StdEnv/2020 ansys/2021R2   # OR newer Ansys module versions

if [ "$SLURM_NNODES" == 1 ]; then
  MEMPAR=0                             # Set to 0 for SMP (shared memory parallel)
else
  MEMPAR=1                             # Set to 1 for DMP (distributed memory parallel)
fi

rm -fv *_files/.lock
MWFILE=~/.mw/Application\ Data/Ansys/`basename $(find $EBROOTANSYS/v* -maxdepth 0 -type d)`/SolveHandlers.xml
sed -re "s/(.AnsysSolution>+)[a-zA-Z0-9]*(<\/Distribute.)/\1$MEMPAR\2/" -i "$MWFILE"
sed -re "s/(.Processors>+)[a-zA-Z0-9]*(<\/MaxNumber.)/\1$SLURM_NTASKS\2/" -i "$MWFILE"
sed -i "s!UserConfigured=\"0\"!UserConfigured=\"1\"!g" "$MWFILE"

export KMP_AFFINITY=disabled
export I_MPI_HYDRA_BOOTSTRAP=ssh

runwb2 -B -E "Update()" -F YOURPROJECT.wbpj
#runwb2 -B -E "Update();Save(Overwrite=True)" -F YOURPROJECT.wbpj
}}
</tab>
</tabs>

== Mechanical ==

Le fichier d'entrée peut être généré dans votre session interactive Workbench Mechanical en cliquant sur <i>Solution -> Tools -> Write Input Files</i> et en spécifiant <code>File name:</code> pour YOURAPDLFILE.inp et <code>Save as type:</code> pour les fichiers APDL en entrée (*.inp).  Les tâches APDL peuvent ensuite être soumises à la queue avec la commande <code>sbatch script-name.sh</code>.

<span id="Slurm_scripts"></span>
=== Scripts pour l'ordonnanceur Slurm ===

<div class="mw-translate-fuzzy">
Les scripts suivants ont été testés sur Graham, Narval, Cedar et Béluga. Les lignes qui commencent par <code>##SBATCH</code> sont suivies d'un commentaire.
</div>

<tabs>
<tab name="Shared Memory Parallel (cpu)">
{{File
|name=script-smp-2023-cpu.sh
|lang="bash"
|contents=
#!/bin/bash
#SBATCH --account=def-account   # Specify your account
#SBATCH --time=00-03:00         # Specify time (DD-HH:MM)
#SBATCH --mem=32G               # Specify memory for all cores
#SBATCH --nodes=1               # Do not change
#SBATCH --tasks=8               # Specify number of cores
#SBATCH --cpus-per-task=1       # Do not change

module load StdEnv/2023
#module load ansys/2023R2
module load ansys/2024R1.03

mkdir outdir-$SLURM_JOBID
[[ "$CC_CLUSTER" = cedar ]] && export LD_LIBRARY_PATH=$EBROOTGCC/../lib/gcc

mapdl -smp -b nolist -np $SLURM_NTASKS -dir outdir-$SLURM_JOBID -i YOURAPDLFILE.inp
}}
</tab>
<tab name="Distributed Memory Parallel (cpu)">
{{File
|name=script-dmp-2023-cpu.sh
|lang="bash"
|contents=
#!/bin/bash
#SBATCH --account=def-account   # Specify your account
#SBATCH --time=00-03:00         # Specify time (DD-HH:MM)
#SBATCH --mem-per-cpu=4G        # Specify memory per core
##SBATCH --nodes=2              # Specify number of nodes (optional)
#SBATCH --ntasks=8              # Specify number of cores
##SBATCH --ntasks-per-node=4    # Specify cores per node (optional)
#SBATCH --cpus-per-task=1       # Do not change

module load StdEnv/2023
#module load ansys/2023R2
module load ansys/2024R1.03

mkdir outdir-$SLURM_JOBID
if [[ "$CC_CLUSTER" = cedar ]]; then
 ln -s $EBROOTGCC/../lib/gcc/libstdc++.so.6.0.29 $PWD/outdir-$SLURM_JOBID/libstdc++.so.6.0.29
 export LD_LIBRARY_PATH=$PWD/outdir-$SLURM_JOBID
fi

if [[ "$CC_CLUSTER" = beluga  ]]; then
  export KMP_AFFINITY=none
  mapdl -dis -mpi intelmpi -b nolist -np $SLURM_NTASKS -dir outdir-$SLURM_JOBID -i YOURAPDLFILE.inp
else
  mapdl -dis -mpi openmpi -b nolist -np $SLURM_NTASKS -dir outdir-$SLURM_JOBID -i YOURAPDLFILE.inp
fi
}}
</tab>
<tab name="Shared Memory Parallel (gpu)">
{{File
|name=script-smp-2023-gpu.sh
|lang="bash"
|contents=
#!/bin/bash
#SBATCH --account=def-account    # Specify your account
#SBATCH --time=00-03:00          # Specify time (DD-HH:MM)
#SBATCH --mem=32G                # Specify memory for all cores
#SBATCH --ntasks=8               # Specify number of cores
#SBATCH --nodes=1                # Do not change
#SBATCH --cpus-per-task=1        # Do not change
#SBATCH --gpus-per-node=1        # Specify [gputype:]quantity
##SBATCH --gpus-per-node=h100:1  # Temporarily required on mini-graham
##SBATCH --partition=debug       # Temporarily required on mini-graham

module load StdEnv/2023
#module load ansys/2023R2
module load ansys/2024R1.03

mkdir outdir-$SLURM_JOBID
[[ "$CC_CLUSTER" = cedar ]] && export LD_LIBRARY_PATH=$EBROOTGCC/../lib/gcc

export ANSGPU_PRINTDEVICES=1
mapdl -smp -acc nvidia -na $SLURM_GPUS_ON_NODE -b nolist -np $SLURM_NTASKS -dir outdir-$SLURM_JOBID  -i YOURAPDLFILE.inp
}}
</tab>
<tab name="Mémoire parallèle distribuée (GPU)">
{{File
|name=script-dmp-2023-gpu.sh
|lang="bash"
|contents=
#!/bin/bash
#SBATCH --account=def-account    # Specify your account
#SBATCH --time=00-03:00          # Specify time (DD-HH:MM)
#SBATCH --mem-per-cpu=4G         # Specify memory per core
#SBATCH --nodes=1                # Specify number of nodes
#SBATCH --ntasks-per-node=8      # Specify cores per node
#SBATCH --cpus-per-task=1        # Do not change
#SBATCH --gpus-per-node=1        # Specify [gputype:]quantity
##SBATCH --gpus-per-node=h100:1  # Temporarily required on mini-graham
##SBATCH --partition=debug       # Temporarily required on mini-graham

module load StdEnv/2023
#module load ansys/2023R2
module load ansys/2024R1.03

mkdir outdir-$SLURM_JOBID
if [[ "$CC_CLUSTER" = cedar ]]; then
 ln -s $EBROOTGCC/../lib/gcc/libstdc++.so.6.0.29 $PWD/outdir-$SLURM_JOBID/libstdc++.so.6.0.29
 export LD_LIBRARY_PATH=$PWD/outdir-$SLURM_JOBID
fi

export ANSGPU_PRINTDEVICES=1
if [[ "$CC_CLUSTER" = beluga  ]]; then 
  export KMP_AFFINITY=none
  mapdl -dis -acc nvidia -na $SLURM_GPUS_ON_NODE -mpi intelmpi -b nolist -np $SLURM_NTASKS -dir outdir-$SLURM_JOBID -i YOURAPDLFILE.inp
else
  mapdl -dis -acc nvidia -na $SLURM_GPUS_ON_NODE -mpi openmpi -b nolist -np $SLURM_NTASKS -dir outdir-$SLURM_JOBID -i YOURAPDLFILE.inp
fi
}}
</tab>
</tabs>

Par défaut, Ansys alloue aux tâches APDL 1024Mo de mémoire totale et 1024Mo de mémoire pour les bases de données. Ces valeurs peuvent être définies manuellement (ou modifiées) avec l'ajout des arguments <code>-m 1024</code> et/ou <code>-db 1024</code> sur la dernière ligne de commande mapdl des scripts ci-dessus. Si vous utilisez à distance un serveur de licence de votre établissement qui a plusieurs licences Ansys, il pourrait être nécessaire d'ajouter des arguments comme <code>-p aa_r</code> ou <code>-ppf anshpc</code>, selon le module que vous utilisez. Comme d'habitude, effectuez des tests détaillés de mise à l'échelle avant de lancer des tâches en production pour vous assurer que vous utilisez le nombre optimal de cœurs et la bonne quantité minimale de mémoire. Les scripts pour nœud simple avec mémoire parallèle partagée (SMP pour <i>Shared Memory Parallel</i>) offriront une meilleure performance que les scripts pour plusieurs nœuds avec mémoire parallèle distribuée (DMP pour <i>Distributed Memory Parallel</i>) et devraient être utilisés autant que possible.  Pour prévenir les problèmes de compatibilité, le module qui est chargé dans votre script devrait idéalement correspondre à la version employée pour générer le fichier en entrée.

  [gra-login2:~/testcase] cat YOURAPDLFILE.inp | grep version
 ! ANSYS input file written by Workbench version 2019 R3

<span id="Rocky"></span>
<div class="mw-translate-fuzzy">
== Ansys ROCKY ==
</div>

<div class="mw-translate-fuzzy">
En plus de pouvoir exécuter des simulations en mode graphique (comme indiqué dans la section <i>Mode graphique</i> ci-dessous), [https://www.ansys.com/products/fluids/ansys-rocky Ansys Rocky] peut également exécuter des simulations en mode non graphique. Les deux modes prennent en charge l'exécution de Rocky avec des processeurs uniquement ou avec des processeurs et [https://www.ansys.com/blog/mastering-multi-gpu-ansys-rocky-software-enhancing-its-performance des GPU]. Dans la section ci-dessous, deux exemples de scripts sont fournis où chacun serait soumis à la file d'attente de Graham avec la commande habituelle <code>sbatch</code>. Au moment de la rédaction de cet article, aucun des deux scripts n'a été testé et des modifications seraient probablement nécessaires. Il est important de noter que ces scripts ne sont utilisables que sur Graham puisque le module Rocky qu'ils chargent tous les deux n'est (pour le moment) installé que sur Graham localement.
</div>

<span id="Slurm_scripts"></span>
=== Scripts pour l'ordonnanceur Slurm ===

<div class="mw-translate-fuzzy">
Pour obtenir une liste complète des options de ligne de commande, exécutez <code>Rocky -h</code> sur la ligne de commande après avoir chargé un module Rocky (seul ansysrocky/2023R2 est présentement disponible sur Graham). En ce qui concerne l'utilisation de Rocky avec des GPU pour résoudre des problèmes couplés, le nombre de CPU que vous devez demander (sur le même nœud) doit être augmenté au maximum jusqu'à ce que la limite de scalabilité de l'application couplée soit atteinte. D'autre part, si Rocky est exécuté avec des GPU pour résoudre des problèmes découplés autonomes, seul un nombre minimal de CPU doit être demandé, ce qui permettra à Rocky de fonctionner de manière optimale; par exemple, seuls 2 ou éventuellement 3 CPU peuvent être nécessaires. Enfin, lorsque Rocky est exécuté avec 4 ou plus CPU, des licences <I>rocky_hpc</I> seront nécessaires, ce que fournit la licence SHARCNET.
</div>  

<tabs>
<tab name="CPU seulement">
{{File
|name=script-rocky-cpu.sh
|lang="bash"
|contents=
#!/bin/bash

#SBATCH --account=account      # Specify your account (def or rrg)
#SBATCH --time=00-02:00        # Specify time (DD-HH:MM)
#SBATCH --mem=24G              # Specify memory (set to 0 to use all node memory)
#SBATCH --cpus-per-task=6      # Specify cores (graham 32 or 44 to use all cores)
#SBATCH --nodes=1              # Request one node (do not change)

<div class="mw-translate-fuzzy">
# the rocky2023R2 module on graham was renamed to ansysrocky/2023R2   Apr24/2025
#module load ansysrocky/2023R2 StdEnv/2020 ansys/2023R2       # only avail on graham
module load ansysrocky/2024R2.0 StdEnv/2023 ansys/2024R2.04   # only avail on graham
</div>

Rocky --simulate “mysim.rocky” --resume=1 --ncpus=$SLURM_CPUS_PER_TASK --use-gpu=0
}}
</tab>
<tab name="GPU based">
{{File
|name=script-rocky-gpu.sh
|lang="bash"
|contents=
#!/bin/bash

#SBATCH --account=account      # Specify your account (def or reg)
#SBATCH --time=00-01:00        # Specify time (DD-HH:MM)
#SBATCH --mem=24G              # Specify memory (set to 0 to use all node memory)
#SBATCH --cpus-per-task=6      # Specify cores (graham 32 or 44 to use all cores)
#SBATCH --gres=gpu:v100:2      # Specify gpu type : gpu quantity
#SBATCH --nodes=1              # Request one node (do not change)

<div class="mw-translate-fuzzy">
# the rocky2023R2 module on graham was renamed to ansysrocky/2023R2   Apr24/2025
#module load ansysrocky/2023R2 StdEnv/2020 ansys/2023R2       # only avail on graham
module load ansysrocky/2024R2.0 StdEnv/2023 ansys/2024R2.04   # only avail on graham
</div>

Rocky --simulate “mysim.rocky” --resume=1 --ncpus=$SLURM_CPUS_PER_TASK --use-gpu=1 --gpu-num=$SLURM_GPUS_ON_NODE
}}
</tab>
</tabs>

== Electronics ==

Slurm scripts for using AnsysEDT is provided in a separate wiki page [https://docs.alliancecan.ca/wiki/AnsysEDT here].

<span id="Graphical_use"></span>
= Mode graphique =

<div class="mw-translate-fuzzy">
Les programmes Ansys fonctionnent interactivement en mode graphique sur les nœuds de calcul des grappes ou sur les nœuds VDI de Graham.
</div> 

 [https://docs.alliancecan.ca/wiki/Nibi#Access_through_Open_OnDemand_(OOD) NIBI]: <code>https://ondemand.sharcnet.ca</code>
 [https://docs.alliancecan.ca/wiki/Fir FIR]: <code>https://jupyterhub.fir.alliancecan.ca</code>
 [https://docs.alliancecan.ca/wiki/Rorqual RORQUAL]: <code>https://jupyterhub.rorqual.alliancecan.ca</code>
 [https://docs.alliancecan.ca/wiki/Narval NARVAL]:  <code>https://jupyterhub.narval.alliancecan.ca/</code>
 [https://docs.scinet.utoronto.ca/index.php/Open_OnDemand_Quickstart TRILLIUM]: <code>https://ondemand.scinet.utoronto.ca</code>

Submit your resource request and then wait.  If you started a Juypter Lab launcher interface then you can simply load an ansys software module from the left side menu and then click one of the ansys icons to start cfx, fluent mapdl or workbench.  Otherwise if you started a Compute/Basic Desktop from the Nibi OnDemand system then you will need to open a terminal window and manually load an ansys module and run one of the following programs from the command line.  For this later case, if your work requires accelerated graphics then either a whole GPU resource (H100 or T4 at the time of this writing) should be requested.  Since the various ansys applications launched in graphical mode behave differently when different ansys module versions are loaded, recommendations for adding command line argument and exporting environment variables for virtualgl or mesa environments have been documented below depending on whether a GPU has been requested or not and wether a On Demand or Juypter Lab system launcher is being used.

<span id="Fluids"></span>
<div class="mw-translate-fuzzy">
=== Fluids ===
::: <code>module load StdEnv/2023 ansys/2022R2</code> (ou versions plus récentes)
::: <code>fluent -mpi=intel -driver opengl|null</code> OU 
::: <code>cfx5 -graphics ogl|mesa</code>
</div>

==== Fluent ====

When starting Fluent from a terminal window command line of an On Demand Desktop or the convenience Icon of a Juypter Lab Desktop the following steps should be done including setting the indicated Environment Variables depending on which type of Compute Node the desktop is being started on (with or without a gpu).

::: <code>module load StdEnv/2023 ansys/2025R1</code>
::: <code>fluent</code>

<b>Compute Node (no GPU requested) or Basic Desktop</b>

module load ansys/2020R1

slurm_hl2hl.py --format ANSYS-FLUENT > machinefile
NCORES=$((SLURM_NTASKS * SLURM_CPUS_PER_TASK))

fluent 3d -t $NCORES -cnf=machinefile -mpi=intel -affinity=0 -g -i sample.jou
}}
</tab>

==== CFX ====

When starting CFX from an On Demand Desktop the following arguments maybe specified on the terminal window command line depending on whether a GPU was requested when the Desktop was started. 

::: <code>module load StdEnv/2023 ansys/2025R1</code>  (or older)
::: <code>cfx5 -graphics mesa</code>   (no GPU requested)
::: <code>cfx5 -graphics ogl</code>    (with GPU requested)

<span id="Mapdl"></span>
<div class="mw-translate-fuzzy">
=== Mapdl ===
::: <code>module load StdEnv/2023 ansys/2022R2</code> (ou versions plus récentes)
::: <code>mapdl -g</code>, ou via le démarreur,
::: <code>launcher</code> --> cliquer sur le bouton RUN
</div>

The following steps for starting the Mechanical APDL gui from the command line of a terminal window should work regardless if you have started your On Demand Desktop on a Compute node with or without a gpu.

::: <code>module load StdEnv/2023 ansys/2022R2</code> (or newer versions)
::: <code>mapdl -g</code>, or,
::: <code>launcher</code> then click <code>RUN</code> button

<span id="Workbench"></span>
<div class="mw-translate-fuzzy">
=== Workbench ===
::: <code>module load StdEnv/2023 ansys/2022R2</code> (ou versions plus récentes)
::: <code>xfwm4 --replace &</code> (nécessaire seulement si vous utilisez Ansys Mechanical)
::: <code>export QTWEBENGINE_DISABLE_SANDBOX=1</code> (nécessaire seulement si vous utilisez CFD-Post)
::: <code>runwb2</code>
::: <br>
::: Remarque : Quand vous exécutez en parallèle un programme d'analyse comme  Mechanical or Fluent sur un nœud simple, ne cochez pas la case <i>Distributed</i> et indiquez un nombre de cœurs égal à votre  <b>session salloc, moins 1</b>. Les menus déroulants du Ansys Mechanical Workbench ne répondent pas correctement. Comme solution, lancez <code>xfwm4 --replace</code> sur la ligne de commande avant de démarrer Workbench. Pour avoir xfwm4 par défaut, modifiez <code>$HOME/.vnc/xstartup</code> et remplacez <code>mate-session</code> par <code>xfce4-session</code>.
</div>

Note that when starting Fluent from within Workbench the same GPU dependent Environment Variable settings should be specified in the Environment Tabl of the Fluent Launcher that are explained in the Fluids section above when starting fluent from a terminal window command line. 

<span id="On_Demand_Desktop"></span>
<div class="mw-translate-fuzzy">
== Problèmes avec SSH==
:::Certains programmes d'interface graphique ANSYS peuvent être exécutés à distance sur un nœud de calcul d'une de nos grappes par redirection X via SSH vers votre ordinateur local. Contrairement à VNC, cette approche n'est ni testée ni prise en charge car elle repose sur un serveur d'affichage X correctement configuré pour votre système d'exploitation particulier OU sur la sélection, l'installation et la configuration d'un paquet d'émulateur client X approprié tel que MobaXterm. La plupart d'entre vous trouverez les temps de réponse interactifs inacceptables pour les tâches de menu de base, sans parler de l'exécution de tâches plus complexes telles que celles nécessitant du rendu graphique. Les temps d'attente pour démarrer des programmes avec interface graphique peuvent également être très longs, dépendant de votre connexion Internet. Dans un test par exemple, il a fallu 40&nbsp;minutes pour obtenir l'interface graphique avec SSH alors que vncviewer n'a pris que 34&nbsp;secondes. Malgré la lenteur potentielle lors de la connexion via SSH pour exécuter des programmes avec interface graphique, cela peut toujours être intéressant si votre seul objectif est d'ouvrir une simulation et d'effectuer des opérations de menu de base ou d'exécuter des calculs. Ces étapes de base sont un point de départ : 1) ssh -Y username@graham.computecanada.ca; 2) salloc --x11 --time=1:00:00 --mem=16G --cpus-per-task =4 [--gpus-per-node=1] --account=def-mygroup; 3) une fois connecté à un nœud de calcul, essayez d'exécuter <code>xclock</code>. Si l'horloge apparaît sur votre bureau, chargez le module Ansys souhaité et essayez d'exécuter le programme.
</div>

<div class="mw-translate-fuzzy">
=== Fluids ===
::: <code>module load CcEnv StdEnv/2023</code>
::: <code>module load ansys/2024R2.04</code> (ou versions moins récentes)
::: <code>unset SESSION_MANAGER</code>
::: <code>fluent | cfx5 | icemcfd</code>
::: o La commande <code>unset SESSION_MANAGER</code> permet d'éviter le message d'erreur suivant au lancement de Fluent.
::: [<span style="Color:#ff7f50">Qt: Session management error: None of the authentication protocols specified are supported</span>]
::: o Si le message suivant est affiché au lancement de icemcfd ...
::: [<span style="Color:#ff7f50">Error segmentation violation - exiting after doing an emergency save</span>]
::: ... ne cliquez pas sur le bouton OK, autrement icemcfd va planter. Faites plutôt ce qui suit (une seule fois)&nbsp;:
::: sélectionnez <i>Settings Tab -> Display -> tick X11 -> Apply -> OK -> File -> Exit</i>
::: L'erreur ne devrait pas se produire quand vous démarrez de nouveau icemcfd.
</div>

<div class="mw-translate-fuzzy">
=== Workbench ===
</div>

=== Ansys EDT ===
::: Ouvrez une fenêtre de terminal et chargez le module avec
:::: <code>module load SnEnv ansysedt/2023R2</code>, ou
:::: <code>module load SnEnv ansysedt/2021R2</code>
::: Dans le terminal, entrez <code>ansysedt</code> et attendez que l'interface s'affiche.
::: Ceci doit être fait une seule fois :
:::: sélectionnez <i>Tools -> Options -> HPC and Analysis Options -> Options</i>
:::: dans le menu déroulant, changez <i>HPC License</i> pour <i><b>Pool</b></i> (pour utiliser plus de 4 cœurs)
:::: cliquez sur <i>OK</i>
::: ----------   EXEMPLES  ----------
::: Pour copier dans votre compte les exemples Antennas de 2023R2 :
:::: connectez-vous à une grappe (par exemple Graham)
:::: <code>module load ansysedt/2023R2</code>
:::: <code>mkdir -p ~/Ansoft/$EBVERSIONANSYSEDT; cd ~/Ansoft/$EBVERSIONANSYSEDT; rm -rf Antennas</code>
:::: <code>cp -a $EBROOTANSYSEDT/v232/Linux64/Examples/HFSS/Antennas ~/Ansoft/$EBVERSIONANSYSEDT</code>
::: Pour faire exécuter un exemple :
:::: ouvrez un fichier .aedt et cliquez sur <i>HFSS -> Validation Check</i>
:::: (si la validation produit une erreur, fermez et ouvrez de nouveau la simulation autant de fois que nécessaire)
:::: pour lancer la simulation, cliquez sur <i>Project -> Analyze All</i>
:::: pour quitter sans sauvegarder la solution, cliquez sur <i>File -> Close -> No </i>
::: si le programme plante et ne repart pas, essayez les commandes suivantes :
:::: <code>pkill -9 -u $USER -f "ansys*|mono|mwrpcss|apip-standalone-service"</code>
:::: <code>rm -rf ~/.mw</code> (au lancement, ansysedt utilisera la configuration initiale)

=== Ensight ===
::: <code>module load SnEnv</code>
::: <code>ansys/2024R2.04</code> (or older versions back to 2021R2)
::: <code>ensight</code><br>

<span id="Jupyter_Lab_Desktop"></span>
<div class="mw-translate-fuzzy">
=== Rocky ===
::: <code>module load ansysrocky/2023R2 StdEnv/2020 ansys/2023R2</code>
::: <code>module load ansysrocky/2024R2.0 StdEnv/2023 ansys/2024R2.04</code>
::: <code>module load StdEnv/2023 ansys/2025R1</code>
::: <code>Rocky</code> Le module Ansys lit le ~/licenses/ansys.lic.<br>
::: <code>Rocky-gui</code> Cette option des modules locaux ansysrocky permet de sélectionner un serveur CMC ou SHARCNET.<br>
::: <code>RockySolver</code> Lance le solveur directement de la ligne de commande (l'ajout de -h pour ''help'' n'est pas testé).
::: <code>RockySchedular</code>, gestionnaire de ressources pour soumettre plusieurs tâches sur le nœud courant (non testé).
::: o Les versions 2024R2 ou moins récentes ne fonctionnent que sur gra-vdi et Graham; l'installation sur les autres grappes est prévue pour juin.
::: o Les versions 2025R1 et plus récentes sont fournies dans le module Ansys sur toutes les grappes (pas encore pris en charge par le serveur de licence SHARCNET).
::: o Le serveur de licence SHARCNET inclut Rocky; son utilisation est gratuite pour la recherche.
::: o Rocky prend en charge le calcul accéléré avec GPU (non testé, non documenté).
::: o Pour demander un nœud de calcul sur Graham pour utilisation interactive avec 4 CPU et 1 GPU pour un maximum de 8 heures, lancez
:::   <code>salloc --time=08:00:00 --nodes=1 --cpus-per-task=4 --gres=gpu:v100:1 --mem=32G --account=someaccount</code>
</div>

<b>Compute Node (no GPU requested)</b>

::: Click to load ansys/2025R1 (or newer version) in the Desktop left hand side menu
::: Click the "Workbench (VNC)" icon located in the Jupyter Lab desktop center window
::: Since the default icon is configured for a gpu node, we must customize it so
::: workbench can be restart in mesa mode.  To proceed, Exit the Workbench desktop,
::: open a terminal window, and run the following commands on the command line:
::: cd ~/Desktop; cp -p $(realpath workbench.desktop) workbench-mesa.desktop
::: then edit workbench-mesa.desktop and change runwb2 -> runwb2 -oglmesa
::: Save the file then click your newly customized icon to start workbench.
::: Note the workbench icon that you created will persist for future sessions
::: until manually deleted with: rm -f ~/Desktop/workbench-mesa.desktop

<b>Compute Node (with GPU requested)</b>

::: Click to load ansys/2025R1 (or newer version) in the Desktop left hand side menu
::: Click the Workbench (VNC) icon located in the Jupyter Lab desktop center window

=== Ensight ===
::: <code>module load StdEnv/2023 ansys/2022R2; A=222; B=5.12.6</code>
::: <code>export LD_LIBRARY_PATH=$EBROOTANSYS/v$A/CEI/apex$A/machines/linux_2.6_64/qt-$B/lib</code>
::: <code>ensight -X</code>

module load StdEnv/2020

== Electronics ==

Information describing howto run AnsysEDT in graphical mode maybe found here [https://docs.alliancecan.ca/wiki/AnsysEDT here].

<span id="Site-specific_usage"></span>
= Particularités selon le site d'utilisation =

<span id="SHARCNET_license"></span>
== Licence SHARCNET ==

<div class="mw-translate-fuzzy">
La licence Ansys de SHARCNET est gratuite pour une utilisation académique par les chercheurs et chercheuses de l'Alliance sur les systèmes de l'Alliance. Le logiciel installé n'a pas de limites de solveur ou de géométrie. La licence SHARCNET peut  <b>uniquement</b> être utilisée à des fins de <b><i>recherche universitaire publiable</i></b>; la production de résultats à des fins commerciales privées est strictement interdite, comme stipulé par la licence. La licence Ansys a été mise à niveau selon la Multiphysics Campus Solution en mai 2020 et inclut les produits suivants&nbsp;: HF, EM, Electronics HPC, Mechanical et CFD  [https://www.ansys.com/academic/educator-tools/academic-product-portfolio comme décrit ici]. 
Rocky et LS-DYNA sont aussi maintenant inclus dans la licence SHARCNET. Lumerical acquis par ANSYS en 2020 n'est pas disponible en ce moment, mais est installé avec les modules Ansys récents et peut donc être utilisé  avec d'autres serveurs Ansys configurés en conséquence.  SpaceClaim sous Linux n'est pas installé sur nos systèmes, mais peut techniquement être utilisé avec la licence SHARCNET]. Un groupe de licences anshpc de 1986 est inclus dans la licence SHARCNET pour prendre en charge les grandes tâches parallèles avec la plupart des produits Ansys.  Avant d'exécuter de longues tâches, il est préférable d'effectuer des tests de scalabilité. Les tâches parallèles qui utilisent moins de 50% en CPU seront probablement signalées par le système et examinées par notre équipe technique.
</div>

<div class="mw-translate-fuzzy">
Depuis décembre 2022, chaque utilisateur peut exécuter 4 travaux en utilisant un total de 252 anshpc (plus 4 anshpc par tâche). Ainsi, les combinaisons de taille de tâches uniformes suivantes sont possibles : une tâche de 256 cœurs, deux tâches de 130 cœurs, trois tâches de 88 cœurs ou quatre tâches de 67 cœurs selon ( (252 + 4* nombre de tâches) / nombre de tâches). MISE À JOUR%nbsp;: En octobre 2024, la limite a été portée à 8 tâches et 512 cœurs HPC par utilisateur (collectivement sur toutes les grappes pour toutes les applications) pour une période de test afin de permettre plus de flexibilité pour l'exploration de paramètres et l'exécution de problèmes de plus grande envergure. Comme la licence sera beaucoup moins utilisée, certains cas d'échec de tâche au démarrage pourront rarement se produire, mais les tâches devront être soumises à nouveau. Néanmoins, en supposant que la plupart continuent à exécuter une ou deux tâches en utilisant 128 cœurs en moyenne au total, cela ne devrait pas poser de problème. Cela dit, il sera utile de fermer les applications Ansys immédiatement après l'achèvement de toute tâche liée à l'interface graphique afin de libérer toutes les licences qui peuvent être consommées pendant que l'application est inactive, pour que d'autres puissent les utiliser.
</div>

<div class="mw-translate-fuzzy">
Vous pouvez aussi acheter votre propre licence Ansys auprès de [https://www.cmc.ca/subscriptions/ CMC] et utiliser leurs serveurs de licences distants. Ceci présente plusieurs avantages&nbsp;:
* un serveur de licences institutionnel local n'est pas nécessaire,
* une licence physique n'a pas besoin d'être obtenue à chaque renouvellement,
* la licence peut être utilisée [https://www.cmc.ca/ansys- campus-solutions-cmc-00200-04847/ presque n'importe où], y compris à la maison, dans des établissements ou dans toutes les grappes de l'Alliance,
* les directives de téléchargement et d'installation pour la version Windows d'Ansys sont fournies afin de permettre l'exécution de spaceclaim sur votre propre ordinateur (ceci n'est pas possible sur nos systèmes qui sont basés sur Linux).
Il existe cependant une limitation potentiellement sérieuse à prendre en compte&nbsp;: selon [https://www.cmc.ca/qsg-ansys-cadpass-r20/ Ansys Quick Start Guides], il pourrait avoir une limite de 64 cœurs par utilisateur.
</div>

<span id="License_file"></span>
==== Fichier du serveur de licence ====

Pour utiliser la licence de SHARCNET sur nos grappes, configurez votre fichier <code>ansys.lic</code> comme suit :
<source lang="bash">
[username@cluster:~] cat ~/.licenses/ansys.lic
setenv("ANSYSLMD_LICENSE_FILE", "1055@license3.sharcnet.ca")
setenv("ANSYSLI_SERVERS", "2325@license3.sharcnet.ca")
</source>

<span id="License_query"></span>
==== Licence  ====

Pour connaître le nombre de licences utilisées qui sont associées à votre nom d'utilisateur et le nombre de licences utilisées par tous les utilisateurs, lancez

<source lang="bash">
ssh graham.computecanada.ca
module load ansys
lmutil lmstat -c $ANSYSLMD_LICENSE_FILE -a | grep "Users of\|$USER"
</source>

Si vous remarquez que des licences sont utilisées sans justification avec votre nom d'utilisateur (ce qui peut se produire si Ansys n'a pas été correctement fermé sur gra-vdi), connectez-vous au nœud en cause, ouvrez une fenêtre de terminal et mettez fin au processus avec <code>pkill -9 -e -u $USER -f "ansys"</code> pour libérer vos licences.  Prenez note que gra-vdi possède deux nœuds (gra-vdi3 et gra-vdi4) qui vous sont assignés au hasard quand vous vous connectez avec tigervnc; ainsi, avant de lancer <tt>pkill</tt>, il est nécessaire d'indiquer le nom complet de l'hôte (gra-vdi3.sharcnet.ca ou grav-vdi4.sharcnet.ca) quand vous vous connectez.

<span id="Local_modules"></span>
=== Modules locaux ===

<div class="mw-translate-fuzzy">
Lors de l'utilisation de gra-vdi, vous avez le choix de charger des modules Ansys à partir de notre environnement global (après avoir chargé CcEnv) ou de charger des modules Ansys installés localement sur la machine elle-même (après avoir chargé SnEnv). Les modules locaux peuvent être intéressants, car ils incluent certains programmes et versions d'Ansys non encore pris en charge par notre environnement standard. Lors du démarrage de programmes à partir de modules Ansys locaux, vous pouvez sélectionner le serveur de licences CMC ou accepter le serveur de licences SHARCNET par défaut. Les paramètres de <code>~/.licenses/ansys.lic</code> ne sont présentement pas utilisés par les modules Ansys locaux, sauf lors du démarrage de <code>runwb2</code> où ils remplacent les paramètres par défaut du serveur de licences SHARCNET. L'utilisation appropriée des programmes Ansys sur gra-vdi comprend&nbsp;: l'exécution interactive d'une seule tâche de test avec jusqu'à 8 cœurs et/ou 128Go de RAM; la création ou la modification de fichiers d'entrée pour la simulation; le post-traitement; ou la visualisation de données.
</div>

<div class="mw-translate-fuzzy">
# Connectez-vous à gra-vdi.computecanada.ca avec [[VNC/fr#Nœuds_VDI|TigerVNC]].
# Ouvrez une fenêtre de terminal et chargez le module &nbsp;:
#; <code>module load SnEnv ansys/2021R2</code>, ou
#; <code>module load SnEnv ansys/2021R1</code>, ou
#; <code>module load SnEnv ansys/2020R2</code>, ou
#; <code>module load SnEnv ansys/2020R1</code>, ou
#; <code>module load SnEnv ansys/2019R3</code>
# Lancez un programme Ansys avec une des commandes suivantes&nbsp;:
#; <code>runwb2|fluent|<b>cfx5</b>|icemcfd|apdl</code>
# Appuyez sur <code>y</code> et <code>Enter</code> pour accepter les conditions.
# Appuyez sur <code>Enter</code> pour accepter l'option <code>n</code> et utilisez le serveur de licence par défaut SHARCNET (dans le cas de runwb2, <i>~/.licenses/ansysedt.lic</i> sera utilisé si trouvé, autrement ANSYSLI_SERVERS et ANSYSLMD_LICENSE_FILE seront utilisés s'ils sont configurés dans votre environnement, pour un autre serveur de licences distant, par exemple).   Pour utiliser le serveur de licence de CMC, changez <code>n</code> pour <code>y</code> et appuyez sur <code>Enter</code>.
</div>

<div class="mw-translate-fuzzy">
où <code><b>cfx5</b></code> de l'étape 3 ci-dessus donne l'option de démarrer les composantes suivantes&nbsp;:
    1) CFX-Launcher  (cfx5 -> cfx5launch)
    2) CFX-Pre       (cfx5pre)
    3) CFD-Post      (cfdpost -> cfx5post)
    4) CFX-Solver    (cfx5solve)
</div>

Les préférences des fonctionnalités de licence précédemment configurées avec <i>anslic_admin</i> ne sont plus prises en charge depuis le 9&nbsp;septembre 2021. Si un problème de licence survient, essayez de supprimer le répertoire <code>~/.ansys</code> de votre compte /home pour effacer les paramètres. Si les problèmes persistent, veuillez contacter notre [[Technical support/fr|soutien technique]] et fournir le contenu de votre fichier <code>~/.licenses/ansys.lic</code>.

<span id="Additive_Manufacturing"></span>
= Additive Manufacturing  =

Configurez d'abord votre fichier <code>~/.licenses/ansys.lic</code> pour l'orienter vers le serveur de licence où se trouve une licence valide pour Ansys Mechanical.  Vous devez faire ceci sur tous les systèmes où vous utiliserez le logiciel.  

<span id="Enable_Additive"></span>
== Activer Additive ==

Nous décrivons ici comment obtenir l'extension ACT de Ansys Additive Manufacturing pour l'utiliser dans votre projet. Les étapes suivantes doivent être effectuées pour chaque version de module Ansys sur chacune des grappes où l'extension sera utilisée. Les extensions nécessaires à votre projet doivent aussi être installées sur la ou les grappes, tel que décrit ci-dessous. Si vous recevez des avertissements à l'effet que des extensions dont vous n'avez pas besoin sont manquantes, par exemple ANSYSMotion, désinstallez-les à partir de votre projet.

=== Télécharger l'extension ===
* téléchargez AdditiveWizard.wbex à partir de https://catalog.ansys.com/
* téléversez AdditiveWizard.wbex sur la grappe où vous allez l'utiliser

=== Lancer Workbench ===
* Voir la section Workbench dans[[ANSYS/fr#Mode_graphique|Mode graphique plus haut]].
* Dans l'interface Workbench, ouvrez votre fichier de projet avec File -> Open.

===  Ouvrir le gestionnaire d'extensions ===
* Cliquez sur la page ACT Start pour faire afficher l'onglet de la page ACT Home.
* Cliquez sur Manage Extensions pour ouvrir le gestionnaire d'extensions.

=== Installer l'extension ===
* Cliquez sur la boîte avec le signe + sous la barre de recherche.
* Sélectionnez et installez votre fichier AdditiveWizard.wbex.

=== Charger l'extension ===
* Cliquez pour sélectionner la boîte AdditiveWizard, ce qui charge l'extension uniquement pour la session en cours.
* Cliquez sur la flèche dans le coin droit au bas de la boîte AdditiveWizard et sélectionnez <i>Load extension</i>, ce qui charge l'extension pour la session en cours et pour les sessions futures.

=== Supprimer l'extension ===
* Cliquez pour désélectionner la boîte AdditiveWizard, ce qui supprime l'extension pour la session en cours.
* Cliquez sur la flèche dans le coin droit au bas de la boîte AdditiveWizard et sélectionnez <I>Do not load as default</i>, ce qui empêche le chargement de l'extension pour les futures sessions.

<span id="Run_Additive"></span>
== Exécuter Additive ==

=== Gra-vdi ===

Vous pouvez exécuter une seule tâche Ansys Additive Manufacturing sur gra-vdi en utilisant jusqu'à 16 cœurs comme suit&nbsp;: 

* Lancez Workbench sur gra-vdi comme décrit ci-dessus dans <i>Additive Manufacturing -> Activer Additive</i>.
* Cliquez sur <i>File -> Open</i> et sélectionnez <i>test.wbpj</i> puis cliquez sur <i>Open</i>.
* Cliquez sur <i>View -> Reset workspace</i> si votre écran est gris.
* Lancez <i>Mechanical, Clear Generated Data</i>, sélectionnez  <i>Distributed</i>, spécifiez <i>Cores</i>.
* Cliquez sur <i>File -> Save Project -> Solve</i>.

Vérifiez l'utilisation :
* Ouvrez un autre terminal et lancez <code>top -u $USER</code> OU <code>ps u -u $USER | grep ansys</code>.
* Terminez les processus non nécessaires créés par des tâches précédentes avec  <code>pkill -9 -e -u $USER -f "ansys|mwrpcss|mwfwrapper|ENGINE"</code>.

Veuillez noter que des processus persistants peuvent bloquer les licences entre les sessions de connexion gra-vdi ou provoquer d'autres erreurs inhabituelles lors de la tentative de démarrage de l'interface graphique sur gra-vdi. Bien que cela soit rare, un processus peut rester en mémoire si une session d'interface graphique Ansys (Fluent, Workbench, etc.) n'est pas correctement terminée avant que vncviewer ne soit terminé manuellement ou de manière inattendue, par exemple en raison d'une panne de réseau temporaire ou d'un système de fichiers bloqué. Dans ce dernier cas, les processus peuvent ne pas être tués tant que l'accès normal au disque n'est pas rétabli.

<span id="Cluster"></span>
=== Grappe ===

Préparation du projet

Certaines préparations doivent être effectuées avant de soumettre un projet Additive nouvellement téléchargé dans la file d'attente d'une grappe avec <code>sbatch scriptname</code>. Pour commencer, ouvrez votre simulation avec l'interface graphique de Workbench (comme décrit ci-dessus dans <i>Additive Manufacturing -> Activer Additive</i>) dans le même répertoire que celui à partir duquel votre tâche sera soumise, puis enregistrez-la à nouveau. Assurez-vous d'utiliser la même version du module Ansys qui sera utilisé pour la tâche. Créez ensuite un script Slurm (comme expliqué dans le paragraphe pour Workbench dans la section <i>Soumettre des tâches en lot sur nos grappes</I> ci-dessus). Pour effectuer des études paramétriques, remplacez <code>Update()</code> par <code>UpdateAllDesignPoints()</code> dans le script Slurm. Déterminez le nombre optimal de cœurs et de mémoire en soumettant plusieurs courtes tâches de test. Pour éviter d'avoir à effacer manuellement la solution <b>et</b> recréer tous les points de conception dans Workbench entre chaque exécution de test, soit 1. remplacez <code>Save(Overwrite=True)</code> par <code>Save (Overwrite=False)</code>; ou 2. enregistrez une copie du fichier YOURPROJECT.wbpj d'origine et du répertoire YOURPROJECT_files correspondant. Vous pouvez aussi créer puis exécuter manuellement un fichier de relecture sur la grappe dans le répertoire de cas de test entre chaque exécution, en notant qu'un seul fichier de relecture peut être utilisé dans différents répertoires en l'ouvrant dans un éditeur de texte et en modifiant le paramètre interne FilePath.

 module load ansys/2019R3
 rm -f test_files/.lock
 runwb2 -R myreplay.wbjn

Utilisation des ressources

Après quelques minutes, vous pouvez obtenir un instantané de l'utilisation des ressources par la tâche en cours d'exécution sur le ou les nœuds de calcul avec la commande <tt>srun</tt>. Le script pour 8 cœurs ci-dessous produit le résultat suivant où on remarque que l'ordonnanceur a choisi 2 nœuds.

 [gra-login1:~] srun --jobid=myjobid top -bn1 -u $USER | grep R | grep -v top
   PID USER   PR  NI    VIRT    RES    SHR S  %CPU %MEM    TIME+  COMMAND
 22843 demo   20   0 2272124 256048  72796 R  88.0  0.2  1:06.24  ansys.e
 22849 demo   20   0 2272118 256024  72822 R  99.0  0.2  1:06.37  ansys.e
 22838 demo   20   0 2272362 255086  76644 R  96.0  0.2  1:06.37  ansys.e
   PID USER   PR  NI    VIRT    RES    SHR S  %CPU %MEM    TIME+  COMMAND
  4310 demo   20   0 2740212 271096 101892 R 101.0  0.2  1:06.26  ansys.e
  4311 demo   20   0 2740416 284552  98084 R  98.0  0.2  1:06.55  ansys.e
  4304 demo   20   0 2729516 268824 100388 R 100.0  0.2  1:06.12  ansys.e
  4305 demo   20   0 2729436 263204 100932 R 100.0  0.2  1:06.88  ansys.e
  4306 demo   20   0 2734720 431532  95180 R 100.0  0.3  1:06.57  ansys.e

Tests de scalabilité

Une fois la tâche complétée, son ''Job Wall-clock time'' peut être obtenu avec <code>seff jobid</code>. Cette valeur peut être utilisée pour effectuer des tests de scalabilité en soumettant de courtes tâches d'abord puis en doublant le nombre de cœurs. Tant que le Wall-clock time  diminue d'environ 50%, vous pouvez continuer de doubler le nombre de cœurs.

<span id="Help_resources"></span>
= Aide =

The official full documentation for recent versions Ansys 202[4|5]R[1|2] is available [https://ansyshelp.ansys.com/public/account/secured?returnurl=/Views/Secured/main_page.html?lang=en here].  Documentation for older versions such as Ansys 2023R[1|2] however requires [https://ansyshelp.ansys.com/ login].  Developer documentation can be found in the Ansys Developer [https://developer.ansys.com Portal]. Additional learning resources include the Ansys HowTo [https://www.youtube.com/@AnsysHowTo/videos videos], the Ansys Educator [https://innovationspace.ansys.com/educator-hub/ Educator Hub] and the Ansys Webinar [https://www.ansys.com/events/ansys-academic-webinar-series series].

<div class="mw-translate-fuzzy">
#module load ansys/2021R1
module load ansys/2021R2
</div>