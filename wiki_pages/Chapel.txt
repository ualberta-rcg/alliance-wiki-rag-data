<languages />
[[Category:Software]]

<translate>

<!--T:2-->
Chapel is a general-purpose, compiled, high-level parallel programming language with built-in abstractions for shared- and distributed-memory parallelism. There are two styles of parallel programming in Chapel: (1) <b>task parallelism</b>, where parallelism is driven by <i>programmer-specified tasks</i>, and (2) <b>data parallelism</b>, where parallelism is driven by applying the same computation on subsets of data elements, which may be in the shared memory of a single node, or distributed over multiple nodes.

<!--T:3-->
These high-level abstractions make Chapel ideal for learning parallel programming for a novice HPC user. Chapel is incredibly intuitive, striving to merge the ease-of-use of [[Python]] and the performance of traditional compiled languages such as [[C]] and [[Fortran]]. Parallel blocks that typically take tens of lines of [[MPI]] code can be expressed in only a few lines of Chapel code. Chapel is open source and can run on any Unix-like operating system, with hardware support from laptops to large HPC systems.

<!--T:4-->
Chapel has a relatively small user base, so many libraries that exist for [[C]], [[C++]], [[Fortran]] have not yet been implemented in Chapel. Hopefully, that will change in coming years if Chapel adoption continues to gain momentum in the HPC community.

<!--T:5-->
For more information, please watch our [https://westgrid.github.io/trainingMaterials/programming/#chapel Chapel webinars].

== Single-locale Chapel == <!--T:6-->

<!--T:7-->
Single-locale (single node; shared-memory only) Chapel on our general-purpose clusters is provided by the module <code>chapel-multicore</code>. You can use <code>salloc</code> to test Chapel codes either in serial:
</translate>
{{Commands
|module load gcc/12.3 chapel-multicore/2.3.0
|salloc --time{{=}}0:30:0 --ntasks{{=}}1 --mem-per-cpu{{=}}3600 --account{{=}}def-someprof
|chpl test.chpl -o test
|./test
}}
<translate>
<!--T:8-->
or on multiple cores on the same node:
</translate>
{{Commands
|module load gcc/12.3 chapel-multicore/2.3.0
|salloc --time{{=}}0:30:0 --ntasks{{=}}1 --cpus-per-task{{=}}3 --mem-per-cpu{{=}}3600 --account{{=}}def-someprof
|chpl test.chpl -o test
|./test
}}
<translate>
<!--T:9-->
For production jobs, please write a [[Running_jobs|job submission script]] and submit it with <code>sbatch</code>.

== Multi-locale Chapel == <!--T:10-->

<!--T:11-->
Multi-locale (multiple nodes; hybrid shared- and distributed-memory) Chapel on our InfiniBand clusters is provided by the module <code>chapel-ucx</code>.

<!--T:20-->
Consider the following Chapel code printing basic information about the nodes available inside your job:
</translate>
{{
File
  |name=probeLocales.chpl
  |lang="chapel"
  |contents=
use MemDiagnostics;
for loc in Locales do
  on loc {
    writeln("locale #", here.id, "...");
    writeln("  ...is named: ", here.name);
    writeln("  ...has ", here.numPUs(), " processor cores");
    writeln("  ...has ", here.physicalMemory(unit=MemUnits.GB, retType=real), " GB of memory");
    writeln("  ...has ", here.maxTaskPar, " maximum parallelism");
  }
}}
<translate>

<!--T:18-->
To run this code on an InfiniBand cluster, you need to load the <code>chapel-ucx</code> module:
{{Commands 
|module load gcc/12.3 chapel-ucx/2.3.0
|salloc --time{{=}}0:30:0 --nodes{{=}}4 --cpus-per-task{{=}}3 --mem-per-cpu{{=}}3500 --account{{=}}def-someprof
}}

<!--T:19-->
Once the [[Running_jobs#Interactive_jobs|interactive job]] starts, you can compile and run your code from the prompt on the first allocated compute node:
{{Commands
|chpl --fast probeLocales.chpl
|./probeLocales -nl 4
}}

<!--T:21-->
For production jobs, please write a [[Running_jobs|Slurm submission script]] and submit your job with <code>sbatch</code> instead.

== Multi-locale Chapel with NVIDIA GPU support == <!--T:23-->

<!--T:24-->
To enable GPU support, please use the module <code>chapel-ucx-cuda</code>. It adds NVIDIA GPU support to multi-locale Chapel on our InfiniBand clusters.

<!--T:25-->
Consider the following basic Chapel GPU code:
</translate>
{{
File
  |name=probeGPU.chpl
  |lang="chapel"
  |contents=
use GpuDiagnostics;
startGpuDiagnostics();
writeln("Locales: ", Locales);
writeln("Current locale: ", here, " named ", here.name, " with ", here.maxTaskPar, " CPU cores",
	" and ", here.gpus.size, " GPUs");
// same code can run on GPU or CPU
var operateOn =
  if here.gpus.size > 0 then here.gpus[0]   // use the first GPU
  else here;                                // use the CPU
writeln("operateOn: ", operateOn);
on operateOn {
  var A : [1..10] int;
  @assertOnGpu foreach a in A do // thread parallelism on a CPU or a GPU
    a += 1;
  writeln(A);
}
stopGpuDiagnostics();
writeln(getGpuDiagnostics());
}}
<translate>

<!--T:26-->
To run this code on an InfiniBand cluster, you need to load the <code>chapel-ucx-cuda</code> module:
{{Commands 
|module load gcc/12.3 cuda/12.2 chapel-ucx-cuda/2.3.0
|salloc --time{{=}}0:30:0 --mem-per-cpu{{=}}3500 --gpus-per-node{{=}}1 --account{{=}}def-someprof
}}

<!--T:27-->
Once the [[Running_jobs#Interactive_jobs|interactive job]] starts, you can compile and run your code from the prompt on the allocated compute node:
{{Commands
|chpl --fast probeGPU.chpl
|./probeGPU -nl 1
}}

<!--T:28-->
For production jobs, please write a [[Running_jobs|Slurm submission script]] and submit your job with <code>sbatch</code> instead.

</translate>