gpu performance
pytorch
batch size
num workers
performance optimization
data loader
gpu memory
data transfer
single gpu training
gpu utilization
deep learning
gpu acceleration
distributed training
model parallelism
libtorch c
cpu parallelism
checkpoints