pytorch
data parallelism
single gpu
gpu utilization
batch size
neural network training
distributeddataparallel
learning rate
hpc clusters
model replicas
gpu computing
distributed training
performance optimization
deepspeed
libtorch
model checkpoints