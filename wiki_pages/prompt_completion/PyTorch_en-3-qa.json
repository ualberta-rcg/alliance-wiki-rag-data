[
  {
    "question": "How can I download the CIFAR10 dataset for use with PyTorch if it's not already stored locally?",
    "answer": "You can either run the dataset line with `download=True` on a login node before submitting your job, or manually download the data from `https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz` and place it under a `./data` directory."
  },
  {
    "question": "What are the command-line steps to manually download and extract the CIFAR10 dataset?",
    "answer": "First, create and navigate into a `data` directory using `mkdir -p data && cd data`. Then, download the dataset with `wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz` and extract it using `tar zxf cifar-10-python.tar.gz`. Finally, navigate back to the parent directory with `cd ..`."
  },
  {
    "question": "How can I observe the impact of PyTorch's native support for CPU parallelism on performance?",
    "answer": "You can observe the effect by simply requesting more CPUs in your job submission script without making any code changes to your Python application."
  },
  {
    "question": "What is an example SLURM script for running a PyTorch job with multi-CPU parallelism?",
    "answer": "The `pytorch-multi-cpu.sh` script, which loads a Python module, creates and activates a virtual environment, installs `torch` and `torchvision`, sets the `OMP_NUM_THREADS` environment variable to `$SLURM_CPUS_PER_TASK`, and then executes `python cifar10-cpu.py`."
  },
  {
    "question": "How can I adjust the number of CPUs requested for a PyTorch multi-CPU job using a SLURM script?",
    "answer": "In the `pytorch-multi-cpu.sh` SLURM script, you can change the `--cpus-per-task` parameter (e.g., to 2, 4, 6, etc.) to control the number of CPUs."
  },
  {
    "question": "What environment variable controls the number of OpenMP threads for PyTorch multi-CPU jobs?",
    "answer": "The `OMP_NUM_THREADS` environment variable, which is typically set to `$SLURM_CPUS_PER_TASK` in the job submission script."
  },
  {
    "question": "Is it always recommended to use a GPU for model training on HPC clusters if one is available?",
    "answer": "No, it's not always recommended. You should not request a GPU if your code is not capable of making reasonable use of its compute capacity, especially since training very small models can sometimes be faster on CPUs."
  },
  {
    "question": "What are the primary sources of performance advantage for GPUs in Deep Learning tasks?",
    "answer": "GPUs derive their performance advantage from their ability to parallelize key numerical operations (like multiply-accumulate) over many thousands of compute cores, and from their much higher memory bandwidth compared to CPUs, which allows for processing larger amounts of data efficiently."
  },
  {
    "question": "What characteristics make a learning task suitable for GPU execution?",
    "answer": "A learning task is suitable for GPU execution if it involves elements that scale with massive parallelism, either in terms of the number of operations, the amount of data required, or ideally both. This typically means large models with many units and layers, large inputs, or both."
  },
  {
    "question": "What GPU-specific libraries does PyTorch utilize for parallel operations like matrix multiplication and convolution?",
    "answer": "PyTorch uses GPU-specific libraries such as CUDNN or MIOpen for parallel implementations of operators, depending on the hardware platform."
  },
  {
    "question": "Which two parameters are crucial for optimizing GPU performance in PyTorch, and what role do they play?",
    "answer": "The `batch_size` and `num_workers` parameters are crucial. `batch_size` influences performance by increasing the input size per iteration, thereby utilizing more of the GPU's capacity. `num_workers` streamlines data movement from the host's (CPU's) memory to the GPU's memory, reducing GPU idle time."
  },
  {
    "question": "What are the key recommendations for optimizing GPU compute performance and data feeding in PyTorch?",
    "answer": "To optimize compute performance, increase your `batch_size` to as much as can fit in the GPU's memory. To streamline data feeding, use a `DataLoader` with as many `num_workers` as you have `cpus-per-task`."
  }
]