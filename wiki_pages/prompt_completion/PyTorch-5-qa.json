[
  {
    "question": "What is Data Parallelism in PyTorch?",
    "answer": "Data Parallelism refers to methods for performing training over multiple replicas of a model in parallel, where each replica receives a different chunk of training data at each iteration. Gradients are then aggregated and parameters updated."
  },
  {
    "question": "What is a key benefit of using Data Parallelism?",
    "answer": "Using Data Parallelism can provide a significant speed-up by iterating through all examples in a large dataset approximately N times faster, where N is the number of model replicas."
  },
  {
    "question": "What is an important caveat when using Data Parallelism to achieve a trained model equivalent to one without parallelism?",
    "answer": "To get a trained model equivalent to one without Data Parallelism, the user must scale either the learning rate or the desired batch size in function of the number of replicas."
  },
  {
    "question": "What is a constraint for models when using Data Parallelism in a multiple-GPU setup?",
    "answer": "In the multiple-GPU case, each GPU hosts a replica of your model, so the model must be small enough to fit inside the memory of a single GPU."
  },
  {
    "question": "What are the ways to perform Data Parallelism using PyTorch?",
    "answer": "PyTorch offers several ways to perform Data Parallelism, including using the DistributedDataParallel class directly with one or multiple GPUs, and using the PyTorch Lightning package."
  },
  {
    "question": "Which class is recommended by PyTorch maintainers for multi-GPU Data Parallelism?",
    "answer": "The DistributedDataParallel class is recommended by PyTorch maintainers for using multiple GPUs, whether they are all on a single node or distributed across multiple nodes."
  },
  {
    "question": "How many GPUs are requested by the `pytorch-ddp-test.sh` example script?",
    "answer": "The `pytorch-ddp-test.sh` example script requests 2 GPUs using `#SBATCH --gres=gpu:2`."
  },
  {
    "question": "How does the `pytorch-ddp-test.sh` script store the master node's IP address for distributed training?",
    "answer": "The script stores the master node's IP address in the `MASTER_ADDR` environment variable using `export MASTER_ADDR=$(hostname)`."
  },
  {
    "question": "What is the purpose of `dist.init_process_group` in the `pytorch-ddp-test.py` script?",
    "answer": "The `dist.init_process_group` function initializes a process group and initiates communications between all processes running on all nodes for distributed training."
  },
  {
    "question": "How is a PyTorch model configured for DistributedDataParallel in the `pytorch-ddp-test.py` example?",
    "answer": "The model `net` is wrapped using `net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[current_device])`, where `current_device` is set based on the local rank."
  },
  {
    "question": "What is the default batch size argument in the `pytorch-ddp-test.py` script?",
    "answer": "The default batch size in the `pytorch-ddp-test.py` script is 768."
  },
  {
    "question": "What is the default distributed backend used in the `pytorch-ddp-test.py` script?",
    "answer": "The default distributed backend used in the `pytorch-ddp-test.py` script is 'nccl'."
  }
]