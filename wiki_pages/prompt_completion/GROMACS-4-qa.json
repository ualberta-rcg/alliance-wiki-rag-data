[
  {
    "question": "How do you load the GROMACS 2024.4 module for CPU use within the StdEnv/2023 environment?",
    "answer": "You can load it using the command `module load StdEnv/2023 gcc/12.3 openmpi/4.1.5 gromacs/2024.4`."
  },
  {
    "question": "What is the command to load GROMACS 2023.2 for CPU calculations using the StdEnv/2020 environment?",
    "answer": "Use `module load StdEnv/2020 gcc/9.3.0 openmpi/4.0.3 gromacs/2023.2`."
  },
  {
    "question": "What module must be loaded first to enable GPU support for GROMACS?",
    "answer": "The `cuda` module needs to be loaded first to enable GPU support."
  },
  {
    "question": "Are GPU-enabled GROMACS versions available with double precision?",
    "answer": "No, GPU-enabled versions are only available with single precision."
  },
  {
    "question": "How do you load the GROMACS 2024.4 module with GPU support for the StdEnv/2023 environment?",
    "answer": "You can load it using the command `module load StdEnv/2023 gcc/12.3 openmpi/4.1.5 cuda/12.2 gromacs/2024.4`."
  },
  {
    "question": "What command loads GROMACS 2023.2 with CUDA support using the StdEnv/2020 environment?",
    "answer": "Use `module load StdEnv/2020 gcc/9.3.0 cuda/11.4 openmpi/4.0.3 gromacs/2023.2`."
  },
  {
    "question": "Where can users find more information about environment modules?",
    "answer": "For more information on environment modules, please refer to the 'Using modules' page."
  },
  {
    "question": "What does the `gmx` binary represent in GROMACS 5.x, 2016.x, and newer releases?",
    "answer": "The `gmx` binary represents mixed (\"single\") precision GROMACS with OpenMP (threading) but without MPI."
  },
  {
    "question": "What is the `gmx_mpi` binary used for in GROMACS 5 and newer versions?",
    "answer": "It is used for mixed (\"single\") precision GROMACS with OpenMP and MPI."
  },
  {
    "question": "Which binary allows for double precision GROMACS with OpenMP but without MPI in versions 5.x and newer?",
    "answer": "The `gmx_d` binary allows for double precision GROMACS with OpenMP but without MPI."
  },
  {
    "question": "Which binary supports double precision GROMACS with OpenMP and MPI in GROMACS 5 and newer?",
    "answer": "The `gmx_mpi_d` binary supports double precision GROMACS with OpenMP and MPI."
  },
  {
    "question": "How are GROMACS tools organized in versions 5 and newer?",
    "answer": "All GROMACS tools from previous versions have been implemented as sub-commands of the `gmx` binaries."
  },
  {
    "question": "What suffix do double precision binaries have in GROMACS 4.6.7?",
    "answer": "The double precision binaries in GROMACS 4.6.7 have the suffix `_d`."
  },
  {
    "question": "What are the parallel single and double precision `mdrun` binaries for GROMACS 4.6.7?",
    "answer": "The parallel `mdrun` binaries for GROMACS 4.6.7 are `mdrun_mpi` for single precision and `mdrun_mpi_d` for double precision."
  },
  {
    "question": "Where can users find help on using the SLURM workload manager for GROMACS jobs?",
    "answer": "Users should refer to the 'Running jobs' page for help on using the SLURM workload manager."
  },
  {
    "question": "What is the purpose of the `serial_gromacs_job.sh` script provided in the documentation?",
    "answer": "It is a simple job script for running a serial `mdrun` simulation of a molecular system in the `em.tpr` file."
  },
  {
    "question": "What are the key elements of the example serial job script for GROMACS?",
    "answer": "The script includes `SBATCH` directives for time limit and memory, module loading commands (`module purge`, `module load StdEnv/2023 gcc/12.3 openmpi/4.1.5 gromacs/2024.4`), setting `OMP_NUM_THREADS`, and executing `gmx mdrun`."
  },
  {
    "question": "Why is it common to use a number of whole nodes for GROMACS simulations?",
    "answer": "Commonly, the systems simulated with GROMACS are so large that using a number of whole nodes for the simulation is recommended."
  },
  {
    "question": "What is the general rule for matching CPU cores when using whole nodes for GROMACS simulations?",
    "answer": "Generally, the product of `--ntasks-per-node` and `--cpus-per-task` should match the number of CPU cores of the cluster\u2019s compute nodes."
  },
  {
    "question": "What potential issue can arise with domain decomposition on clusters having a large number of CPU cores per compute node?",
    "answer": "Domain decomposition can become a limiting factor when choosing `--ntasks-per-node` on clusters with a large number of CPU cores per compute node."
  },
  {
    "question": "What should you do if GROMACS reports an error about domain decomposition being impossible given the system size and requested number of domains?",
    "answer": "If GROMACS reports such an error, you should halve `--ntasks-per-node` and double `--cpus-per-task`."
  }
]