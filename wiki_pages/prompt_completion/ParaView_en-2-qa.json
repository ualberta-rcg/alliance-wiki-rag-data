[
  {
    "question": "How do you specify the amount of memory for a single-core JupyterLab session for visualization?",
    "answer": "You set the 'Memory' in the JupyterHub Server Options based on the maximum amount of data you expect to process at a time."
  },
  {
    "question": "What happens after pressing the 'Start' button in the JupyterHub Server Options for a visualization session?",
    "answer": "Pressing 'Start' will submit a Slurm job to the cluster in the background, and you'll need to wait about one minute for the job to start and for the JupyterLab dashboard to appear."
  },
  {
    "question": "What are the two main options to launch ParaView after a JupyterLab dashboard appears for single-core visualization?",
    "answer": "The first option is to load the 'boost/1.85.0' and 'paraview/6.0.0' modules under 'Software Modules' on the left-hand side, then click the 'ParaView (VNC) button'. The alternative is to click your preferred 'Desktop' button, open a terminal inside the virtual desktop, and type 'module load boost/1.85.0 paraview/6.0.0' followed by 'paraview'."
  },
  {
    "question": "What modules need to be loaded to run ParaView in a JupyterLab virtual desktop terminal for single-core visualization?",
    "answer": "You need to load the 'boost/1.85.0' and 'paraview/6.0.0' modules."
  },
  {
    "question": "Is the ParaView GUI application inherently multi-threaded?",
    "answer": "No, the ParaView GUI application itself is single-threaded and cannot directly use multiple cores."
  },
  {
    "question": "Which ParaView filters support multithreading?",
    "answer": "Some filters, such as contouring, clipping, or resampling, support multithreading via VTK backends like TBB or OpenMP."
  },
  {
    "question": "How is true parallel rendering achieved with ParaView in JupyterLab?",
    "answer": "True parallel rendering requires connecting a single-core ParaView client to a parallel ParaView server, both of which can be launched within JupyterLab."
  },
  {
    "question": "What JupyterHub Server Options settings are different for multi-core visualization compared to single-core?",
    "answer": "For multi-core visualization, you need to select your desired 'Number of Cores' (e.g., 4) and scale your 'Memory' request accordingly (e.g., 3600 MB per core, so 14400 MB for 4 cores)."
  },
  {
    "question": "How do you start the ParaView server for multi-core visualization within a JupyterLab virtual desktop?",
    "answer": "After loading the 'boost/1.85.0' and 'paraview/6.0.0' modules, you open a terminal and type 'mpirun --oversubscribe -np 4 pvserver' (replacing 4 with your desired number of cores)."
  },
  {
    "question": "What information from the 'pvserver' command output is needed to connect the ParaView client?",
    "answer": "You need the 'Connection URL' which provides the host and port number (e.g., 'cs://fc30669:11111')."
  },
  {
    "question": "How do you configure the ParaView GUI client to connect to a parallel ParaView server running in the same JupyterLab session?",
    "answer": "In the ParaView GUI, click 'Connect', then 'Add Server'. Set 'Server Type' to 'Client/Server', 'Host' to 'localhost', 'Port' to '11111' (or the port from 'pvserver' output), and 'Startup Type' to 'Manual'. Then click 'Connect' again."
  },
  {
    "question": "How can you verify that parallel rendering is active in ParaView?",
    "answer": "You can colour your dataset by the 'Process Id' variable, which is unavailable when running in serial."
  },
  {
    "question": "Which clusters support single-core visualization via Open OnDemand?",
    "answer": "Single-core visualization via Open OnDemand is supported on Nibi and Trillium."
  },
  {
    "question": "What are the web portals for Open OnDemand on Nibi and Trillium?",
    "answer": "For Nibi, it's https://ondemand.sharcnet.ca, and for Trillium, it's https://ondemand.scinet.utoronto.ca/pun/sys/dashboard."
  },
  {
    "question": "How do you launch a single-core ParaView instance via Open OnDemand?",
    "answer": "After logging into Open OnDemand, find 'Desktop' (e.g., 'Compute Nodes | Nibi Desktop'), specify a CPU-only Slurm account and 1 CPU core, click 'Launch', wait for the job to start, then click 'Launch Nibi Desktop'. Inside the desktop, open a terminal and type 'module load paraview/6.0.0' followed by 'paraview'."
  },
  {
    "question": "What are the resource limits for multi-core visualization on Nibi's Open OnDemand?",
    "answer": "On Nibi's Open OnDemand, you can request up to 128GB memory and up to 8 cores for multi-core visualization."
  },
  {
    "question": "What is required for a ParaView client running on your computer to connect to a ParaView server on an HPC cluster?",
    "answer": "ParaView requires the same major version on the local client and the remote host to prevent incompatibility and ensure a successful client-server connection (e.g., client version 6.0.x for server version 6.0.0)."
  },
  {
    "question": "What is the 'Remote Render Threshold' setting in ParaView preferences and how does it affect performance?",
    "answer": "The 'Remote Render Threshold' (default 20MB) in 'Render View -> Remote/Parallel Rendering Options' determines where rendering occurs. If set to default, small rendering happens on your local GPU for fast rotation, but anything modestly intensive (under 20MB) is shipped to your computer. If set to 0MB, all rendering, including rotation, is done remotely on cluster resources, which is good for large data processing but less interactive."
  },
  {
    "question": "Can HPC clusters perform rasterization and ray tracing using CPUs, and what libraries are used?",
    "answer": "Yes, HPC clusters can do both rasterization and ray tracing on cluster CPUs. Modern CPU-based libraries like OSPRay and OpenSWR offer performance similar to GPU-based rendering."
  },
  {
    "question": "How can you estimate the number of cores needed for large-scale interactive visualization?",
    "answer": "A rough estimate is to divide the amount of memory needed for rendering (e.g., 40GB for a dataset) by approximately 3.5 GB/core. It's recommended not to allocate more than 4GB/core, and additional memory should be considered for filters and data processing."
  },
  {
    "question": "What is a specific core allocation requirement for ParaView visualization on the Trillium cluster?",
    "answer": "On Trillium, you must schedule on whole nodes, meaning core allocations must be in multiples of 192 cores, making 192 cores the minimum for a visualization job."
  },
  {
    "question": "What is the command to start a parallel CPU interactive job for large-scale visualization on an HPC cluster?",
    "answer": "The command is 'salloc --time=1:00:0 --ntasks=... --mem-per-cpu=3600 --account=def-someprof'. For Trillium, '--ntasks' would be 192."
  },
  {
    "question": "What module needs to be loaded specifically on Trillium before loading the ParaView module?",
    "answer": "On Trillium, you must load the 'StdEnv/2023' module before attempting to load 'paraview/6.0.0'."
  },
  {
    "question": "What command starts the ParaView server on an HPC cluster for large-scale interactive visualization?",
    "answer": "The command is 'srun pvserver --force-offscreen-rendering --opengl-window-backend OSMesa'."
  },
  {
    "question": "How do you create an SSH tunnel from your local computer to the compute node where the ParaView server is running on the cluster?",
    "answer": "In a terminal on your computer (Mac/Linux), use the command 'ssh <username>@fir.alliancecan.ca -L 11111:fc30669:11111', replacing 'fir' with the actual cluster name and 'fc30669' with the compute node name from the server output."
  }
]