[
  {
    "question": "What is Fully Sharded Data Parallelism (FSDP) in PyTorch?",
    "answer": "Fully Sharded Data Parallelism (FSDP) is a technique that enables distributed storage and computation of various training task elements, such as optimizer states, model weights, model gradients, and model activations, across multiple devices like GPUs, CPUs, or local hard disks."
  },
  {
    "question": "What is the benefit of using FSDP?",
    "answer": "FSDP allows models with massive amounts of parameters to be trained efficiently across multiple nodes by pooling resources, especially for storage."
  },
  {
    "question": "When should FSDP not be used?",
    "answer": "You should not use FSDP if your model has layers that do not fit entirely into the memory of a single GPU, because a sharded model layer might still be collected inside a single device during a forward or backward pass."
  },
  {
    "question": "How does Tensor Parallelism (TP) differ from FSDP?",
    "answer": "Tensor Parallelism (TP) differs from FSDP in that it splits the computation of a forward or backward pass through a model layer, along with the layer's weights, across multiple devices. This means computation steps are performed locally on the device where a model shard is placed, avoiding the overhead of moving shards between devices that FSDP can incur."
  },
  {
    "question": "What is Pipeline Parallelism (PP)?",
    "answer": "Pipeline Parallelism (PP) is a model sharding approach where groups of consecutive layers of a model are placed on different devices. Computations through the model are performed sequentially across these devices."
  },
  {
    "question": "How does Pipeline Parallelism mitigate device idle time?",
    "answer": "To mitigate device idle time in Pipeline Parallelism, every input batch is broken into 'micro-batches', which are fed to the model in sequence. This ensures all devices remain busy as the first micro-batch reaches the final model block."
  },
  {
    "question": "Why is it important to create model checkpoints during training?",
    "answer": "Creating model checkpoints is important to save a snapshot of your model at a given point during training. This helps in breaking up long jobs into shorter ones, potentially leading to quicker cluster allocations, and prevents loss of progress due to unexpected errors or node failures."
  },
  {
    "question": "How do you create a checkpoint with PyTorch Lightning?",
    "answer": "With PyTorch Lightning, checkpoints are created by using the `callbacks` parameter of the `Trainer()` class, for example: `pl.callbacks.ModelCheckpoint(dirpath=\"./ckpt\",every_n_epochs=1)`. Ensure the specified path for the checkpoint exists."
  },
  {
    "question": "How does PyTorch Lightning handle loading checkpoints?",
    "answer": "The same code snippet used for creating checkpoints with PyTorch Lightning will also load an existing checkpoint from the specified `dirpath` (e.g., `./ckpt`) if one is found, allowing training to resume from that point."
  },
  {
    "question": "Where can one find documentation for creating checkpoints in custom PyTorch training loops?",
    "answer": "For examples on creating and loading checkpoints within custom training loops, users should refer to the official PyTorch documentation on saving and loading a general checkpoint."
  },
  {
    "question": "When performing distributed training, which process should handle checkpointing?",
    "answer": "During distributed training, checkpointing should typically be performed by only one process (e.g., rank 0) of your program, as all ranks will have the same model state at the end of each iteration."
  },
  {
    "question": "What is a potential issue when loading checkpoints in distributed training, and how can it be avoided?",
    "answer": "A potential issue is that a process might try to load a checkpoint before another process has finished saving it, leading to errors or incorrect results. This can be avoided by adding `torch.distributed.barrier()` to ensure the checkpoint is fully written to disk before other processes attempt to load it."
  },
  {
    "question": "How should `map_location` be used when loading checkpoints in distributed training?",
    "answer": "When loading checkpoints in distributed training, `map_location` should be passed to `torch.load` (e.g., `map_location = f\"cuda:{local_rank}\"`) to ensure tensors are loaded onto the correct GPU for each specific rank, as `torch.load` defaults to loading tensors to the GPU that originally saved them (e.g., `cuda:0`)."
  }
]