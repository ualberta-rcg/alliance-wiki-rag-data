[
  {
    "question": "What are the specifications of the 33 Narval nodes?",
    "answer": "The 33 nodes on Narval each have 64 cores, 2009G (2057500M) of available memory, and are equipped with 2 x AMD EPYC 7502 (Zen 2) processors running at 2.50 GHz with 128M L3 cache."
  },
  {
    "question": "Describe the characteristics of the 3 Narval nodes.",
    "answer": "The 3 Narval nodes each feature 64 cores, 4000G (4096000M) of available memory, and 2 x AMD EPYC 7502 (Zen 2) processors at 2.50 GHz with 128M L3 cache."
  },
  {
    "question": "What are the hardware specifications of the 159 Narval nodes?",
    "answer": "The 159 Narval nodes are configured with 48 cores, 498G (510000M) of available memory, 2 x AMD EPYC 7413 (Zen 3) processors at 2.65 GHz with 128M L3 cache, 1 x 3.84 TB SSD for storage, and 4 x NVidia A100SXM4 (40 GB memory) GPUs connected via NVLink."
  },
  {
    "question": "Which instruction sets do Narval's AMD processors support?",
    "answer": "Narval's 2nd and 3rd generation AMD EPYC processors support the AVX2 instruction set."
  },
  {
    "question": "Does Narval support the AVX512 instruction set?",
    "answer": "No, Narval does not support the AVX512 instruction set."
  },
  {
    "question": "Can Intel compilers be used for applications on Narval's AMD processors?",
    "answer": "Yes, Intel compilers can compile applications for Narval's AMD processors, supporting AVX2 and earlier instruction sets."
  },
  {
    "question": "What option should be used with Intel compilers to produce executables compatible with both Intel and AMD processors on Narval?",
    "answer": "Use the `-march=core-avx2` option with Intel compilers for compatibility on Narval."
  },
  {
    "question": "Which Intel compiler options should not be used on Narval, and why?",
    "answer": "Options like `-xXXXX` or `-xCORE-AVX2` should not be used if compiled on an Intel system, as they add Intel processor verification instructions. Also, `-xHOST` and `-march=native` are equivalent to `-march=pentium` and should not be used."
  },
  {
    "question": "What is the standard software environment on Narval?",
    "answer": "The standard software environment on Narval is `StdEnv/2023`."
  },
  {
    "question": "Are older standard software environments available on Narval?",
    "answer": "No, previous versions `2016` and `2018` have been intentionally blocked. You should contact technical support if an application requires an older environment."
  },
  {
    "question": "Which BLAS and LAPACK library is favored on Narval?",
    "answer": "The FlexiBLAS library is favored on Narval, although Intel MKL also works with AMD processors but not optimally."
  },
  {
    "question": "How do you request one A100-40gb GPU for a job on Narval?",
    "answer": "To request one A100-40gb GPU, use the Slurm option `--gpus=a100:1`."
  },
  {
    "question": "What Slurm options are used to request multiple A100-40gb GPUs per node?",
    "answer": "To request multiple A100-40gb GPUs per node, you can use `--gpus-per-node=a100:2`, `--gpus-per-node=a100:3`, or `--gpus-per-node=a100:4`."
  },
  {
    "question": "How can you request a specific number of full A100 GPUs that can be spread across different nodes?",
    "answer": "Use the Slurm option `--gpus=a100:n`, replacing `n` with the desired number of GPUs."
  },
  {
    "question": "What are the available sizes for Multi-Instance GPU (MIG) technology on Narval?",
    "answer": "Four sizes are available: 1g.5gb (1/8 compute capacity, 5 GB memory), 2g.10gb (2/8 compute capacity, 10 GB memory), 3g.20gb (3/8 compute capacity, 20 GB memory), and 4g.20gb (4/8 compute capacity, 20 GB memory, with fewer available)."
  },
  {
    "question": "How do you request a single 1g.5gb Multi-Instance GPU for a compute job?",
    "answer": "Use the Slurm option `--gpus=a100_1g.5gb:1` to request one 1g.5gb GPU instance."
  },
  {
    "question": "What is the Slurm option to request one 2g.10gb GPU instance?",
    "answer": "The Slurm option for requesting one 2g.10gb GPU instance is `--gpus=a100_2g.10gb:1`."
  },
  {
    "question": "How can a user request a single 3g.20gb Multi-Instance GPU?",
    "answer": "To request one 3g.20gb GPU instance, use the Slurm option `--gpus=a100_3g.20gb:1`."
  },
  {
    "question": "What Slurm option is used to request one 4g.20gb GPU instance?",
    "answer": "The Slurm option `--gpus=a100_4g.20gb:1` is used to request a single 4g.20gb GPU instance."
  }
]