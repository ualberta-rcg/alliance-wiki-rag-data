[
  {
    "question": "How do you request a single CPU core and one GPU for a Slurm job?",
    "answer": "You can use a script like 'gpu_serial_job.sh' with the following SBATCH directives: `#SBATCH --gpus-per-node=1`, `#SBATCH --mem=4000M`, and `#SBATCH --time=0-03:00`."
  },
  {
    "question": "What is an example Slurm script for a single-core GPU job?",
    "answer": "A script named 'gpu_serial_job.sh' is provided as an example:\n```bash\n#!/bin/bash\n#SBATCH --account=def-someuser\n#SBATCH --gpus-per-node=1\n#SBATCH --mem=4000M               # memory per node\n#SBATCH --time=0-03:00\n./program                         # you can use 'nvidia-smi' for a test\n```"
  },
  {
    "question": "How do you submit a multi-threaded GPU job requiring multiple CPUs on a single node?",
    "answer": "You would include SBATCH directives such as `--gpus-per-node=1` for the GPU and `--cpus-per-task=6` for CPU cores/threads, along with `--mem=4000M` and `--time=0-03:00`."
  },
  {
    "question": "What is an example Slurm script for a multi-threaded GPU job?",
    "answer": "A script named 'gpu_threaded_job.sh' is provided as an example:\n```bash\n#!/bin/bash\n#SBATCH --account=def-someuser\n#SBATCH --gpus-per-node=1         # Number of GPU(s) per node\n#SBATCH --cpus-per-task=6         # CPU cores/threads\n#SBATCH --mem=4000M               # memory per node\n#SBATCH --time=0-03:00\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n./program\n```"
  },
  {
    "question": "What are the recommended maximum CPU cores per GPU on Fir?",
    "answer": "On Fir, it is recommended to use no more than 12 CPU cores per GPU requested."
  },
  {
    "question": "What are the recommended maximum CPU cores per GPU on Narval?",
    "answer": "On Narval, it is recommended to use no more than 12 CPU cores per GPU requested."
  },
  {
    "question": "What are the recommended maximum CPU cores per GPU on Nibi?",
    "answer": "On Nibi, it is recommended to use no more than 14 CPU cores per GPU requested."
  },
  {
    "question": "What are the recommended maximum CPU cores per GPU on Rorqual?",
    "answer": "On Rorqual, it is recommended to use no more than 16 CPU cores per GPU requested."
  },
  {
    "question": "How do you submit an MPI job that uses multiple GPUs?",
    "answer": "You can use SBATCH directives like `--gpus=8` for the total number of GPUs, `--ntasks-per-gpu=1` for MPI processes, `--cpus-per-task=6` for CPU cores per MPI process, and `--mem-per-cpu=5G` for host memory per CPU core."
  },
  {
    "question": "What is an example Slurm script for an MPI GPU job?",
    "answer": "A script named 'gpu_mpi_job.sh' is provided as an example:\n```bash\n#!/bin/bash\n#SBATCH --account=def-someuser\n#SBATCH --gpus=8                  # total number of GPUs\n#SBATCH --ntasks-per-gpu=1        # total of 8 MPI processes\n#SBATCH --cpus-per-task=6         # CPU cores per MPI process\n#SBATCH --mem-per-cpu=5G          # host memory per CPU core\n#SBATCH --time=0-03:00            # time (DD-HH:MM)\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nsrun --cpus-per-task=$SLURM_CPUS_PER_TASK ./program\n```"
  },
  {
    "question": "When should you ask Slurm for a whole node?",
    "answer": "If your application can efficiently use an entire node and its associated GPUs, requesting a whole node will likely result in shorter wait times."
  },
  {
    "question": "What tool is recommended for packing multiple single-GPU or 2-GPU jobs into one Slurm job, especially for tasks longer than 24 hours?",
    "answer": "GNU Parallel is recommended for packing multiple single-GPU or 2-GPU programs into one Slurm job, especially if they run for longer than 24 hours."
  },
  {
    "question": "How do you ensure tasks don't use the same GPU simultaneously when using GNU Parallel?",
    "answer": "You can use `CUDA_VISIBLE_DEVICES=$(({%} - 1))` with GNU Parallel to ensure that each task is assigned a unique GPU ID, preventing them from trying to use the same GPU at the same time."
  },
  {
    "question": "What does the `-j4` parameter signify in a GNU Parallel command?",
    "answer": "The `-j4` parameter in a GNU Parallel command specifies that GNU Parallel can run a maximum of four concurrent tasks, launching a new one as soon as an existing one finishes."
  },
  {
    "question": "What should the `params.input` file contain when using GNU Parallel for packing single-GPU jobs?",
    "answer": "The `params.input` file should include input parameters for each task, with each line containing a different input, for example, `code1.py`, `code2.py`, etc."
  },
  {
    "question": "On which clusters is GPU profiling not available, and why?",
    "answer": "GPU profiling is not available on Fir and Nibi because performance counters are not accessible on these clusters."
  },
  {
    "question": "On which clusters is GPU profiling possible?",
    "answer": "GPU profiling is possible on Narval and Rorqual."
  },
  {
    "question": "What is required to enable GPU profiling on Narval and Rorqual?",
    "answer": "To enable GPU profiling on Narval and Rorqual, you must disable the NVIDIA Data Center GPU Manager (DCGM) by setting the `DISABLE_DCGM` environment variable during job submission."
  },
  {
    "question": "How do you disable DCGM during job submission for GPU profiling?",
    "answer": "You can disable DCGM by including `DISABLE_DCGM=1` in your `salloc` command, for example: `DISABLE_DCGM=1 salloc --account=def-someuser --gpus-per-node=1 --mem=4000M --time=03:00`."
  },
  {
    "question": "How do you verify that DCGM has been disabled in an interactive job for profiling?",
    "answer": "You can wait until DCGM is disabled by running: `while [ ! -z \"$(dcgmi -v | grep 'Hostengine build info:')\" ]; do  sleep 5; done`."
  },
  {
    "question": "Where can users find more details about profilers?",
    "answer": "Users can find more details on profilers by referring to the 'Debugging and profiling' documentation."
  }
]