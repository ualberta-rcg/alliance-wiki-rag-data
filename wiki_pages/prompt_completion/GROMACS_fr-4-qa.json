[
  {
    "question": "What precision options are available for CPU-only GROMACS versions?",
    "answer": "CPU-only GROMACS versions are available in single and double precision, except for version 2019.3, where double precision is not available for AVX512."
  },
  {
    "question": "How do you load GROMACS CPU modules for StdEnv/2023?",
    "answer": "You can load GROMACS CPU modules for StdEnv/2023 using the command: `module load StdEnv/2023 gcc/12.3 openmpi/4.1.5 gromacs/2024.4`."
  },
  {
    "question": "How do you load GROMACS CPU modules for StdEnv/2020?",
    "answer": "You can load GROMACS CPU modules for StdEnv/2020 using the command: `module load StdEnv/2020 gcc/9.3.0 openmpi/4.0.3 gromacs/2023.2`."
  },
  {
    "question": "Do these GROMACS versions support GPUs, and if so, what precision?",
    "answer": "Yes, these GROMACS versions can use GPUs, but only in single precision."
  },
  {
    "question": "What command is used to load a GROMACS version with GPU support for StdEnv/2023?",
    "answer": "To load a GROMACS version with GPU support for StdEnv/2023, you must first load the `cuda` module, then use the command: `module load StdEnv/2023 gcc/12.3 openmpi/4.1.5 cuda/12.2 gromacs/2024.4`."
  },
  {
    "question": "What command is used to load a GROMACS version with GPU support for StdEnv/2020?",
    "answer": "To load a GROMACS version with GPU support for StdEnv/2020, you must first load the `cuda` module, then use the command: `module load StdEnv/2020 gcc/9.3.0 cuda/11.4 openmpi/4.0.3 gromacs/2023.2`."
  },
  {
    "question": "What additional module is required to load GROMACS versions with GPU support?",
    "answer": "The `cuda` module must be loaded first to enable GPU support for GROMACS versions."
  },
  {
    "question": "Where can I find more information about environment modules?",
    "answer": "More information on environment modules is available by consulting the 'Utiliser des modules' (Using modules) documentation."
  },
  {
    "question": "How are GROMACS tools handled in versions 5.x, 2016.x, and later?",
    "answer": "In GROMACS versions 5 and later, all tools are implemented as sub-commands of the `gmx` binaries, replacing the separate tools of previous versions."
  },
  {
    "question": "Where can I find documentation on GROMACS 5.0 tool changes?",
    "answer": "You can consult 'GROMACS 5.0 Tool Changes' and the general GROMACS documentation for information on tool changes."
  },
  {
    "question": "What does the `gmx` binary represent in GROMACS versions 5.x and later?",
    "answer": "`gmx` represents GROMACS in mixed (single) precision with OpenMP, but without MPI."
  },
  {
    "question": "What does the `gmx_mpi` binary represent in GROMACS versions 5.x and later?",
    "answer": "`gmx_mpi` represents GROMACS in mixed (simple) precision with OpenMP and MPI."
  },
  {
    "question": "What does the `gmx_d` binary represent in GROMACS versions 5.x and later?",
    "answer": "`gmx_d` represents GROMACS in double precision with OpenMP, but without MPI."
  },
  {
    "question": "What does the `gmx_mpi_d` binary represent in GROMACS versions 5.x and later?",
    "answer": "`gmx_mpi_d` represents GROMACS in double precision with OpenMP and MPI."
  },
  {
    "question": "What are the suffixes for double precision binaries in GROMACS version 4.6.7?",
    "answer": "Double precision binaries in GROMACS version 4.6.7 have the suffix `_d`."
  },
  {
    "question": "What are the names of the parallel simple and double precision `mdrun` binaries in GROMACS version 4.6.7?",
    "answer": "In GROMACS version 4.6.7, the parallel simple precision `mdrun` binary is `mdrun_mpi`, and the parallel double precision binary is `mdrun_mpi_d`."
  },
  {
    "question": "Where can I find information on how to use the Slurm scheduler for job submission?",
    "answer": "You can consult the 'Ex\u00e9cuter des t\u00e2ches' (Running jobs) documentation for information on how to use the Slurm scheduler."
  },
  {
    "question": "What is an example Slurm script for a sequential GROMACS mdrun job using StdEnv/2023?",
    "answer": "A simple Slurm script for a sequential `mdrun` job would include `#SBATCH --time=0-0:30`, `#SBATCH --mem-per-cpu=1000M`, `module load StdEnv/2023 gcc/12.3 openmpi/4.1.5 gromacs/2024.4`, and `gmx mdrun -nt 1 -deffnm em`."
  },
  {
    "question": "What file is used for molecular system simulation execution in the provided sequential job example?",
    "answer": "The molecular system simulation will be executed using the `em.tpr` file."
  },
  {
    "question": "Why might it be necessary to use multiple whole nodes for GROMACS simulations?",
    "answer": "GROMACS simulations often involve very large molecular systems, making it desirable to use multiple whole nodes for computation."
  },
  {
    "question": "How should `--ntasks-per-node` and `--cpus-per-task` generally be configured for GROMACS on a cluster?",
    "answer": "The product of `--ntasks-per-node` and `--cpus-per-task` should generally correspond to the number of CPU cores on the cluster's compute nodes."
  },
  {
    "question": "What issue can arise with domain decomposition on clusters with many CPU cores per node?",
    "answer": "On clusters with a large number of CPU cores (e.g., 192) per compute node, domain decomposition can become a limiting factor when choosing `--ntasks-per-node`, potentially leading to errors if the system size is too small for the requested number of domains."
  },
  {
    "question": "How can domain decomposition errors be addressed if GROMACS reports them?",
    "answer": "If GROMACS reports an error about domain decomposition being impossible, you should halve the `--ntasks-per-node` value and double the `--cpus-per-task` value."
  }
]