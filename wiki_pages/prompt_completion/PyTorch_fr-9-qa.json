[
  {
    "question": "What is Fully Sharded Data Parallelism (FSDP) in PyTorch?",
    "answer": "FSDP enables distributed storage and computing of different elements of a training task, such as optimizer states, model weights, model gradients, and model activations, across multiple devices like GPU, CPU, and local hard disk."
  },
  {
    "question": "What is the primary benefit of using FSDP?",
    "answer": "FSDP allows models with massive amounts of parameters to be trained efficiently across multiple nodes by pooling storage resources."
  },
  {
    "question": "When should you avoid using FSDP?",
    "answer": "You should not use FSDP if your model has layers that do not fit entirely in the memory of a single GPU, as a model layer sharded across devices may still be collected inside a single device during a forward or backward pass."
  },
  {
    "question": "How does Tensor Parallelism (TP) differ from FSDP?",
    "answer": "In Tensor Parallelism, the computation of a forward or backward pass through a model layer is split along with the layers' weights across multiple devices, unlike FSDP where computation steps might still collect shards on a single device."
  },
  {
    "question": "What is an advantage of Tensor Parallelism (TP) regarding computation steps?",
    "answer": "With TP, computation steps are performed locally in the device where a model shard is placed, which avoids the overhead of moving model shards across devices that FSDP might incur."
  },
  {
    "question": "What is Pipeline Parallelism (PP) in the context of model sharding?",
    "answer": "Pipeline Parallelism is a model sharding approach where groups of consecutive layers of a model are placed on different devices, requiring computations on each device to be performed in sequence."
  },
  {
    "question": "How does Pipeline Parallelism mitigate delays caused by sequential processing across devices?",
    "answer": "To mitigate delays, every input batch is broken into 'micro-batches' which are fed to the model in sequence, ensuring all devices stay busy as the first micro-batch reaches the last model block."
  },
  {
    "question": "What is a model checkpoint?",
    "answer": "A checkpoint is a snapshot of your model at a given point during the training process (after a certain number of iterations or epochs) that is saved to disk and can be loaded at a later time."
  },
  {
    "question": "What are the benefits of creating model checkpoints during training?",
    "answer": "Checkpoints are useful for breaking up long jobs into multiple shorter jobs that may get allocated more quickly on a cluster, and for avoiding loss of progress due to unexpected errors or node failures."
  },
  {
    "question": "How do you enable checkpoint creation at the end of each epoch using PyTorch Lightning?",
    "answer": "You can use the `callbacks` parameter of the `Trainer()` class, specifying `pl.callbacks.ModelCheckpoint(dirpath='./ckpt', every_n_epochs=1)`."
  },
  {
    "question": "How are checkpoints handled when using PyTorch Lightning for distributed training?",
    "answer": "With PyTorch Lightning, no additional code is required for distributed training checkpoints; the `callbacks` parameter is simply used as in single-GPU training."
  },
  {
    "question": "When using DistributedDataParallel or Horovod, which process should create checkpoints?",
    "answer": "Checkpoints should be created by a single process (rank), typically rank 0, because all processes will have the same state after each iteration."
  },
  {
    "question": "What steps can be taken to prevent issues when loading checkpoints in distributed training?",
    "answer": "You can add a barrier using `torch.distributed.barrier()` to ensure the checkpoint is fully saved before other processes attempt to load it, and pass `map_location` to `torch.load` to load tensors onto the correct GPU identified by each process (e.g., `map_location=f\"cuda:{local_rank}\"`)."
  },
  {
    "question": "What is the default behavior of `torch.load` regarding tensor placement?",
    "answer": "`torch.load` will by default attempt to load tensors onto the GPU where they were initially saved, which is `cuda:0` in the provided example."
  }
]