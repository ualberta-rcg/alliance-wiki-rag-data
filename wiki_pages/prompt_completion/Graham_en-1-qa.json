[
  {
    "question": "What system has replaced Graham?",
    "answer": "Graham has been retired and replaced by a new system called Nibi."
  },
  {
    "question": "Where can users find information about system capacity, reductions, and outages during the transition to Nibi?",
    "answer": "Users should check the Infrastructure renewal page for system capacity, reductions, and outages during the installation and transition to the new systems."
  },
  {
    "question": "When did the Graham system become available?",
    "answer": "Graham has been in production since June 2017."
  },
  {
    "question": "What is the login node for Graham?",
    "answer": "The login node for Graham is graham.alliancecan.ca."
  },
  {
    "question": "What is the Globus collection name for Graham?",
    "answer": "The Globus collection name for Graham is computecanada#graham-globus."
  },
  {
    "question": "Which nodes should be used for data transfer methods like rsync, scp, or sftp on Graham?",
    "answer": "For data transfer methods like rsync, scp, or sftp, users should use robot or login nodes."
  },
  {
    "question": "What type of cluster is Graham and where is it located?",
    "answer": "Graham is a heterogeneous cluster, suitable for a variety of workloads, and located at the University of Waterloo."
  },
  {
    "question": "Who is Graham named after?",
    "answer": "Graham is named after Wes Graham, the first director of the Computing Centre at Waterloo."
  },
  {
    "question": "What are the parallel filesystem and external persistent storage on Graham similar to?",
    "answer": "The parallel filesystem and external persistent storage on Graham are similar to Cedar's."
  },
  {
    "question": "How is the Graham system cooled?",
    "answer": "Graham is entirely liquid cooled, using rear-door heat exchangers."
  },
  {
    "question": "Where can users find information on getting started with Graham?",
    "answer": "Users can find information on getting started with Graham on the 'Getting started' page."
  },
  {
    "question": "Where can users find instructions on how to run jobs on Graham?",
    "answer": "Instructions on how to run jobs on Graham can be found on the 'Running jobs' page."
  },
  {
    "question": "Where can users find information about transferring data on Graham?",
    "answer": "Information about transferring data on Graham can be found on the 'Transferring data' page."
  },
  {
    "question": "Can Graham's compute nodes access the internet?",
    "answer": "By policy, Graham's compute nodes cannot access the internet."
  },
  {
    "question": "What information is required to request an exception for internet access on Graham's compute nodes?",
    "answer": "To request an exception for internet access, users need to provide IP, Port/s, Protocol (TCP or UDP), Contact, and Removal Date to technical support."
  },
  {
    "question": "Is Crontab available on Graham?",
    "answer": "No, Crontab is not offered on Graham."
  },
  {
    "question": "What is the minimum duration for a regular job on Graham?",
    "answer": "The minimum duration for a regular job on Graham is at least one hour."
  },
  {
    "question": "What is the minimum duration for a test job on Graham?",
    "answer": "The minimum duration for a test job on Graham is five minutes."
  },
  {
    "question": "What is the maximum duration for a job on Graham?",
    "answer": "The maximum duration for a job on Graham is 168 hours (seven days)."
  },
  {
    "question": "What is the maximum number of running and queued jobs a user can have on Graham?",
    "answer": "A user cannot have more than 1000 jobs, running and queued, at any given moment."
  },
  {
    "question": "How is an array job counted towards the total job limit on Graham?",
    "answer": "An array job is counted as the number of tasks in the array."
  },
  {
    "question": "What is the total volume of Home space on Graham?",
    "answer": "The total volume of Home space on Graham is 133TB."
  },
  {
    "question": "What is Home space used for on Graham?",
    "answer": "Home space is the location of home directories."
  },
  {
    "question": "Does each home directory on Graham have a quota?",
    "answer": "Yes, each home directory on Graham has a small, fixed quota."
  },
  {
    "question": "Is Home space allocated via RAS or RAC on Graham?",
    "answer": "No, Home space is not allocated via RAS or RAC."
  },
  {
    "question": "Where should larger storage requests go if not Home space?",
    "answer": "Larger storage requests should go to Project space."
  },
  {
    "question": "Is Home space backed up daily on Graham?",
    "answer": "Yes, Home space has daily backup."
  },
  {
    "question": "What is the total volume of Scratch space on Graham?",
    "answer": "The total volume of Scratch space on Graham is 3.2PB."
  },
  {
    "question": "What type of filesystem is Scratch space?",
    "answer": "Scratch space is a parallel high-performance filesystem."
  },
  {
    "question": "What is Scratch space primarily used for?",
    "answer": "Scratch space is for active or temporary (`/scratch`) storage."
  },
  {
    "question": "Is Scratch space allocated on Graham?",
    "answer": "No, Scratch space is not allocated."
  },
  {
    "question": "Does Scratch space have quotas?",
    "answer": "Yes, Scratch space has a large fixed quota per user."
  },
  {
    "question": "What happens to inactive data in Scratch space?",
    "answer": "Inactive data in Scratch space will be purged."
  },
  {
    "question": "What is the total volume of Project space on Graham?",
    "answer": "The total volume of Project space on Graham is 16PB."
  },
  {
    "question": "What type of storage is Project space?",
    "answer": "Project space is external persistent storage."
  },
  {
    "question": "How is Project space allocated on Graham?",
    "answer": "Project space is allocated via RAS or RAC."
  },
  {
    "question": "Is Project space suitable for parallel I/O workloads?",
    "answer": "No, Project space is not designed for parallel I/O workloads; Scratch space should be used instead."
  },
  {
    "question": "Does Project space have quotas?",
    "answer": "Yes, Project space has a large adjustable quota per project."
  },
  {
    "question": "Is Project space backed up daily on Graham?",
    "answer": "Yes, Project space has daily backup."
  },
  {
    "question": "What types of InfiniBand interconnects are used on Graham?",
    "answer": "Graham uses Mellanox FDR (56Gb/s) and EDR (100Gb/s) InfiniBand interconnects."
  },
  {
    "question": "Which node types use FDR InfiniBand on Graham?",
    "answer": "FDR InfiniBand is used for GPU and cloud nodes on Graham."
  },
  {
    "question": "Which node types use EDR InfiniBand on Graham?",
    "answer": "EDR InfiniBand is used for other node types on Graham."
  },
  {
    "question": "How many ports does the central director switch have on Graham's interconnect?",
    "answer": "The central director switch has 324 ports."
  },
  {
    "question": "How many cloud nodes are on Graham?",
    "answer": "There are 56 cloud nodes on Graham."
  },
  {
    "question": "What connects all nodes and scratch storage on Graham?",
    "answer": "A low-latency high-bandwidth Infiniband fabric connects all nodes and scratch storage."
  },
  {
    "question": "What kind of Ethernet network do cloud-provisioning nodes on Graham have?",
    "answer": "Nodes configurable for cloud provisioning have a 10Gb/s Ethernet network."
  },
  {
    "question": "What is the uplink speed to scratch storage for cloud-provisioning nodes?",
    "answer": "Cloud-provisioning nodes have 40Gb/s uplinks to scratch storage."
  },
  {
    "question": "What is the maximum number of cores Graham is designed to support for simultaneous parallel jobs in a non-blocking manner?",
    "answer": "Graham is designed to support multiple simultaneous parallel jobs of up to 1024 cores in a fully non-blocking manner."
  },
  {
    "question": "What is the blocking factor for larger jobs on Graham's interconnect?",
    "answer": "For larger jobs, the interconnect has an 8:1 blocking factor."
  },
  {
    "question": "What is the address for Graham's dedicated visualization nodes?",
    "answer": "The address for Graham's dedicated visualization nodes is gra-vdi.alliancecan.ca."
  },
  {
    "question": "What type of connections are allowed to Graham's visualization nodes?",
    "answer": "Only VNC connections are allowed to Graham's dedicated visualization nodes."
  },
  {
    "question": "Where can users find instructions on how to use Graham's visualization nodes?",
    "answer": "Instructions on how to use Graham's visualization nodes can be found on the VNC page."
  },
  {
    "question": "Why was Graham's capacity reduced in early 2025?",
    "answer": "Graham's capacity was reduced in early 2025 to make space for the installation of the new Nibi cluster."
  },
  {
    "question": "When was the list of remaining nodes for Graham updated?",
    "answer": "The list of remaining nodes for Graham was updated as of February, 2025."
  },
  {
    "question": "Is Intel Turbo Boost enabled on Graham nodes?",
    "answer": "Yes, Turbo Boost is enabled for all Graham nodes."
  },
  {
    "question": "How many V100 Volta GPUs do the 40-core Intel Xeon Gold 6248 Cascade Lake nodes have?",
    "answer": "The 40-core Intel Xeon Gold 6248 Cascade Lake nodes have 8 NVIDIA V100 Volta GPUs."
  },
  {
    "question": "What kind of CPU is on the 2 Graham nodes with 377GB available memory?",
    "answer": "The 2 Graham nodes with 377GB available memory have 2 x Intel Xeon Gold 6248 Cascade Lake @ 2.5GHz CPUs."
  },
  {
    "question": "How much storage is available on the 6 Graham nodes with Intel Xeon Silver 4110 Skylake CPUs?",
    "answer": "The 6 Graham nodes with Intel Xeon Silver 4110 Skylake CPUs have 11.0TB SATA SSD storage."
  },
  {
    "question": "What type of CPU and GPU are on the 30 Graham nodes with 44 cores and 187G available memory?",
    "answer": "The 30 Graham nodes have 2 x Intel Xeon Gold 6238 Cascade Lake @ 2.10GHz CPUs and 4 x NVIDIA T4 Turing GPUs."
  },
  {
    "question": "What is the storage capacity for the 136 Graham nodes without GPUs?",
    "answer": "The 136 Graham nodes without GPUs have 879GB SATA SSD storage."
  },
  {
    "question": "Which Graham node configuration offers 2000GB of memory and 8 NVIDIA A100 Ampere GPUs?",
    "answer": "One Graham node has 128 cores, 2000G of memory, 2 x AMD EPYC 7742 CPUs, 3.5TB SATA SSD storage, and 8 x NVIDIA A100 Ampere GPUs."
  },
  {
    "question": "How many NVIDIA A100 Ampere GPUs are available on the Graham nodes with 2 x Intel Xeon Gold 6326 Cascade Lake CPUs?",
    "answer": "The Graham nodes with 2 x Intel Xeon Gold 6326 Cascade Lake CPUs have 4 NVIDIA A100 Ampere GPUs."
  },
  {
    "question": "What CPU is used in the 11 Graham nodes equipped with 4 x NVIDIA RTX A5000 Ampere GPUs?",
    "answer": "The 11 Graham nodes with 4 x NVIDIA RTX A5000 Ampere GPUs use 1 x AMD EPYC 7713 CPU."
  },
  {
    "question": "How much storage is available on the 6 Graham nodes with 1024G of memory and an AMD EPYC 7543 CPU?",
    "answer": "The 6 Graham nodes with 1024G of memory and an AMD EPYC 7543 CPU have 8x2TB NVMe storage."
  },
  {
    "question": "What is the recommendation for selecting specific node types for jobs on Graham?",
    "answer": "It is recommended not to select a specific node type for jobs, as performance differences between Skylake and Cascade Lake nodes are expected to be small compared to job waiting times."
  },
  {
    "question": "How can a user constrain a CPU job to a Cascade Lake node on Graham?",
    "answer": "To constrain a CPU job to a Cascade Lake node, use `--constraint=cascade`."
  },
  {
    "question": "What is the best practice for using local on-node storage on Graham?",
    "answer": "The best practice for local on-node storage is to use the temporary directory generated by Slurm, `$SLURM_TMPDIR`."
  },
  {
    "question": "What happens to the contents of `$SLURM_TMPDIR` after a job completes?",
    "answer": "The `$SLURM_TMPDIR` directory and its contents will disappear upon job completion."
  },
  {
    "question": "What happens if a job's memory requirements exceed the available memory on Graham?",
    "answer": "The scheduler will never allocate jobs whose memory requirements exceed the specified amount of available memory to avoid swapping/paging."
  },
  {
    "question": "How can an I/O-intensive job benefit when requesting memory on Graham?",
    "answer": "An I/O-intensive job will often benefit from requesting somewhat more memory than the aggregate size of processes."
  },
  {
    "question": "How many generations of Tesla GPUs does Graham contain?",
    "answer": "Graham contains Tesla GPUs from three different generations."
  },
  {
    "question": "Which generation of GPUs on Graham includes V100 Volta GPUs?",
    "answer": "V100 Volta GPUs are one of the generations available on Graham."
  },
  {
    "question": "Which generation of GPUs on Graham includes T4 Turing GPUs?",
    "answer": "T4 Turing GPUs are one of the generations available on Graham."
  },
  {
    "question": "Which generation of GPUs on Graham includes A100 Ampere GPUs?",
    "answer": "A100 Ampere GPUs are one of the generations available on Graham."
  },
  {
    "question": "Have P100 GPUs been decommissioned on Graham?",
    "answer": "Yes, P100 GPUs have been decommissioned."
  },
  {
    "question": "How does the V100 GPU compare to its predecessor (P100) in performance?",
    "answer": "The V100 GPU offers about double the performance for standard computation and about 8X performance for deep learning computations compared to its predecessor."
  },
  {
    "question": "What are the key features and limitations of T4 Turing GPUs on Graham?",
    "answer": "T4 Turing GPUs are targeted specifically at deep learning workloads, do not support efficient double precision computations, but have good performance for single precision, tensor cores, and support for reduced precision integer calculations."
  },
  {
    "question": "Are Pascal GPU nodes available on Graham?",
    "answer": "No, Pascal GPU nodes are no longer available on Graham."
  },
  {
    "question": "How many Volta nodes does Graham have?",
    "answer": "Graham has a total of 2 Volta nodes."
  },
  {
    "question": "Do Volta nodes on Graham have NVLINK interconnect?",
    "answer": "Yes, Volta nodes on Graham have high bandwidth NVLINK interconnect."
  },
  {
    "question": "Who can use the Volta GPU nodes on Graham?",
    "answer": "The Volta GPU nodes are available to all users."
  },
  {
    "question": "What is the maximum job duration for Volta GPU nodes on Graham?",
    "answer": "The maximum job duration for Volta GPU nodes is seven days."
  },
  {
    "question": "What does the `module load arch/avx512 StdEnv/2018.3` command ensure in a Volta GPU job script?",
    "answer": "The `module load arch/avx512 StdEnv/2018.3` command ensures that modules compiled for Skylake architecture will be used."
  },
  {
    "question": "What is the recommended CPU to GPU ratio for 28-core Volta nodes on Graham?",
    "answer": "For 28-core Volta nodes, the recommended ratio of CPUs to GPUs is 3.5 or less."
  },
  {
    "question": "How many CPU cores should be requested for a job using 4 GPUs on a 28-core Volta node?",
    "answer": "For a job using 4 GPUs on a 28-core Volta node, at most 14 CPU cores should be requested."
  },
  {
    "question": "How many CPU cores should be requested for a job using 1 GPU on a 28-core Volta node?",
    "answer": "For a job with 1 GPU on a 28-core Volta node, at most 3 CPU cores should be requested."
  },
  {
    "question": "Are users allowed to break the CPU to GPU ratio rule for testing purposes on Volta nodes?",
    "answer": "Yes, users are allowed to run a few short test jobs (shorter than 1 hour) that break this rule to see how their code performs."
  },
  {
    "question": "How many CPU cores per GPU can be used on the newest 40-core Volta nodes with NVLINK?",
    "answer": "On the newest 40-core Volta nodes with NVLINK, users can use 5 CPU cores per GPU."
  },
  {
    "question": "How do you request a Volta NVLINK node with the `--constraint=cascade,v100` parameter?",
    "answer": "To request a Volta NVLINK node, add the `--constraint=cascade,v100` parameter to the job submission script."
  },
  {
    "question": "Do Volta nodes have fast local disks, and when should they be used?",
    "answer": "Yes, Volta nodes have a fast local disk, which should be used for jobs if the amount of I/O performed by the job is significant."
  },
  {
    "question": "What environment variable specifies the location of the temporary directory on the fast local disk for jobs on Volta nodes?",
    "answer": "The environment variable `$SLURM_TMPDIR` specifies the location of the temporary directory on the fast local disk for jobs on Volta nodes."
  },
  {
    "question": "What is the recommended workflow for using `$SLURM_TMPDIR` on Volta nodes for I/O intensive jobs?",
    "answer": "Users should copy input files to `$SLURM_TMPDIR` at the start of the job script and output files out at the end of the job script."
  },
  {
    "question": "Do users need to manually clean up files in `$SLURM_TMPDIR` after a job on Volta nodes?",
    "answer": "No, all files in `$SLURM_TMPDIR` will be removed once the job ends."
  },
  {
    "question": "Can Python virtual environments be created in `$SLURM_TMPDIR` on Volta nodes?",
    "answer": "Yes, Python virtual environments can be created in this temporary space for greater efficiency."
  },
  {
    "question": "How do you request two T4 GPUs on Graham?",
    "answer": "To request two T4 GPUs, you should specify `--gres=gpu:t4:2`."
  },
  {
    "question": "How do you request two A100 Ampere GPUs on Graham?",
    "answer": "To request two A100 Ampere GPUs, you should specify `--gres=gpu:a100:2`."
  },
  {
    "question": "How do you request two RTX A5000 Ampere GPUs on Graham?",
    "answer": "To request two RTX A5000 Ampere GPUs, you should specify `--gres=gpu:a5000:2`."
  }
]