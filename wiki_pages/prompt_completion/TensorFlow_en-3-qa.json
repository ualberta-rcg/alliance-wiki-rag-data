[
  {
    "question": "How does TensorFlow 2.x offer strategies for utilizing multiple GPUs?",
    "answer": "TensorFlow 2.x provides different strategies for using multiple GPUs through its high-level API `tf.distribute`."
  },
  {
    "question": "Which strategy is recommended for TensorFlow 2.x multi-GPU training on a single node?",
    "answer": "For multi-GPU training on a single node in TensorFlow 2.x, the `tf.distribute.MirroredStrategy()` is recommended."
  },
  {
    "question": "What SLURM parameters are used to request resources for a single-node TensorFlow 2.x MirroredStrategy job with 4 GPUs?",
    "answer": "The SLURM script for a single-node MirroredStrategy job requests resources using `#SBATCH --nodes 1` and `#SBATCH --gres=gpu:4`."
  },
  {
    "question": "How do you install TensorFlow within a virtual environment for a single-node MirroredStrategy job?",
    "answer": "After loading the `python/3` module and creating/activating a virtual environment in `$SLURM_TMPDIR/env`, TensorFlow is installed using `pip install --no-index tensorflow`."
  },
  {
    "question": "What environment variable can be set to use the NCCL backend for inter-GPU communication with MirroredStrategy?",
    "answer": "You can set the environment variable `export NCCL_BLOCKING_WAIT=1` to use the NCCL backend for inter-GPU communication."
  },
  {
    "question": "What is the primary difference in strategy implementation for multi-node TensorFlow 2.x GPU training compared to single-node?",
    "answer": "For multiple nodes, `tf.distribute.MultiWorkerMirroredStrategy()` is used instead of `tf.distribute.MirroredStrategy()`, and `tf.distribute.cluster_resolver.SlurmClusterResolver()` is employed to get job information from SLURM."
  },
  {
    "question": "Why is `CommunicationImplementation.NCCL` explicitly added to `MultiWorkerMirroredStrategy` but not `MirroredStrategy`?",
    "answer": "It is necessary to explicitly add `CommunicationImplementation.NCCL` to `MultiWorkerMirroredStrategy()` because NCCL is the default backend for `MirroredStrategy()` in the single-node case but needs to be specified for multi-worker setups."
  },
  {
    "question": "If the CIFAR10 dataset is not stored locally, how can it be obtained for the example TensorFlow 2.x scripts?",
    "answer": "The CIFAR10 dataset can be downloaded from `https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz`, renamed to 'cifar-10-batches-py.tar.gz', and placed under `~/.keras/datasets`, or by running the dataset loading line on a login node prior to job submission."
  }
]