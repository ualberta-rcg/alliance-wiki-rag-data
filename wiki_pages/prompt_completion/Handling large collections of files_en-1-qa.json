[
  {
    "question": "What problem can arise when managing a very large number of small files in domains like AI and Machine Learning on clusters?",
    "answer": "A problem arises due to filesystem quotas on clusters that limit the number of filesystem objects, which creates significant issues for the performance of shared filesystems and automated backups."
  },
  {
    "question": "How can a user identify folders with a large number of files in the current directory?",
    "answer": "A user can use the following code: `for FOLDER in $(find . -maxdepth 1 -type d | tail -n +2); do echo -ne \"$FOLDER:\\t\"; find $FOLDER -type f | wc -l; done` to recursively count all files in folders."
  },
  {
    "question": "What command can be used to find the top 10 directories using the most disk space from the current directory?",
    "answer": "The command `du -sh * | sort -hr | head -10` will output the 10 directories using the most disk space from your current directory."
  },
  {
    "question": "What are the general performance characteristics of local disks attached to compute nodes?",
    "answer": "Local disks attached to compute nodes are at least SATA SSD or better, and generally offer considerably better performance than project or scratch filesystems."
  },
  {
    "question": "How is local disk space allocated or shared on compute nodes?",
    "answer": "A local disk is shared by all running jobs on that node without being allocated by the scheduler."
  },
  {
    "question": "How much local disk space is available on B\u00e9luga's CPU and GPU nodes?",
    "answer": "B\u00e9luga offers roughly 370GB of local disk for the CPU nodes, and GPU nodes have a 1.6TB NVMe disk (1.6TB)."
  },
  {
    "question": "Do Niagara's compute nodes have local storage?",
    "answer": "Niagara does not have local storage on its compute nodes."
  },
  {
    "question": "What is the assumed minimum local disk size for clusters other than B\u00e9luga and Niagara?",
    "answer": "For other clusters, you can assume the available disk size to be at least 190GB."
  },
  {
    "question": "How can local disk be accessed inside a job?",
    "answer": "You can access local disk inside of a job using the environment variable `$SLURM_TMPDIR`."
  },
  {
    "question": "Describe a common approach for using local disk with datasets within a job.",
    "answer": "One approach is to keep your dataset archived as a single `tar` file in the project space, copy it to the local disk at the beginning of your job, extract it, use the dataset, and then re-archive the contents and copy it back to the project space at the job's end if changes were made."
  },
  {
    "question": "Which filesystem can be utilized as a RAM disk on compute nodes?",
    "answer": "The `/tmp` file system can be used as a RAM disk on the compute nodes."
  },
  {
    "question": "What technology is used to implement the `/tmp` RAM disk?",
    "answer": "The `/tmp` file system, when used as a RAM disk, is implemented using `tmpfs`."
  },
  {
    "question": "Is the contents of `/tmp` cleared after a job completes?",
    "answer": "Yes, `/tmp` is cleared at job end."
  },
  {
    "question": "How does the memory usage of the `/tmp` RAM disk relate to job requests?",
    "answer": "Like all of a job's other memory use, the `/tmp` RAM disk's memory usage falls under the cgroup limit corresponding to the sbatch request."
  },
  {
    "question": "What is dar?",
    "answer": "dar is a Disk archive utility, conceived of as a significant modernization of the venerable `tar` tool."
  },
  {
    "question": "What is HDF5 used for?",
    "answer": "HDF5 is a high-performance binary file format that can be used to store a variety of different kinds of data, including extended objects such as matrices and image data."
  },
  {
    "question": "Are there tools for manipulating HDF5 files in common programming languages?",
    "answer": "Yes, there exist tools for manipulating HDF5 files in several common programming languages, such as Python (e.g., `h5py`)."
  }
]