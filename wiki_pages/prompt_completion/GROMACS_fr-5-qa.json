[
  {
    "question": "What is the primary purpose of the provided Slurm scripts for GROMACS?",
    "answer": "These Slurm scripts are designed to run GROMACS simulations on entire compute nodes."
  },
  {
    "question": "Which specific GROMACS executable is used in these whole-node job scripts?",
    "answer": "The `gmx_mpi` executable is used, which implies a GROMACS version compiled for mixed (simple) precision with OpenMP and MPI support."
  },
  {
    "question": "What environment modules are loaded as a prerequisite for running GROMACS in these scripts?",
    "answer": "The scripts load `StdEnv/2023`, `gcc/12.3`, `openmpi/4.1.5`, and `gromacs/2024.4`."
  },
  {
    "question": "How is the `OMP_NUM_THREADS` environment variable configured in these Slurm scripts?",
    "answer": "The `OMP_NUM_THREADS` environment variable is set to the value of `SLURM_CPUS_PER_TASK`, defaulting to 1 if `SLURM_CPUS_PER_TASK` is not defined, using `export OMP_NUM_THREADS=\"${SLURM_CPUS_PER_TASK:-1}\"`."
  },
  {
    "question": "What is the `srun` command used to initiate the GROMACS simulation?",
    "answer": "The `srun` command used is `srun --cpus-per-task=$OMP_NUM_THREADS gmx_mpi mdrun -deffnm md`."
  },
  {
    "question": "What is the requested memory limit per CPU for the GROMACS whole-node jobs?",
    "answer": "The requested memory limit is 2000 MB per CPU (`--mem-per-cpu=2000M`)."
  },
  {
    "question": "What is the specified time limit for these GROMACS whole-node Slurm jobs?",
    "answer": "The time limit for the jobs is 1 hour (`--time=0-01:00`)."
  },
  {
    "question": "How many nodes are requested for each of the GROMACS whole-node job examples?",
    "answer": "All the provided scripts request 1 node (`--nodes=1`)."
  },
  {
    "question": "For a GROMACS job on a Narval cluster node, how many MPI tasks per node are requested and what is the total number of CPUs?",
    "answer": "On Narval, 32 MPI tasks per node (`--ntasks-per-node=32`) are requested, with 2 OpenMP threads per MPI task (`--cpus-per-task=2`), totaling 64 CPUs per node (32 x 2)."
  },
  {
    "question": "For GROMACS jobs on Rorqual, Fir, Nibi, or Trillium cluster nodes, what are the `--ntasks-per-node` and `--cpus-per-task` settings, and how many total CPUs does this provide?",
    "answer": "For Rorqual, Fir, Nibi, or Trillium, 96 MPI tasks per node (`--ntasks-per-node=96`) are requested, with 2 OpenMP threads per MPI task (`--cpus-per-task=2`), resulting in a total of 192 CPUs per node (96 x 2)."
  },
  {
    "question": "What GROMACS version and related software components are used in the whole-node scripts for StdEnv/2023?",
    "answer": "The scripts specify `gromacs/2024.4`, `gcc/12.3`, and `openmpi/4.1.5` within the `StdEnv/2023` environment."
  }
]