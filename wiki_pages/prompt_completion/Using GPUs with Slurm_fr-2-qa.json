[
  {
    "question": "What is MIG (Multi-Instance GPU) technology?",
    "answer": "MIG technology allows for partitioning a GPU into multiple instances."
  },
  {
    "question": "Why is it important to specify the type of GPU when submitting a task?",
    "answer": "It is important to specify the GPU type because some clusters have more than one type of GPU, and on others, GPUs are only available on certain nodes. Without specification, Slurm might randomly route your task to any GPU-equipped node, which may not be desirable."
  },
  {
    "question": "What resources are required for a task using a GPU instance?",
    "answer": "Each GPU instance requires a certain number of CPU cores (1 by default) and a specific amount of system memory."
  },
  {
    "question": "What is the default number of CPU cores allocated per GPU instance?",
    "answer": "The default number of CPU cores allocated per GPU instance is 1."
  },
  {
    "question": "What is the recommended maximum number of CPU cores per GPU on the Fir cluster?",
    "answer": "On the Fir cluster, a maximum of 12 CPU cores is recommended per GPU."
  },
  {
    "question": "What is the recommended maximum number of CPU cores per GPU on the Narval cluster?",
    "answer": "On the Narval cluster, a maximum of 12 CPU cores is recommended per GPU."
  },
  {
    "question": "What is the recommended maximum number of CPU cores per GPU on the Nibi cluster?",
    "answer": "On the Nibi cluster, a maximum of 14 CPU cores is recommended per GPU."
  },
  {
    "question": "What is the recommended maximum number of CPU cores per GPU on the Rorqual cluster?",
    "answer": "On the Rorqual cluster, a maximum of 16 CPU cores is recommended per GPU."
  },
  {
    "question": "How can you request a single GPU and 4000MB of memory for a serial job using Slurm?",
    "answer": "For a serial job, you can use the Slurm directives `#SBATCH --gpus-per-node=1` and `#SBATCH --mem=4000M`."
  },
  {
    "question": "How do you specify multiple CPU cores for a multi-threaded GPU job in Slurm?",
    "answer": "For a multi-threaded GPU job, you can specify multiple CPU cores using `#SBATCH --cpus-per-task=N` (e.g., `--cpus-per-task=6`) and set `export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK`."
  },
  {
    "question": "How can requesting an entire node potentially benefit your application?",
    "answer": "If your application can efficiently use an entire node and its associated GPUs, requesting a whole node can probably reduce the waiting time."
  },
  {
    "question": "What tool is recommended for grouping single-GPU tasks that run for more than 24 hours?",
    "answer": "GNU Parallel is recommended for grouping single-GPU tasks, especially those running for more than 24 hours."
  },
  {
    "question": "In GNU Parallel, how is the GPU identifier determined using `CUDA_VISIBLE_DEVICES` with `{%}`?",
    "answer": "The GPU identifier is calculated by subtracting 1 from the slot identifier (`{%}`)."
  },
  {
    "question": "What does the `{#}` variable represent in a GNU Parallel command?",
    "answer": "The `{#}` variable represents the task identifier, with values starting from 1."
  },
  {
    "question": "What is the function of the `-j4` parameter in a GNU Parallel command?",
    "answer": "The `-j4` parameter ensures that GNU Parallel will execute four tasks concurrently, launching a new task as soon as the previous one is finished."
  },
  {
    "question": "How can you prevent tasks from competing for the same GPU when using GNU Parallel?",
    "answer": "To prevent tasks from competing for the same GPU, you should use `CUDA_VISIBLE_DEVICES` with GNU Parallel."
  },
  {
    "question": "What is the expected format for the `params.input` file when used with GNU Parallel for task grouping?",
    "answer": "The `params.input` file should contain the parameters for each task on separate lines, like `code1.py`, `code2.py`, etc."
  }
]