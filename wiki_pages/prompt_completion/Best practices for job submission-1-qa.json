[
  {
    "question": "What is the primary goal when choosing parameters for submitting a job on a cluster?",
    "answer": "The primary goal is to ensure your job doesn't waste resources or create problems for other users and yourself, leading to quicker starts, correct finishes, and desired output for your research."
  },
  {
    "question": "Why is it difficult to estimate job resources for first-time users?",
    "answer": "For your first jobs, it's understandably difficult to estimate how much time or memory may be needed for a particular simulation or analysis."
  },
  {
    "question": "What happens if a job requests too many resources?",
    "answer": "The more resources (time, memory, CPU cores, GPUs) your job asks for, the more difficult it will be for the scheduler to find these resources, resulting in a longer wait in the queue."
  },
  {
    "question": "What is the consequence of requesting insufficient resources for a job?",
    "answer": "If not enough resources are requested, the job can be stopped if it exceeds its time limit or its memory limit."
  },
  {
    "question": "Why might estimating required resources based on a local computer be inaccurate?",
    "answer": "Estimating required resources based on the performance of a local computer could be misleading because the processor type and speed on the cluster can differ significantly."
  },
  {
    "question": "Name some reasons why compute tasks in a job script might waste resources.",
    "answer": "Compute tasks might waste resources if the program does not scale well with the number of CPU cores, is not made for multiple node usage, or if the processors are waiting after read-write operations."
  },
  {
    "question": "What is the recommended initial approach for submitting jobs on the cluster?",
    "answer": "The best approach is to begin by submitting a few relatively small test jobs, asking for a fairly standard amount of memory (e.g., `#SBATCH --mem-per-cpu=2G`) and time, for example one or two hours."
  },
  {
    "question": "Why should you know the expected outcome of test jobs?",
    "answer": "Ideally, you should already know what the answer will be in these test jobs, allowing you to verify that the software is running correctly on the cluster."
  },
  {
    "question": "How should you adjust a job's duration if it ends prematurely during testing?",
    "answer": "If the job ends before the computation finished, you can increase the duration by doubling it until the job's duration is sufficient."
  },
  {
    "question": "What action should be taken if a job terminates with an 'OOM event' message?",
    "answer": "If your job ends with a message about an 'OOM event' (Out Of Memory), you should try doubling the memory you've requested."
  },
  {
    "question": "What is the benefit of running test jobs before submitting more realistic ones?",
    "answer": "By means of these test jobs, you should gain some familiarity with how long certain analyses require on the cluster and how much memory is needed, enabling you to make an intelligent estimate for more realistic jobs."
  },
  {
    "question": "What is the recommended minimum duration for non-test jobs?",
    "answer": "For jobs which are not tests, the duration should be at least one hour."
  },
  {
    "question": "What should you do if your computation requires less than an hour to complete?",
    "answer": "If your computation requires less than an hour, you should consider using tools like GLOST, META, or GNU Parallel to regroup several of your computations into a single Slurm job with a duration of at least an hour to avoid stressing the scheduler."
  },
  {
    "question": "Why is it important for your job duration estimate to be relatively accurate?",
    "answer": "An accurate estimate of the job duration is important because overestimating (e.g., asking for five days when it finishes in sixteen hours) leads to your job spending much more time waiting to start than necessary."
  },
  {
    "question": "Which monitoring tool can be used to check the duration of completed jobs, and what field should be looked at?",
    "answer": "You can use monitoring tools like the `seff` command, specifically checking the `Job Wall-clock time` field in its output, to see how long completed jobs took."
  },
  {
    "question": "By what percentage should you increase the estimated duration of a job as a buffer?",
    "answer": "You should increase the estimated duration by 5% or 10%, just in case, to leave a certain amount of room for error in the estimate."
  },
  {
    "question": "What should be considered for jobs with a duration exceeding 48 hours?",
    "answer": "Longer jobs, such as those with a duration exceeding 48 hours, should consider using checkpoints if the software permits this."
  },
  {
    "question": "How do checkpoints benefit long-running jobs?",
    "answer": "With a checkpoint, the program writes a snapshot of its state to a diskfile, and it can then be restarted from this diskfile at that precise point in the calculation, preventing significant work loss even if there's an interruption."
  },
  {
    "question": "How often should a program ideally write checkpoint files for long jobs to minimize work loss?",
    "answer": "To minimize work loss, a program should ideally write a checkpoint file every six or eight hours."
  }
]