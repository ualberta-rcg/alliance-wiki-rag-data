[
  {
    "question": "When does the Cedar service end?",
    "answer": "The Cedar service ends on 2025 September 12."
  },
  {
    "question": "What is the login node for the Cedar cluster?",
    "answer": "The login node for Cedar is cedar.alliancecan.ca."
  },
  {
    "question": "What is the Globus collection name for Cedar?",
    "answer": "The Globus collection name for Cedar is computecanada#cedar-globus."
  },
  {
    "question": "Where can I find the system status page for Cedar?",
    "answer": "The system status page for Cedar is at https://status.alliancecan.ca/."
  },
  {
    "question": "Where is the Cedar cluster located?",
    "answer": "The Cedar cluster is located at Simon Fraser University."
  },
  {
    "question": "What is the Cedar cluster named after?",
    "answer": "Cedar is named for the Western Red Cedar, B.C.\u2019s official tree."
  },
  {
    "question": "Who sells and supports the Cedar cluster?",
    "answer": "Cedar is sold and supported by Scalar Decisions, Inc."
  },
  {
    "question": "Which manufacturers provide components for Cedar nodes, /scratch filesystem, and interconnect?",
    "answer": "Dell is the node manufacturer, DDN provides the /scratch filesystem, and Intel provides the interconnect."
  },
  {
    "question": "How is the Cedar cluster cooled?",
    "answer": "Cedar is entirely liquid-cooled, using rear-door heat exchangers."
  },
  {
    "question": "Which Globus endpoint should be used for Cedar?",
    "answer": "Please use the version 5 endpoint computecanada#cedar-globus."
  },
  {
    "question": "What is the total volume of the Home space on Cedar?",
    "answer": "The Home space on Cedar has a total volume of 526TB."
  },
  {
    "question": "What is the primary function of the /home directories on Cedar?",
    "answer": "The /home directories are located in the Home space."
  },
  {
    "question": "Is the Home space backed up daily?",
    "answer": "Yes, the Home space has daily backup."
  },
  {
    "question": "What is the total volume of the Scratch space on Cedar?",
    "answer": "The Scratch space on Cedar has a total volume of 5.4PB."
  },
  {
    "question": "What type of filesystem is the Scratch space?",
    "answer": "The Scratch space is a parallel high-performance filesystem."
  },
  {
    "question": "What happens to inactive data in the Scratch space?",
    "answer": "Inactive data in the Scratch space will be purged."
  },
  {
    "question": "What is the total volume of the Project space on Cedar?",
    "answer": "The Project space on Cedar has a total volume of 23PB."
  },
  {
    "question": "Is the Project space designed for parallel I/O workloads?",
    "answer": "No, Project space is not designed for parallel I/O workloads; /scratch space should be used instead."
  },
  {
    "question": "Does the Project space have daily backup?",
    "answer": "Yes, the Project space has daily backup."
  },
  {
    "question": "What technology is the /scratch storage space based on?",
    "answer": "The /scratch storage space is a Lustre filesystem based on DDN model ES14K technology."
  },
  {
    "question": "What kind of interconnect does Cedar use and what is its bandwidth?",
    "answer": "Cedar uses an Intel OmniPath (version 1) interconnect with 100Gbit/s bandwidth."
  },
  {
    "question": "What is the blocking factor for larger jobs on Cedar's interconnect?",
    "answer": "For larger jobs, the interconnect has a 2:1 blocking factor."
  },
  {
    "question": "How many CPU cores and GPU devices does Cedar have for computation?",
    "answer": "Cedar has 100,400 CPU cores for computation and 1352 GPU devices."
  },
  {
    "question": "Is Turbo Boost activated on Cedar nodes?",
    "answer": "No, Turbo Boost is deactivated for all Cedar nodes."
  },
  {
    "question": "How many Broadwell nodes with 125G of available memory are there?",
    "answer": "There are 256 Broadwell nodes, each with 32 cores and 125G (or 128000M) of available memory."
  },
  {
    "question": "What are the specifications of the nodes using AMD EPYC 7302 CPUs?",
    "answer": "There are 6 such nodes, each with 32 cores, 4000G (or 4096000M) available memory, and 2 x 480G SSD storage."
  },
  {
    "question": "Which GPU nodes are equipped with 4 x NVIDIA P100 Pascal (12G HBM2 memory)?",
    "answer": "There are 96 nodes with 24 cores, 125G available memory, 2 x Intel E5-2650 v4 Broadwell @ 2.2GHz CPUs, and 1 x 800G SSD storage that are equipped with 4 x NVIDIA P100 Pascal (12G HBM2 memory)."
  },
  {
    "question": "Why is the available memory on Cedar nodes less than the physical RAM?",
    "answer": "The available memory is less than the physical RAM because some of it is permanently occupied by the kernel and OS."
  },
  {
    "question": "How does the scheduler manage memory allocation to avoid swapping?",
    "answer": "The scheduler will never allocate jobs whose memory requirements exceed the amount of available memory shown to avoid wasting time by swapping/paging."
  },
  {
    "question": "Do Cedar nodes have local temporary storage, and how should it be used?",
    "answer": "Yes, all nodes have local (on-node) temporary storage. It should be used through the job-specific directory created by the scheduler, `$SLURM_TMPDIR`."
  },
  {
    "question": "What is the local temporary storage capacity for compute nodes (excluding GPU nodes)?",
    "answer": "Compute nodes (except GPU nodes) have two 480GB SSD drives, for a total raw capacity of 960GB."
  },
  {
    "question": "Are 48-core nodes or 32-core nodes reserved for jobs requiring whole nodes?",
    "answer": "A number of 48-core nodes are reserved for jobs that require whole nodes; no 32-core nodes are set aside for whole node processing."
  },
  {
    "question": "Can jobs requesting less than 48 cores per node share nodes with other jobs?",
    "answer": "Yes, jobs that request less than 48 cores per node can end up sharing nodes with other jobs."
  },
  {
    "question": "Is it recommended to select a specific node type (Broadwell, Skylake, Cascade Lake) for jobs?",
    "answer": "No, it is recommended that you do not select a specific node type for your jobs, as performance differences are expected to be small compared to job waiting times."
  },
  {
    "question": "How can a user specify a Cascade Lake node type if necessary?",
    "answer": "Users can specify a Cascade Lake node type using `--constraint=cascade`."
  },
  {
    "question": "What constraint should be used for jobs requiring any AVX512 node?",
    "answer": "For jobs requiring any AVX512 node, use `--constraint=[skylake|cascade]`."
  },
  {
    "question": "When did the policy forbidding jobs from running in the /home filesystem take effect?",
    "answer": "The policy took effect as of April 17, 2019."
  },
  {
    "question": "What should a user do if they receive a message indicating that submitting jobs from /home is not permitted?",
    "answer": "Users should transfer the files either to their /project or /scratch directory and submit the job from there."
  },
  {
    "question": "What is Cedar's theoretical peak double precision performance for CPUs and GPUs?",
    "answer": "Cedar's theoretical peak double precision performance is 6547 teraflops for CPUs and 7434 teraflops for GPUs."
  },
  {
    "question": "What is the total theoretical peak double precision performance of Cedar?",
    "answer": "The total theoretical peak double precision performance of Cedar is almost 14 petaflops."
  },
  {
    "question": "How is Cedar's network topology structured?",
    "answer": "Cedar's network topology is made up of islands with a 2:1 blocking factor between islands."
  },
  {
    "question": "What is the interconnect blocking factor within an island?",
    "answer": "Within an island, the interconnect (Omni-Path fabric) is fully non-blocking."
  },
  {
    "question": "How many Broadwell nodes are there in each of the 16 islands?",
    "answer": "Each of the 16 islands contains 32 Broadwell nodes, totaling 1024 cores per island."
  }
]