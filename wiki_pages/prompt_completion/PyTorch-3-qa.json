[
  {
    "question": "How can you download the CIFAR10 dataset for the provided CPU performance test script?",
    "answer": "To download the CIFAR10 dataset, first create a 'data' directory and navigate into it (`mkdir -p data && cd data`). Then, use `wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz` to download the archive and `tar zxf cifar-10-python.tar.gz` to extract it."
  },
  {
    "question": "What is the purpose of the `pytorch-multi-cpu.sh` script?",
    "answer": "The `pytorch-multi-cpu.sh` script is designed to demonstrate the effect of PyTorch's native support for parallelism on performance by requesting different numbers of CPUs without requiring code changes."
  },
  {
    "question": "How does the `pytorch-multi-cpu.sh` script prepare the environment for running a PyTorch job?",
    "answer": "The script loads a Python module, creates and activates a virtual environment, and then installs `torch` and `torchvision` using `pip install --no-index`."
  },
  {
    "question": "How is intra-op parallelism enabled for CPU-based PyTorch jobs in the `pytorch-multi-cpu.sh` script?",
    "answer": "Intra-op parallelism is enabled by exporting the `OMP_NUM_THREADS` environment variable and setting it to the value of `SLURM_CPUS_PER_TASK`."
  },
  {
    "question": "Is it always recommended to use a GPU for model training if one is available on HPC clusters?",
    "answer": "No, it is a common misconception. While training very small models might be faster on a GPU on a local workstation, it's not the case on HPC clusters, and you should not request a GPU if your code cannot make reasonable use of its compute capacity."
  },
  {
    "question": "What are the two primary reasons GPUs offer a performance advantage in Deep Learning tasks?",
    "answer": "GPUs gain performance from their ability to parallelize key numerical operations over thousands of compute cores (compared to single-digit CPU cores) and their much higher memory bandwidth, allowing them to process larger amounts of data efficiently."
  },
  {
    "question": "Which GPU-specific libraries does PyTorch use for parallel implementations of operators?",
    "answer": "PyTorch uses GPU-specific libraries such as CUDNN or MIOpen for parallel implementations of operators like matrix multiplication and convolution, depending on the hardware platform."
  },
  {
    "question": "When is a learning task considered suitable for running on a GPU?",
    "answer": "A learning task is suitable for running on a GPU when it involves elements that scale out with massive parallelism, either in terms of the number of operations, the amount of data required, or ideally both, such as large models or large inputs."
  },
  {
    "question": "What two parameters are highlighted as important for influencing GPU performance in the provided example?",
    "answer": "The two parameters highlighted as important for influencing GPU performance are `batch_size` and `num_workers`."
  },
  {
    "question": "How does increasing `batch_size` improve GPU performance?",
    "answer": "Increasing `batch_size` improves GPU performance by increasing the size of the inputs at each iteration, which allows more of the GPU's capacity to be utilized."
  },
  {
    "question": "How do `num_workers` influence GPU performance by streamlining data movement?",
    "answer": "`num_workers` influence GPU performance by streamlining the movement of inputs from the Host's (CPU's) memory to the GPU's memory, which reduces the time the GPU spends idle waiting for data to process."
  }
]