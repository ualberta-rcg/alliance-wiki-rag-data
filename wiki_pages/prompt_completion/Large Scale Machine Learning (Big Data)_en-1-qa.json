[
  {
    "question": "Why are Deep Learning training tasks naturally scalable to large quantities of data?",
    "answer": "The widespread use of mini-batching strategies along with first-order iterative solvers makes most common training tasks naturally scalable to large quantities of data."
  },
  {
    "question": "What are common difficulties in scaling traditional machine learning code to very large datasets?",
    "answer": "Many algorithms in traditional machine learning packages, such as scikit-learn, often require the entire training set to be loaded in memory, do not leverage thread or process parallelism, and may use memory-intensive solvers."
  },
  {
    "question": "What is the primary focus of this document?",
    "answer": "This page covers options to scale out traditional machine learning methods to very large datasets."
  },
  {
    "question": "What is scikit-learn?",
    "answer": "Scikit-learn is a Python module for machine learning that is built on top of SciPy and distributed under the 3-Clause BSD license."
  },
  {
    "question": "What are the main limitations of scikit-learn implementations when dealing with massive datasets for models like GLMs and SVMs?",
    "answer": "Many scikit-learn implementations assume the entire training set can be loaded in memory, and some algorithms default to memory-intensive solvers."
  },
  {
    "question": "What is a common cause of Out-Of-Memory (OOM) errors if the training set fits entirely in memory?",
    "answer": "If your training set is small enough that it can be loaded entirely in memory but you experience OOM errors, the culprit is likely a memory-intensive solver."
  },
  {
    "question": "How can OOM errors often be resolved in scikit-learn for methods with memory-intensive solvers?",
    "answer": "Replacing the default solver with a Stochastic Gradient Descent (SGD)-based one is often a straightforward solution to OOM errors."
  },
  {
    "question": "What command can be used to monitor memory usage during Python code execution?",
    "answer": "You can monitor memory usage by running the command `htop` on the terminal while the Python code runs."
  },
  {
    "question": "What is an alternative to Ridge Regression that further reduces memory usage, and what is its limitation?",
    "answer": "Using `SGDRegressor` instead of `Ridge` reduces memory usage even more, but it only works if the output is unidimensional (a scalar)."
  },
  {
    "question": "What is 'out-of-core learning' in scikit-learn?",
    "answer": "Scikit-learn refers to leaving data on disk and loading it in batches during training as *out-of-core learning*."
  },
  {
    "question": "When is 'out-of-core learning' a viable option in scikit-learn?",
    "answer": "Out-of-core learning is a viable option whenever an estimator has the `partial_fit` method available."
  },
  {
    "question": "How does `partial_fit` function in out-of-core learning for `SGDClassifier`?",
    "answer": "Each call to `partial_fit` will run one epoch of stochastic gradient descent over a batch of data."
  },
  {
    "question": "How can large `numpy` arrays be used for out-of-core learning with `SGDClassifier`?",
    "answer": "They can be stored on disk as `.npy` files and memory-mapped, allowing only small batches of rows to be loaded into memory at a time during iteration."
  },
  {
    "question": "How can data from CSV files be processed in batches for machine learning models like LASSO regression?",
    "answer": "Data can be read in batches from a CSV file using the `pandas` package with its `read_csv` function's `chunksize` and `iterator` parameters."
  }
]