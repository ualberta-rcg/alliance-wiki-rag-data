[
  {
    "question": "What are the potential consequences of running GPU-enabled PyTorch code with `torch >= 1.12.0` compared to `torch < 1.12.0` on A100, H100, or newer Nvidia GPUs?",
    "answer": "Users may experience a significant slowdown and/or obtain different results when running the exact same GPU-enabled code with `torch >= 1.12.0` compared to `torch < 1.12.0` on these specific Nvidia GPU architectures."
  },
  {
    "question": "How can users enable or disable TF32 for matrix multiplications in PyTorch versions 1.12.0 and higher?",
    "answer": "Users can set the flag `torch.backends.cuda.matmul.allow_tf32` to `True` or `False` accordingly."
  },
  {
    "question": "How can users enable or disable TF32 for convolutions in PyTorch versions 1.12.0 and higher?",
    "answer": "Users can set the flag `torch.backends.cudnn.allow_tf32` to `True` or `False` accordingly."
  },
  {
    "question": "Where can users find more information about TF32 in PyTorch?",
    "answer": "More information on TF32 can be found in PyTorch's official documentation at https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere."
  },
  {
    "question": "What are the two main types of native CPU parallelism supported by PyTorch?",
    "answer": "PyTorch natively supports two types of CPU parallelism: intra-op parallelism and inter-op parallelism."
  },
  {
    "question": "What is intra-op parallelism in PyTorch and how does it work?",
    "answer": "Intra-op parallelism refers to PyTorch's parallel implementations of common Deep Learning operators like matrix multiplication and convolution, which automatically leverage multi-threading over available CPU cores using OpenMP or low-level libraries such as MKL and OneDNN."
  },
  {
    "question": "What is inter-op parallelism in PyTorch and when is it typically used?",
    "answer": "Inter-op parallelism is PyTorch's ability to execute different parts of your code concurrently. This typically requires explicit program design, such as leveraging `torch.jit` to run asynchronous tasks in a TorchScript program."
  },
  {
    "question": "When is it recommended to use multiple CPUs instead of a GPU for PyTorch models?",
    "answer": "For small scale models, it is strongly recommended to use multiple CPUs instead of a GPU. This is because the speed-up on a GPU might not be significant if the model and dataset are not large enough, leading to inefficient resource usage on HPC clusters."
  },
  {
    "question": "In the `cifar10-cpu.py` example, how is the number of threads for PyTorch operations determined?",
    "answer": "In the `cifar10-cpu.py` example, the number of threads for PyTorch operations is set using `torch.set_num_threads(int(os.environ['SLURM_CPUS_PER_TASK']))`, which configures PyTorch to utilize the number of CPU cores available to the job."
  }
]