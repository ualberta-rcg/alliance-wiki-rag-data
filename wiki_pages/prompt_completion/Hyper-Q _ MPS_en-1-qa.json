[
  {
    "question": "What is Hyper-Q?",
    "answer": "Hyper-Q (or MPS) is a feature of NVIDIA GPUs."
  },
  {
    "question": "What is MPS?",
    "answer": "MPS (or Hyper-Q) is a feature of NVIDIA GPUs."
  },
  {
    "question": "Which NVIDIA GPUs support Hyper-Q or MPS?",
    "answer": "It is available in GPUs with CUDA compute capability 3.5 and higher."
  },
  {
    "question": "According to NVIDIA, what is the design purpose of the MPS runtime architecture?",
    "answer": "The MPS runtime architecture is designed to transparently enable co-operative multi-process CUDA applications, typically MPI jobs, to utilize Hyper-Q capabilities on the latest NVIDIA (Kepler and later) GPUs."
  },
  {
    "question": "What functionality does Hyper-Q provide for CUDA kernels?",
    "answer": "Hyper-Q allows CUDA kernels to be processed concurrently on the same GPU."
  },
  {
    "question": "When can Hyper-Q improve performance?",
    "answer": "This can benefit performance when the GPU compute capacity is underutilized by a single application process."
  },
  {
    "question": "How does MPS affect the total GPU flop rate according to tests?",
    "answer": "In our tests, MPS may increase the total GPU flop rate even when the GPU is being shared by unrelated CPU processes."
  },
  {
    "question": "What type of CUDA applications benefit most from MPS?",
    "answer": "MPS is great for CUDA applications with relatively small problem sizes, which on their own cannot efficiently saturate modern GPUs with thousands of cores."
  },
  {
    "question": "Is MPS enabled by default?",
    "answer": "MPS is not enabled by default."
  },
  {
    "question": "What commands are needed to enable MPS before running a CUDA application?",
    "answer": "You need to execute the following commands: `export CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps`, `export CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log`, and `nvidia-cuda-mps-control -d`."
  },
  {
    "question": "Under what conditions can the MPS feature be utilized?",
    "answer": "You can use the MPS feature if you have more than one CPU thread accessing the GPU."
  },
  {
    "question": "What are some examples of applications or scenarios where MPS can be used?",
    "answer": "This will happen if you run a hybrid MPI/CUDA application, a hybrid OpenMP/CUDA application, or multiple instances of a serial CUDA application (GPU farming)."
  },
  {
    "question": "Where can more details about MPS be found?",
    "answer": "Additional details on MPS can be found in the CUDA Multi Process Service (MPS) - NVIDIA Documentation."
  },
  {
    "question": "When is GPU farming with MPS particularly useful?",
    "answer": "One situation when the MPS feature can be very useful is when you need to run multiple instances of a CUDA application, but the application is too small to saturate a modern GPU."
  },
  {
    "question": "How does MPS facilitate running multiple CUDA application instances on a single GPU?",
    "answer": "MPS allows you to run multiple instances of the application sharing a single GPU, as long as there is enough of GPU memory for all of the instances of the application."
  },
  {
    "question": "What is the benefit of using MPS for GPU farming?",
    "answer": "In many cases this should result in a significantly increased throughput from all of your GPU processes."
  },
  {
    "question": "In a GPU farming job script, what is the purpose of using '&' on the code execution line?",
    "answer": "The `&` on the code execution line sends the code processes to the background."
  },
  {
    "question": "What is the role of the 'wait' command at the end of a GPU farming script?",
    "answer": "The `wait` command at the end of the script ensures that the job runs until all background processes end."
  }
]