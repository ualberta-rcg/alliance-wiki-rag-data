[
  {
    "question": "What is Tensor Parallelism (TP) in PyTorch?",
    "answer": "Tensor Parallelism (TP) is a model sharding approach where the computation of a forward or backward pass through a model layer, along with the layer's weights, is split across multiple devices."
  },
  {
    "question": "How does Tensor Parallelism differ from Fully Sharded Data Parallelism (FSDP)?",
    "answer": "While FSDP shards model weights across devices but may collect them on a single device during certain computation steps, Tensor Parallelism performs computation steps locally on the device where a model shard is placed."
  },
  {
    "question": "What is an advantage of using Tensor Parallelism?",
    "answer": "With Tensor Parallelism, computation steps are done locally on the device where a model shard is placed, avoiding overhead from moving model shards across devices and allowing layers that do not fit entirely in the memory of a single device to be processed."
  },
  {
    "question": "What is Pipeline Parallelism (PP)?",
    "answer": "Pipeline Parallelism (PP) is a model sharding approach where groups of consecutive layers of a model are placed on different devices."
  },
  {
    "question": "How does Pipeline Parallelism mitigate device idleness during sequential computations?",
    "answer": "To mitigate device idleness, Pipeline Parallelism breaks every input batch into 'micro-batches,' which are then fed to the model in sequence, ensuring all devices stay busy as the first micro-batch reaches the last model block."
  },
  {
    "question": "What is a model checkpoint in the context of training?",
    "answer": "A model checkpoint is a snapshot of your model at a given point during the training process (e.g., after a certain number of iterations or epochs) that is saved to disk."
  },
  {
    "question": "Why is it recommended to create model checkpoints during training?",
    "answer": "Creating model checkpoints is a good habit because it allows you to break up long jobs into shorter ones for faster cluster allocation, and it helps avoid losing progress in case of unexpected code errors or node failures."
  },
  {
    "question": "How can you create a model checkpoint using PyTorch Lightning?",
    "answer": "To create a checkpoint with PyTorch Lightning, you can use the `pl.callbacks.ModelCheckpoint` class in the `Trainer()`'s callbacks parameter, for example: `callbacks = [pl.callbacks.ModelCheckpoint(dirpath=\"./ckpt\",every_n_epochs=1)] trainer = pl.Trainer(callbacks=callbacks) trainer.fit(model)`."
  },
  {
    "question": "How do you load a checkpoint and continue training with PyTorch Lightning?",
    "answer": "The same code snippet used for creating a checkpoint with `pl.callbacks.ModelCheckpoint` (`trainer.fit(model)`) will automatically load an existing checkpoint from the specified `dirpath` (e.g., `./ckpt`) and continue training from that point."
  },
  {
    "question": "Where can you find information on creating and loading checkpoints in custom PyTorch training loops?",
    "answer": "For custom training loops, you should refer to the official PyTorch documentation on saving and loading a general checkpoint."
  },
  {
    "question": "When performing distributed training with PyTorch Lightning, what extra code is required for checkpointing?",
    "answer": "No extra code is required for checkpointing with PyTorch Lightning during distributed training, beyond using the standard checkpoint callback."
  },
  {
    "question": "When using DistributedDataParallel or Horovod for distributed training, which process should handle checkpointing?",
    "answer": "Only one process (one of the ranks), typically rank 0, should perform checkpointing during distributed training, as all ranks will have the same state at the end of each iteration."
  },
  {
    "question": "What precautions should be taken when loading a checkpoint during distributed training, especially if multiple processes are involved?",
    "answer": "To avoid errors or wrong results, add a `torch.distributed.barrier()` to ensure the checkpointing process finishes writing to disk before other processes attempt to load it, and pass `map_location` to `torch.load` to ensure tensors are loaded onto the correct GPU for each rank (e.g., `map_location = f\"cuda:{local_rank}\"`)."
  }
]