[
  {
    "question": "What is happening to the Graham system?",
    "answer": "The Graham system will soon be retired and replaced by a new system named Nibi."
  },
  {
    "question": "When did the Graham system become available?",
    "answer": "The Graham system has been in production since June 2017."
  },
  {
    "question": "What is the login node for Graham?",
    "answer": "The login node for Graham is graham.alliancecan.ca."
  },
  {
    "question": "What is the Globus collection name for Graham?",
    "answer": "The Globus collection for Graham is computecanada#graham-globus."
  },
  {
    "question": "How can users transfer data to or from Graham?",
    "answer": "Users can transfer data using robot or login nodes via methods like rsync, scp, or sftp."
  },
  {
    "question": "Where is the Graham cluster located?",
    "answer": "The Graham cluster is located at the University of Waterloo."
  },
  {
    "question": "Who is the Graham cluster named after?",
    "answer": "The Graham cluster is named after Wes Graham, the first director of the Computing Centre at Waterloo."
  },
  {
    "question": "How is the Graham cluster cooled?",
    "answer": "The Graham cluster is entirely liquid cooled, using rear-door heat exchangers."
  },
  {
    "question": "Can Graham's compute nodes access the internet?",
    "answer": "By policy, Graham's compute nodes cannot access the internet."
  },
  {
    "question": "How can a user request an exception for internet access on Graham compute nodes?",
    "answer": "To request an exception, contact technical support with the IP, Port/s, Protocol (TCP or UDP), Contact person, and a Removal Date."
  },
  {
    "question": "Is crontab offered on Graham?",
    "answer": "No, crontab is not offered on Graham."
  },
  {
    "question": "What are the minimum and maximum job durations on Graham?",
    "answer": "Each job on Graham should have a duration of at least one hour (five minutes for test jobs) and no more than 168 hours (seven days)."
  },
  {
    "question": "What is the maximum number of jobs a user can have on Graham?",
    "answer": "A user cannot have more than 1000 jobs, running and queued, at any given moment, with array jobs counted as the number of tasks in the array."
  },
  {
    "question": "What is the total volume of Home space on Graham?",
    "answer": "Graham has 133TB of total Home space volume."
  },
  {
    "question": "How is Home space allocated on Graham?",
    "answer": "Home space has a small, fixed quota per directory and is not allocated via RAS or RAC, with larger requests going to Project space."
  },
  {
    "question": "Does Graham's Home space have backups?",
    "answer": "Yes, Graham's Home space has daily backup."
  },
  {
    "question": "What is the purpose of Scratch space on Graham?",
    "answer": "Scratch space is for active or temporary storage (located at `/scratch`) and is designed as a parallel high-performance filesystem."
  },
  {
    "question": "How is Scratch space managed regarding quotas and allocation?",
    "answer": "Scratch space is not allocated and has a large fixed quota per user; inactive data will be purged."
  },
  {
    "question": "What is the total volume of Project space on Graham?",
    "answer": "Graham has 16PB of total Project space volume."
  },
  {
    "question": "How is Project space allocated on Graham?",
    "answer": "Project space is allocated via RAS or RAC and has a large adjustable quota per project."
  },
  {
    "question": "Is Project space suitable for parallel I/O workloads on Graham?",
    "answer": "No, Project space is not designed for parallel I/O workloads; users should use Scratch space instead for those."
  },
  {
    "question": "What types of InfiniBand interconnects are used on Graham?",
    "answer": "Graham uses Mellanox FDR (56Gb/s) and EDR (100Gb/s) InfiniBand interconnects."
  },
  {
    "question": "Which InfiniBand interconnect type is used for GPU and cloud nodes on Graham?",
    "answer": "Mellanox FDR (56Gb/s) InfiniBand is used for GPU and cloud nodes, while EDR (100Gb/s) is used for other node types."
  },
  {
    "question": "How can users access visualization nodes on Graham?",
    "answer": "Graham has dedicated visualization nodes at gra-vdi.alliancecan.ca that allow only VNC connections."
  },
  {
    "question": "Has Graham's capacity been reduced recently?",
    "answer": "Yes, in early 2025, Graham's capacity was reduced to make space for the installation of the new Nibi cluster."
  },
  {
    "question": "Is Intel Turbo Boost enabled on Graham nodes?",
    "answer": "Yes, Turbo Boost is enabled for all Graham nodes."
  },
  {
    "question": "What is the best practice for using local on-node storage on Graham?",
    "answer": "The best practice is to use the temporary directory generated by Slurm, `$SLURM_TMPDIR`."
  },
  {
    "question": "What happens to the contents of `$SLURM_TMPDIR` after a job completes on Graham?",
    "answer": "The `$SLURM_TMPDIR` directory and its contents will disappear upon job completion."
  },
  {
    "question": "Why is the available memory on Graham nodes less than the 'round number' suggested by hardware configuration?",
    "answer": "Some memory is permanently occupied by the kernel and OS, so the scheduler only allocates jobs based on the specified 'available' memory to prevent swapping/paging."
  },
  {
    "question": "Which generations of Tesla GPUs are available on Graham?",
    "answer": "Graham contains V100 Volta, T4 Turing, and A100 Ampere Tesla GPUs."
  },
  {
    "question": "Which GPU models have been decommissioned from Graham?",
    "answer": "P100 GPUs and Pascal GPU nodes have been decommissioned from Graham."
  },
  {
    "question": "How does the V100 GPU performance compare to the decommissioned P100 on Graham?",
    "answer": "V100 offers about double the performance for standard computation and about 8X performance for deep learning computations utilizing its tensor core units compared to P100."
  },
  {
    "question": "What are the characteristics of the T4 Turing GPUs on Graham?",
    "answer": "T4 Turing GPUs are targeted specifically at deep learning workloads, do not support efficient double-precision computations, but have good single-precision performance, tensor cores, and support for reduced-precision integer calculations."
  },
  {
    "question": "How many Volta GPU nodes are available on Graham, and what is their maximum job duration?",
    "answer": "Graham has a total of 2 Volta nodes available to all users with a maximum job duration of seven days."
  },
  {
    "question": "What CPU-to-GPU ratio should be maintained when submitting jobs to the 28-core Volta nodes on Graham?",
    "answer": "Users should scale the number of CPUs requested, keeping the ratio of CPUs to GPUs at 3.5 or less; for example, 4 GPUs should request at most 14 CPU cores."
  },
  {
    "question": "What CPU-to-GPU ratio should be maintained for the newest 40-core Volta nodes on Graham?",
    "answer": "For the newest 40-core Volta nodes, users can use 5 CPU cores per GPU."
  },
  {
    "question": "How can a user request one of the NVLINK-enabled Volta nodes on Graham?",
    "answer": "To use one of the NVLINK nodes, add the `--constraint=cascade,v100` parameter to the job submission script."
  },
  {
    "question": "How do you request Turing GPU cards for a job on Graham?",
    "answer": "When requesting Turing GPU nodes, you should specify `--gres=gpu:t4:2` for two T4 cards per node."
  },
  {
    "question": "How do you request Ampere GPU cards for a job on Graham?",
    "answer": "When requesting Ampere GPU nodes, you should specify `--gres=gpu:a100:2` or `--gres=gpu:a5000:2` for two Ampere cards per node."
  },
  {
    "question": "When did Graham's cluster capacity reduction begin?",
    "answer": "Starting January 13, 2025, the Graham cluster began operating at approximately 25% capacity."
  },
  {
    "question": "What are the specifications for Graham's nodes equipped with 8 NVIDIA V100 Volta GPUs?",
    "answer": "These 2 nodes have 40 cores, 377G of available memory, 2 x Intel Xeon Gold 6248 Cascade Lake CPUs, and 5.0TB NVMe SSD storage."
  },
  {
    "question": "Describe the Graham node configuration that includes 8 NVIDIA A100 Ampere GPUs.",
    "answer": "There is 1 such node with 128 cores, 2000G of available memory, 2 x AMD EPYC 7742 CPUs, and 3.5TB SATA SSD storage."
  },
  {
    "question": "How many Graham nodes are equipped with 4 NVIDIA T4 Turing GPUs and what are their CPU types?",
    "answer": "There are 6 nodes with 2 x Intel Xeon Silver 4110 Skylake CPUs and 30 nodes with 2 x Intel Xeon Gold 6238 Cascade Lake CPUs that are equipped with 4 NVIDIA T4 Turing GPUs."
  },
  {
    "question": "What are the specifications of Graham's non-GPU compute nodes with Intel CPUs?",
    "answer": "There are 136 such nodes, each with 44 cores, 187G of available memory, 2 x Intel Xeon Gold 6238 Cascade Lake CPUs, and 879GB SATA SSD storage."
  },
  {
    "question": "What is the configuration of Graham's nodes featuring AMD EPYC CPUs and NVMe storage without GPUs?",
    "answer": "There are 6 such nodes, each with 32 cores, 1024G of available memory, 1 x AMD EPYC 7543 CPU, and 8x2TB NVMe storage."
  }
]