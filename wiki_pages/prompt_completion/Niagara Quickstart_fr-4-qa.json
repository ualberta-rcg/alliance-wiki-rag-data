[
  {
    "question": "Can compute nodes on Niagara access the internet?",
    "answer": "No, the compute nodes cannot access the internet."
  },
  {
    "question": "Where should data be downloaded before starting a job on Niagara?",
    "answer": "Before starting, data should be downloaded on a login node."
  },
  {
    "question": "How are resource requests for tasks scheduled on Niagara?",
    "answer": "All resource requests for tasks are scheduled in multiples of nodes."
  },
  {
    "question": "Are the nodes used by a task shared with other users?",
    "answer": "No, the nodes used by your tasks are for your exclusive use, meaning no other user has access to them."
  },
  {
    "question": "Can I monitor my tasks on the allocated nodes using SSH?",
    "answer": "Yes, you can access the tasks with SSH to monitor their progress."
  },
  {
    "question": "Is it necessary to request a specific amount of memory for a job?",
    "answer": "No, it is useless to make a request for a quantity of memory, as your task always obtains Nx202GB of RAM, where N represents the number of nodes."
  },
  {
    "question": "How many cores should a task ideally use per allocated node?",
    "answer": "You should try to use all 40 cores of the nodes allocated to your task, meaning Nx40 cores."
  },
  {
    "question": "What happens if a task doesn't use all allocated cores?",
    "answer": "If a task doesn't use all allocated cores, SciNet will contact you to help optimize your work."
  },
  {
    "question": "What factors influence job limits on Niagara?",
    "answer": "Job limits depend on whether a user is part of a group with a Resources for Research Group allocation and in which 'partition' the job runs."
  },
  {
    "question": "What is a 'partition' in SLURM-speak?",
    "answer": "'Partitions' are SLURM-speak for use cases."
  },
  {
    "question": "How do you specify a partition for a job?",
    "answer": "You specify the partition with the -p parameter to sbatch or salloc."
  },
  {
    "question": "What is the default partition for jobs if none is specified?",
    "answer": "If you do not specify a partition, your job will run in the 'compute' partition, which is the most common case."
  },
  {
    "question": "What are the limits for compute jobs with an allocation in the 'compute' partition?",
    "answer": "For compute jobs with an allocation in the 'compute' partition, you can have 50 running jobs, 1000 submitted jobs, use 1 to 1000 nodes (40-40000 cores), and have a walltime of 15 minutes to 24 hours."
  },
  {
    "question": "What are the limits for compute jobs without an allocation in the 'compute' partition?",
    "answer": "For compute jobs without an allocation in the 'compute' partition, you can have 50 running jobs, 200 submitted jobs, use 1 to 20 nodes (40-800 cores), and have a walltime of 15 minutes to 24 hours."
  },
  {
    "question": "What are the limits for testing or troubleshooting jobs in the 'debug' partition?",
    "answer": "For testing or troubleshooting jobs in the 'debug' partition, you can have 1 running job, 1 submitted job, use 1 to 4 nodes (40-160 cores), and the maximum walltime is 1 hour."
  },
  {
    "question": "What are the limits for archiving or retrieving data jobs in the 'archivelong' partition?",
    "answer": "For archiving or retrieving data jobs in the 'archivelong' partition, you can have 2 per user (max 5 total) running jobs, 10 per user submitted jobs, and a walltime of 15 minutes to 72 hours."
  },
  {
    "question": "What are the limits for inspecting archived data or small archival actions in the 'archiveshort' partition?",
    "answer": "For inspecting archived data or small archival actions in the 'archiveshort' partition, you can have 2 per user running jobs, 10 per user submitted jobs, and a walltime of 15 minutes to 1 hour."
  },
  {
    "question": "What influences the waiting time for a job in the queue?",
    "answer": "The waiting time depends on factors such as the allocation amount, how much allocation was used recently, the number of nodes and the walltime, and how many other jobs are waiting in the queue."
  },
  {
    "question": "Are files accessible from both login and compute nodes on Niagara?",
    "answer": "Yes, your files can be seen on all Niagara login and compute nodes."
  },
  {
    "question": "Which file systems use GPFS on Niagara?",
    "answer": "$HOME, $SCRATCH, and $PROJECT all use the parallel file system called GPFS."
  },
  {
    "question": "What is GPFS primarily designed for?",
    "answer": "GPFS is a high-performance file system which provides rapid reads and writes to large data sets in parallel from many nodes."
  },
  {
    "question": "What kind of file access leads to poor performance on GPFS?",
    "answer": "Accessing data sets which consist of many, small files leads to poor performance on GPFS."
  },
  {
    "question": "What should be avoided when performing file I/O?",
    "answer": "Avoid reading and writing lots of small amounts of data to disk, as many small files waste space and are slower to access, read, and write."
  },
  {
    "question": "What is recommended if you must write many small files?",
    "answer": "If you must write many small files, use ramdisk."
  },
  {
    "question": "What format is recommended for writing data for better performance and space efficiency?",
    "answer": "Write data out in a binary format, as this is faster and takes less space."
  },
  {
    "question": "What is the Burst Buffer used for?",
    "answer": "The Burst Buffer is better for I/O heavy jobs and to speed up checkpoints."
  }
]