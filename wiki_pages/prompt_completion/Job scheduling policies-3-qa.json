[
  {
    "question": "What is the maximum run-time for jobs on the Trillium cluster?",
    "answer": "Trillium accepts jobs of up to 24 hours run-time."
  },
  {
    "question": "What are the maximum job run-time limits for the Fir, Narval, Nibi, and Rorqual clusters?",
    "answer": "The Fir, Narval, Nibi, and Rorqual clusters accept jobs of up to 7 days run-time."
  },
  {
    "question": "Who has the discretion to change the job time limits?",
    "answer": "These limits are subject to change at the discretion of each site's sysadmin team."
  },
  {
    "question": "How are jobs with varying run-times managed on general-purpose clusters?",
    "answer": "Longer jobs are restricted to use only a fraction of the cluster by 'partitions' based on their time limits."
  },
  {
    "question": "What are the different time-limit partitions available for jobs?",
    "answer": "There are partitions for jobs of 3 hours or less, 12 hours or less, 24 hours (1 day) or less, 72 hours (3 days) or less, and 7 days or less."
  },
  {
    "question": "Do shorter jobs have more scheduling opportunities than longer jobs?",
    "answer": "Yes, shorter jobs have more scheduling opportunities because they can run in partitions with longer time-limits."
  },
  {
    "question": "What is the purpose of backfilling in job scheduling?",
    "answer": "The scheduler employs backfilling to improve overall system usage."
  },
  {
    "question": "How does backfill scheduling work?",
    "answer": "Backfill scheduling will start lower priority jobs if doing so does not delay the expected start time of any higher priority jobs."
  },
  {
    "question": "What is important for backfill scheduling to function effectively?",
    "answer": "Reasonably accurate time limits are important for backfill scheduling to work well, as the expected start time of pending jobs depends on the expected completion time of running jobs."
  },
  {
    "question": "Which type of jobs primarily benefits from backfilling?",
    "answer": "Backfilling primarily benefits jobs with short time limits, such as those under 3 hours."
  },
  {
    "question": "What are the general-purpose clusters mentioned in the context of node partitioning?",
    "answer": "The general-purpose clusters mentioned are Fir, Narval, Nibi, and Rorqual."
  },
  {
    "question": "Into what initial categories are nodes partitioned?",
    "answer": "Nodes are initially partitioned into Base nodes (4 or 8 GB of memory per core), Large memory nodes (16 to 96 GB of memory per core), and GPU nodes."
  },
  {
    "question": "How is a job routed to a specific node category?",
    "answer": "Upon submission, a job will be routed to one of the node categories based on the resources it requests."
  },
  {
    "question": "What is the difference between 'by-node' and 'by-core' partitions?",
    "answer": "Within node categories, 'by-node' partitions are for jobs that use complete nodes (all resources), while 'by-core' partitions are for jobs that use only a few cores (or a single core) and are limited to a subset of the category."
  },
  {
    "question": "How does the requested walltime impact a job's access to resources?",
    "answer": "The nodes are partitioned based on the walltime requested by your job, and shorter jobs have access to more resources."
  },
  {
    "question": "Can a job with a 3-hour walltime limit run on a node that allows 12-hour jobs?",
    "answer": "Yes, a job with less than 3 hours of requested walltime can run on any node that allows 12 hours, but there are nodes which accept 3-hour jobs that do not accept 12-hour jobs."
  },
  {
    "question": "What information does the `partition-stats` utility provide?",
    "answer": "The `partition-stats` utility shows how many jobs are waiting to run ('queued'), how many jobs are currently running, how many nodes are currently idle, and how many nodes are assigned to each partition."
  },
  {
    "question": "How can the division of resources by policy be understood from the `partition-stats` output?",
    "answer": "At the bottom of the `partition-stats` table, the 'Total Number of nodes by partition Type' shows the division of resources by policy, independent of the immediate number of jobs."
  },
  {
    "question": "What analogy is used to describe how partitions are structured based on walltime?",
    "answer": "The partitions are described as being like Matryoshka (Russian) dolls, where a shorter time-limit partition contains the nodes for longer time-limit partitions as a subset."
  },
  {
    "question": "What specific information does the `partition-stats` utility *not* provide?",
    "answer": "The `partition-stats` utility does not give information about the number of cores represented by running or waiting jobs, the number of cores free in partly-assigned nodes in by-core partitions, nor about available memory associated with free cores in by-core partitions."
  },
  {
    "question": "Why should users avoid writing scripts that repeatedly call `partition-stats`?",
    "answer": "Running `partition-stats` is somewhat costly to the scheduler, so users should not write scripts which automatically call it repeatedly."
  },
  {
    "question": "Is there a limit on the number of jobs a user can have in the system at one time?",
    "answer": "Yes, there may be a limit on the number of jobs a user can have in the system at any one time."
  },
  {
    "question": "What is the job limit for normal accounts on Graham and B\u00e9luga clusters?",
    "answer": "On Graham and B\u00e9luga, normal accounts can have no more than 1000 jobs in a pending or running state at any time."
  },
  {
    "question": "How are tasks within a job array counted towards the job limit?",
    "answer": "Each task of a job array counts as one job towards the system limit."
  },
  {
    "question": "Which Slurm parameter is used to apply the job submission limit?",
    "answer": "The job limit is applied using Slurm's `MaxSubmit` parameter."
  }
]