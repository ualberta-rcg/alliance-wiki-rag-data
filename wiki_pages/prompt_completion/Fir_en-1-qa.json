[
  {
    "question": "What is the availability date for the Fir cluster?",
    "answer": "The Fir cluster will be available on August 11, 2025."
  },
  {
    "question": "What is the login node address for Fir?",
    "answer": "The login node for Fir is fir.alliancecan.ca."
  },
  {
    "question": "What is the automation node address for the Fir cluster?",
    "answer": "The automation node for Fir is robot.fir.alliancecan.ca."
  },
  {
    "question": "What is the Globus collection name for Fir?",
    "answer": "The Globus collection for Fir is alliancecan#fir-globus."
  },
  {
    "question": "What is the JupyterHub address for Fir?",
    "answer": "The JupyterHub address for Fir is jupyterhub.fir.alliancecan.ca."
  },
  {
    "question": "Where is the Fir computing cluster hosted?",
    "answer": "Fir is hosted at Simon Fraser University (SFU) in Burnaby, British Columbia."
  },
  {
    "question": "What kind of computing cluster is Fir?",
    "answer": "Fir is a versatile, heterogeneous computing cluster designed to support a wide range of scientific computations."
  },
  {
    "question": "Which partners were involved in building the Fir cluster?",
    "answer": "Fir was built in partnership with Lenovo Canada and Data Direct Networks (DDN)."
  },
  {
    "question": "What is the Fir cluster named after?",
    "answer": "Fir is named after the Red Creek Fir\u2014the largest known Douglas fir tree on Earth by volume."
  },
  {
    "question": "What is SFU's commitment regarding high-performance computing with Fir?",
    "answer": "SFU remains committed to environmentally sustainable high-performance computing and is transitioning from traditional air cooling to advanced direct-to-chip liquid cooling with Fir."
  },
  {
    "question": "What are the benefits of the new liquid cooling system in Fir?",
    "answer": "The new liquid cooling system significantly improves energy efficiency and reduces power consumption associated with cooling."
  },
  {
    "question": "How does Fir's InfiniBand network performance compare to the previous-generation Cedar cluster?",
    "answer": "The new high-speed InfiniBand network in Fir delivers more than twice the performance of the previous-generation Cedar cluster."
  },
  {
    "question": "What is Fir's ranking on the June 2025 TOP500 list?",
    "answer": "Fir is ranked #78 on the June 2025 TOP500 list of the world\u2019s most powerful supercomputers."
  },
  {
    "question": "How can a researcher request access to Fir?",
    "answer": "Researchers must request access in CCDB, via Resources--> Access Systems. Then, select Fir from the list on the left and select 'I request access'."
  },
  {
    "question": "How long does it typically take for access to Fir to be enabled?",
    "answer": "It can take up to one hour for access to Fir to be enabled."
  },
  {
    "question": "Do Fir's compute nodes have access to the internet?",
    "answer": "Yes, Fir's compute nodes have full access to the internet."
  },
  {
    "question": "Is the crontab tool supported on Fir?",
    "answer": "No, the crontab tool is not supported on Fir."
  },
  {
    "question": "What is the minimum duration for a regular job on Fir?",
    "answer": "Each job should have a duration of at least one hour."
  },
  {
    "question": "What is the minimum duration for a test job on Fir?",
    "answer": "Test jobs on Fir have a minimum duration of five minutes."
  },
  {
    "question": "What is the maximum job duration allowed on Fir?",
    "answer": "The maximum job duration on Fir is 7 days (168 hours)."
  },
  {
    "question": "Which endpoint should be used for data transfer via Globus?",
    "answer": "For transferring data via Globus, the endpoint specified at the top of the page (alliancecan#fir-globus) should be used."
  },
  {
    "question": "Which node should be used for data transfer tools like rsync, scp, and sftp?",
    "answer": "For tools like rsync, scp, and sftp, please use the login node."
  },
  {
    "question": "What is the total storage capacity of Fir?",
    "answer": "Fir has 51PB of high-performance DDN Lustre storage."
  },
  {
    "question": "How is the 51PB storage divided?",
    "answer": "The 51PB storage consists of 2PB NVME and 49PB SAS."
  },
  {
    "question": "Do all storage mounts share the available storage on Fir?",
    "answer": "Yes, all mounts share the available storage on Fir."
  },
  {
    "question": "What is the access path for the HOME storage area?",
    "answer": "The access path for the HOME storage area is the default `$HOME`."
  },
  {
    "question": "What kind of quota applies to the HOME storage area and can it be increased?",
    "answer": "The HOME storage area has a small per-user quota that cannot be increased. Users should use `/project` for larger storage needs."
  },
  {
    "question": "Is the HOME storage area backed up?",
    "answer": "Yes, the HOME storage area has daily automatic backup."
  },
  {
    "question": "What is the access path for the SCRATCH storage area?",
    "answer": "The access path for the SCRATCH storage area is `$HOME/scratch`."
  },
  {
    "question": "What is the purpose of the SCRATCH storage area?",
    "answer": "The SCRATCH storage area is for temporary files, and old files are purged automatically."
  },
  {
    "question": "Does the SCRATCH storage area have backups?",
    "answer": "No, the SCRATCH storage area has no backup."
  },
  {
    "question": "What is the access path for the PROJECT storage area?",
    "answer": "The access path for the PROJECT storage area is `$HOME/project/${def-project-id}`."
  },
  {
    "question": "What kind of quota applies to the PROJECT storage area and what is its purpose?",
    "answer": "The PROJECT storage area has a large and adjustable per-project quota, and it is intended for group data sharing and large datasets."
  },
  {
    "question": "Is the PROJECT storage area backed up?",
    "answer": "Yes, the PROJECT storage area has daily backup."
  },
  {
    "question": "What type of interconnect does Fir use?",
    "answer": "Fir uses an InfiniBand NDR interconnect."
  },
  {
    "question": "What is the blocking factor for CPU node island size?",
    "answer": "The CPU node island size blocking factor is 27:5 over 216 nodes of 192 cores."
  },
  {
    "question": "What is the blocking factor for GPU nodes?",
    "answer": "The GPU nodes have a 2:1 blocking factor."
  },
  {
    "question": "What are the characteristics of storage access on Fir?",
    "answer": "Storage access on Fir is fully non-blocking."
  },
  {
    "question": "How many CPU nodes feature 2 x AMD EPYC 9655 processors?",
    "answer": "There are 864 CPU nodes featuring 2 x AMD EPYC 9655 (Zen 5) @ 2.7 GHz processors."
  },
  {
    "question": "How much available memory do the 864 CPU nodes typically have?",
    "answer": "The 864 CPU nodes have 750G or 768000M of available memory."
  },
  {
    "question": "What CPU configuration do the 8 CPU nodes with 6000G memory use?",
    "answer": "The 8 CPU nodes with 6000G (6144000M) available memory use 2 x AMD EPYC 9654 (Zen 4) @ 2.4 GHz processors, each with 384MB cache L3."
  },
  {
    "question": "How many GPU nodes are there and what CPU do they use?",
    "answer": "There are 160 GPU nodes, each using 1 x AMD EPYC 9454 (Zen 4) @ 2.75 GHz processor with 256MB cache L3."
  },
  {
    "question": "How many cores and how much memory do the GPU nodes have?",
    "answer": "The GPU nodes have 48 cores and 1125G or 1152000M of available memory."
  },
  {
    "question": "What GPUs are present in the GPU nodes?",
    "answer": "Each GPU node contains 4 x NVidia H100 SXM5 (80 GB memory) GPUs."
  },
  {
    "question": "What is the total number of physical cores in a CPU node with AMD EPYC 9655 processors?",
    "answer": "Each CPU node with 2 x AMD EPYC 9655 (Zen 5) processors has a total of 192 physical cores."
  },
  {
    "question": "What type of architecture are the CPU nodes built on?",
    "answer": "The CPU nodes are built on a chiplet-based NUMA architecture."
  },
  {
    "question": "How many sockets are there per CPU node (AMD EPYC 9655)?",
    "answer": "There are 2 sockets per CPU node (AMD EPYC 9655)."
  },
  {
    "question": "How many cores and CCDs are in each socket of an AMD EPYC 9655 CPU node?",
    "answer": "Each socket in an AMD EPYC 9655 CPU node has 96 cores and 12 CCDs (chiplets)."
  },
  {
    "question": "How many cores and L3 cache does each CCD contain in the AMD EPYC 9655 CPU node?",
    "answer": "Each CCD contains 8 cores and 32 MiB shared L3 cache."
  },
  {
    "question": "What is the total number of NUMA nodes per CPU node (AMD EPYC 9655)?",
    "answer": "There are 24 NUMA nodes per CPU node (12 per socket \u00d7 2)."
  },
  {
    "question": "What is a performance tuning recommendation for CPU nodes to align tasks to CCDs?",
    "answer": "To align tasks to CCDs, use `#SBATCH --cpus-per-task=8`. This keeps threads within a single CCD to avoid inter-chiplet communication latency."
  },
  {
    "question": "What is the recommendation for distributing tasks across NUMA nodes on CPU nodes?",
    "answer": "To fully utilize all CCDs without overloading any single NUMA node, launch 24 tasks per node using `#SBATCH --ntasks-per-node=24`."
  },
  {
    "question": "What CPU does each GPU node contain and how many physical cores does it have?",
    "answer": "Each GPU node contains 1 \u00d7 AMD EPYC 9454 (Zen 4) @ 2.75 GHz processor with 48 physical cores."
  },
  {
    "question": "What NUMA mode do GPU nodes use?",
    "answer": "GPU nodes use the NPS=4 mode (NUMA Per Socket)."
  },
  {
    "question": "How many NUMA nodes are there per socket when NPS=4 is configured on GPU nodes?",
    "answer": "With NPS=4, the socket on GPU nodes is split into 4 NUMA nodes."
  },
  {
    "question": "How many cores and memory channels does each NUMA node have in the GPU nodes with NPS=4?",
    "answer": "Each NUMA node in the GPU nodes has 12 cores (1.5 CCDs per node) and 3 memory channels."
  },
  {
    "question": "How are the GPU accelerators interconnected in the GPU nodes?",
    "answer": "The 4 node accelerators are interconnected by SXM5."
  },
  {
    "question": "What Slurm option is recommended to bind threads to CCDs on GPU nodes?",
    "answer": "To bind threads to CCDs on GPU nodes, use `#SBATCH --cpus-per-task=8`."
  },
  {
    "question": "What Slurm options are recommended to match tasks to NUMA nodes for optimal performance on GPU nodes?",
    "answer": "For optimal performance, launch 4 tasks per node (or a multiple thereof) using `#SBATCH --ntasks-per-node=4` and `#SBATCH --cpus-per-task=12`."
  },
  {
    "question": "How can one request a single full H100-80gb GPU for a job?",
    "answer": "To request one full H100-80gb GPU, use the Slurm option `--gpus=h100:1`."
  },
  {
    "question": "What Slurm options can be used to request multiple H100-80gb GPUs per node?",
    "answer": "To request multiple H100-80gb GPUs per node, use `--gpus-per-node=h100:2`, `--gpus-per-node=h100:3`, or `--gpus-per-node=h100:4`."
  },
  {
    "question": "What Slurm option is used to request multiple full H100 GPUs that can be spread anywhere?",
    "answer": "To request multiple full H100 GPUs spread anywhere, use `--gpus=h100:n`, replacing 'n' with the desired number of GPUs."
  },
  {
    "question": "What technology is configured on approximately half of the GPU nodes?",
    "answer": "Approximately half of the GPU nodes are configured with MIG (Multi-Instance GPU) technology."
  },
  {
    "question": "What are the available GPU instance sizes for MIG technology?",
    "answer": "The available GPU instance sizes are 1g.10gb, 2g.20gb, and 3g.40gb."
  },
  {
    "question": "Describe the 1g.10gb GPU instance.",
    "answer": "The 1g.10gb GPU instance provides 1/8th of the computing power with 10GB of GPU memory."
  },
  {
    "question": "How do you request a 2g.20gb GPU instance using Slurm?",
    "answer": "To request a 2g.20gb GPU instance, use the Slurm option `--gpus=nvidia_h100_80gb_hbm3_2g.20gb:1`."
  }
]