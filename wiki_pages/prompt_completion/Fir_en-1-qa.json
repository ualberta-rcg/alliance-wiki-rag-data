[
  {
    "question": "When will the Fir cluster be available?",
    "answer": "The Fir cluster will be available on August 11, 2025."
  },
  {
    "question": "What is the login node for the Fir cluster?",
    "answer": "The login node for Fir is fir.alliancecan.ca."
  },
  {
    "question": "What is the automation node for Fir?",
    "answer": "The automation node for Fir is robot.fir.alliancecan.ca."
  },
  {
    "question": "What is the Globus collection ID for Fir?",
    "answer": "The Globus collection is alliancecan#fir-globus, accessible via https://globus.alliancecan.ca/file-manager?origin_id=8dec4129-9ab4-451d-a45f-5b4b8471f7a3&two_pane=false."
  },
  {
    "question": "What is the JupyterHub URL for the Fir cluster?",
    "answer": "The JupyterHub URL for Fir is jupyterhub.fir.alliancecan.ca."
  },
  {
    "question": "Who are the partners involved in building the Fir computing cluster?",
    "answer": "Fir is built in partnership with Lenovo Canada and Data Direct Networks (DDN)."
  },
  {
    "question": "Where is the Fir cluster hosted?",
    "answer": "The Fir cluster is hosted at Simon Fraser University (SFU) in Burnaby, British Columbia."
  },
  {
    "question": "What is the origin of the name 'Fir' for the cluster?",
    "answer": "Fir is named after the Red Creek Fir\u2014the largest known Douglas fir tree on Earth by volume."
  },
  {
    "question": "What cooling technology does Fir utilize to improve energy efficiency?",
    "answer": "Fir is transitioning from traditional air cooling to advanced direct-to-chip liquid cooling to significantly improve energy efficiency and reduce power consumption."
  },
  {
    "question": "How does Fir's InfiniBand network performance compare to the Cedar cluster?",
    "answer": "The new high-speed InfiniBand network in Fir delivers more than twice the performance of the previous-generation Cedar cluster."
  },
  {
    "question": "What is Fir's ranking on the June 2025 TOP500 list?",
    "answer": "Fir is ranked #78 on the June 2025 TOP500 list of the world\u2019s most powerful supercomputers."
  },
  {
    "question": "How do researchers request access to the Fir cluster?",
    "answer": "Researchers must request access in CCDB, via Resources--> Access Systems, select Fir from the list, and then choose 'I request access'."
  },
  {
    "question": "How long does it typically take for access to the Fir cluster to be enabled?",
    "answer": "It can take up to one hour for your access to be enabled."
  },
  {
    "question": "Do Fir's compute nodes have internet access?",
    "answer": "Yes, Fir's compute nodes have full access to the internet."
  },
  {
    "question": "Is the crontab tool supported on the Fir cluster?",
    "answer": "No, the crontab tool is not supported on Fir."
  },
  {
    "question": "What is the minimum duration for a standard job on Fir?",
    "answer": "Each job should have a duration of at least one hour, or at least five minutes for test jobs."
  },
  {
    "question": "What is the maximum job duration allowed on Fir?",
    "answer": "The maximum job duration allowed on Fir is 7 days (168 hours)."
  },
  {
    "question": "Which node should be used for data transfer tools like rsync and scp?",
    "answer": "For tools like rsync and scp, users should use the login node."
  },
  {
    "question": "What is the total storage capacity of Fir and its breakdown?",
    "answer": "Fir has 51PB of high-performance DDN Lustre storage, with 2PB NVMe and 49PB SAS."
  },
  {
    "question": "What are the characteristics of the HOME storage area on Fir?",
    "answer": "The HOME storage area uses the default `$HOME` path, has a small per-user quota that cannot be increased, and receives daily automatic backups. Users should use `/project` for larger storage."
  },
  {
    "question": "Describe the SCRATCH storage area on Fir.",
    "answer": "The SCRATCH storage area is accessed via `$HOME/scratch`, has a large per-user quota, receives no backup, is for temporary files, and old files are purged automatically."
  },
  {
    "question": "What are the details of the PROJECT storage area on Fir?",
    "answer": "The PROJECT storage area is accessed via `$HOME/project/${def-project-id}`, has a large and adjustable per-project quota, receives daily backup, and is for group data sharing and large datasets."
  },
  {
    "question": "What type of interconnect does Fir use?",
    "answer": "Fir uses an InfiniBand NDR interconnect."
  },
  {
    "question": "How many CPU nodes with 192 cores and 750G DDR5 memory are in Fir?",
    "answer": "There are 864 CPU nodes in Fir with 192 cores and 750G DDR5 memory."
  },
  {
    "question": "What CPUs are used in the main 864 CPU nodes?",
    "answer": "The main 864 CPU nodes use 2 x AMD EPYC 9655 (Zen 5) @ 2.7 GHz processors, each with 384MB cache L3."
  },
  {
    "question": "How much NVMe storage is available per node for the 864 CPU nodes?",
    "answer": "Each of the 864 CPU nodes has 7.84TB NVMe storage."
  },
  {
    "question": "How many GPU nodes are available in Fir?",
    "answer": "Fir has 160 GPU nodes."
  },
  {
    "question": "What kind of GPUs are installed in the Fir GPU nodes?",
    "answer": "The GPU nodes contain 4 x NVidia H100 SXM5 (80 GB memory) GPUs."
  },
  {
    "question": "What processor is used in the GPU nodes?",
    "answer": "Each GPU node contains 1 \u00d7 AMD EPYC 9454 (Zen 4) @ 2.75 GHz processor with 48 physical cores."
  },
  {
    "question": "What is the total number of physical cores in each CPU node based on the AMD EPYC 9655 processors?",
    "answer": "Each CPU node featuring 2 \u00d7 AMD EPYC 9655 processors has a total of 192 physical cores."
  },
  {
    "question": "How many NUMA nodes are present per CPU node (AMD EPYC 9655 based)?",
    "answer": "There are 24 NUMA nodes per CPU node (12 per socket \u00d7 2)."
  },
  {
    "question": "What Slurm option is recommended to align tasks to CCDs on CPU nodes?",
    "answer": "To align tasks to CCDs (NUMA Domains) on CPU nodes, use `#SBATCH --cpus-per-task=8`."
  },
  {
    "question": "What Slurm options are recommended to fully utilize all CCDs on a CPU node?",
    "answer": "To fully utilize all CCDs on a CPU node, launch 24 tasks per node using `#SBATCH --ntasks-per-node=24` together with `--cpus-per-task=8`."
  },
  {
    "question": "What NPS mode do the GPU nodes use?",
    "answer": "GPU nodes use the NPS=4 mode (NUMA Per Socket), which divides the socket into four NUMA nodes for better memory locality."
  },
  {
    "question": "How many cores are in each NUMA node on a GPU node configured with NPS=4?",
    "answer": "Each NUMA node on a GPU node configured with NPS=4 has 12 cores."
  },
  {
    "question": "What Slurm command is used to bind threads to CCDs on GPU nodes for optimal performance?",
    "answer": "To bind threads to CCDs on GPU nodes, use `#SBATCH --cpus-per-task=8`."
  },
  {
    "question": "What Slurm options are recommended to match tasks to NUMA nodes on GPU nodes?",
    "answer": "To match tasks to NUMA nodes for best performance on GPU nodes, it is recommended to use `#SBATCH --ntasks-per-node=4` and `#SBATCH --cpus-per-task=12`."
  },
  {
    "question": "What is the Slurm option to request a single full H100-80gb GPU?",
    "answer": "To request one full H100-80gb GPU, use `--gpus=h100:1`."
  },
  {
    "question": "How can a user request multiple H100-80gb GPUs per node, for example, 3?",
    "answer": "To request 3 H100-80gb GPUs per node, use `--gpus-per-node=h100:3`."
  },
  {
    "question": "How can a user request a specific number 'n' of full H100 GPUs spread across the cluster?",
    "answer": "To request 'n' number of full H100 GPUs spread anywhere, use `--gpus=h100:n` (replace n with the number of GPUs you want)."
  },
  {
    "question": "Are all GPU nodes in Fir configured with MIG technology?",
    "answer": "Approximately half of the GPU nodes are configured with MIG technology."
  },
  {
    "question": "What are the available GPU instance sizes for MIG-enabled nodes?",
    "answer": "The available GPU instance sizes are 1g.10gb (1/8th of computing power with 10GB GPU memory), 2g.20gb (2/8th of computing power with 20GB GPU memory), and 3g.40gb (3/8th of computing power with 40GB GPU memory)."
  },
  {
    "question": "What is the Slurm option to request a 1g.10gb GPU instance?",
    "answer": "To request a 1g.10gb GPU instance for a compute job, use `--gpus=nvidia_h100_80gb_hbm3_1g.10gb:1`."
  },
  {
    "question": "What is the CPU configuration for the 8 nodes with 6000G DDR5 memory?",
    "answer": "The 8 nodes with 6000G DDR5 memory use 2 x AMD EPYC 9654 (Zen 4) @ 2.4 GHz processors, each with 384MB cache L3."
  },
  {
    "question": "What is the total L3 cache available per CPU node (AMD EPYC 9655 based)?",
    "answer": "A CPU node based on AMD EPYC 9655 processors has a total of 768 MiB L3 cache."
  },
  {
    "question": "What cache is shared by cores within a CCD on the AMD EPYC 9655 CPU nodes?",
    "answer": "Each CCD (chiplet) on the AMD EPYC 9655 CPU nodes has 8 cores and a 32 MiB shared L3 cache."
  }
]