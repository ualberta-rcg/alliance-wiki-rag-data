[
  {
    "question": "What are the learning objectives of this document regarding GPU parallelism?",
    "answer": "The learning objectives are to understand various levels of parallelism on a GPU, interpret compiler messages about parallelization, learn how to get optimization advice from the visual profiler, and understand how to specify parallelization parameters to the compiler."
  },
  {
    "question": "What performance gain was observed in previous steps compared to CPU performance?",
    "answer": "A performance gain of about 3 was obtained compared to the CPU performance in previous steps."
  },
  {
    "question": "Why is it important for a programmer to understand compiler-generated parallelization?",
    "answer": "It is important for a programmer to understand compiler-generated parallelization to potentially give the compiler tips for further optimization and improve performance."
  },
  {
    "question": "Which compiler command options were used to get detailed feedback on parallelization?",
    "answer": "The compiler command `pgc++ -fast -ta=tesla,lineinfo -Minfo=all,intensity,ccff -c -o main.o main.cpp` was used to get detailed feedback."
  },
  {
    "question": "How did the compiler parallelize the loops in the `initialize_vector`, `dot`, and `waxpby` functions based on the provided feedback?",
    "answer": "The compiler parallelized each loop using `#pragma acc loop gang, vector(128)` in the `initialize_vector`, `dot`, and `waxpby` functions."
  },
  {
    "question": "What specific optimization was generated by the compiler for the `dot` function?",
    "answer": "A sum reduction was generated for the `sum` variable in the `dot` function."
  },
  {
    "question": "What does `vector(128)` signify in the compiler's output regarding data processing?",
    "answer": "`vector(128)` signifies that the compiler generated instructions for chunks of data of length 128."
  },
  {
    "question": "Why was a `vector` length of 128 considered inefficient in the example where matrix rows had 27 elements?",
    "answer": "It was considered inefficient because the compiler generated instructions that wasted computation on 101 elements (128 - 27) for each row, as the rows only contained 27 elements."
  },
  {
    "question": "What are the three levels of parallelism that can be used in OpenACC?",
    "answer": "The three levels of parallelism that can be used in OpenACC are `vector`, `worker`, and `gang`."
  },
  {
    "question": "Describe the function of `vector` threads in OpenACC.",
    "answer": "`vector` threads perform a single operation on multiple data (SIMD) in a single step, and if there are fewer data than the vector's length, the operation is still performed on null values and the result is discarded."
  },
  {
    "question": "What is the relationship between an OpenACC `worker` and a `vector`?",
    "answer": "An OpenACC `worker` computes one `vector`."
  },
  {
    "question": "How are `gang`s structured in OpenACC and how do `worker`s within a `gang` interact?",
    "answer": "A `gang` comprises one or multiple `worker`s. All `worker`s within a `gang` can share resources, such as cache memory or processor, while multiple `gang`s run completely independently."
  },
  {
    "question": "Is there a direct mapping between OpenACC parallelism levels and CUDA's threads, blocks, and warps?",
    "answer": "No, there is no direct mapping because OpenACC is meant as a language for generic accelerators, and the mapping is left to the compiler."
  },
  {
    "question": "According to OpenACC standard 2.0, what is the required order for `gang` and `vector` parallelism levels?",
    "answer": "Version 2.0 of the standard states that `gang` must be the outermost level of parallelism, while `vector` must be the innermost level."
  },
  {
    "question": "What are the typical correspondences between OpenACC `vector`, `worker`, `gang` and CUDA's threads, warps, and thread blocks?",
    "answer": "Typically, OpenACC `vector` corresponds to CUDA threads, OpenACC `worker` corresponds to CUDA warps, and OpenACC `gang` corresponds to CUDA thread blocks."
  },
  {
    "question": "Which clauses are used with the OpenACC `loop` directive to control parallelism?",
    "answer": "The `loop` directive can be used with `gang`, `worker`, `vector` for parallelization, or `seq` to run the loop sequentially."
  },
  {
    "question": "What is the correct order for applying multiple OpenACC parallelism clauses to a single loop?",
    "answer": "Multiple clauses must be specified in a top-down order, with `gang` first and `vector` last."
  },
  {
    "question": "How can OpenACC parallelism clauses be applied to specific accelerator types?",
    "answer": "The `device_type` clause can be used to specify which type of accelerator a subsequent clause applies to, e.g., `device_type(nvidia) vector`."
  },
  {
    "question": "How can the size of `vector`, `worker`, and `gang` parallelism levels be explicitly set?",
    "answer": "Each of the `vector`, `worker`, and `gang` clauses can take a parameter to explicitly state their size, such as `worker(32) vector(32)`."
  },
  {
    "question": "What are the size limitations for `vector` length on NVidia GPUs?",
    "answer": "On NVidia GPUs, `vector` length must be a multiple of 32, up to a maximum of 1024."
  },
  {
    "question": "How is the `gang` size determined on NVidia GPUs, and what is its maximum limit?",
    "answer": "The `gang` size on NVidia GPUs is given by the number of `worker`s times the size of a `vector`, and this number cannot be larger than 1024."
  },
  {
    "question": "How was the `vector` length reduced to 32 using the `kernels` directive in the example code?",
    "answer": "The `vector` length was reduced by adding `#pragma acc loop device_type(nvidia) vector(32)` to the inner loop within the `kernels` directive."
  },
  {
    "question": "How is `vector` length specified for the top loop when using the `parallel loop` directive?",
    "answer": "When using the `parallel loop` directive, the `vector` length is specified at the level of the top loop with the `vector_length` clause, such as `vector_length(32)`."
  },
  {
    "question": "What was the observed impact on run time on a K20 GPU after manually reducing the `vector` length from 128 to 32?",
    "answer": "The run time on a K20 GPU increased from about 10 seconds to about 15 seconds, suggesting the compiler had a clever optimization in place."
  },
  {
    "question": "What is the first step in performing guided analysis with the NVidia Visual Profiler?",
    "answer": "The first step is to go to the \"Analysis\" tab and click on \"Examine GPU Usage\" in the NVidia Visual Profiler."
  },
  {
    "question": "What type of bottleneck was identified after performing kernel analysis in the example using the NVidia Visual Profiler?",
    "answer": "The performance was identified as being limited by memory latency after performing kernel analysis."
  },
  {
    "question": "What information did the NVidia Visual Profiler provide about the GPU's active threads and occupancy?",
    "answer": "The profiler showed that the GPU was running 512 active threads while it could run 2048, resulting in only 25% occupancy."
  },
  {
    "question": "Based on the \"Warps\" table in the profiler, what was the suggested way to achieve bigger gangs and better GPU utilization?",
    "answer": "The suggestion was to add more workers while keeping the vector size at 32, as the GPU was running too few vector threads and warps per block/gang."
  },
  {
    "question": "How was the number of `worker`s increased to 32 per `gang` using the `kernels` directive?",
    "answer": "The number of workers was increased by adding `#pragma acc loop device_type(nvidia) gang worker(32)` to the outer loop within the `kernels` directive."
  },
  {
    "question": "What was the performance improvement observed after adding more `worker`s to the code?",
    "answer": "Adding more workers resulted in a performance gain of almost a factor of two, reducing the run time on a K20 from roughly 10 seconds to about 6 seconds."
  },
  {
    "question": "What is the function of the OpenACC `collapse(N)` clause?",
    "answer": "The `collapse(N)` clause causes the next N loops to be collapsed into a single, flattened loop, which is useful for many nested loops or very short loops."
  },
  {
    "question": "When is the OpenACC `tile(N,[M,...])` clause beneficial for optimization?",
    "answer": "The `tile(N,[M,...])` clause is beneficial if an algorithm has high locality, as it breaks loops into tiles, allowing the device to use data from nearby tiles."
  },
  {
    "question": "What is the challenge presented in the bonus exercise?",
    "answer": "The challenge is to port a code that solves the Laplace Equation using the Jacobi method to a GPU using OpenACC and determine the performance gain."
  }
]