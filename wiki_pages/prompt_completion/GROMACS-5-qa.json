[
  {
    "question": "What is the purpose of the provided job scripts for GROMACS?",
    "answer": "The provided job scripts are examples for running GROMACS whole node simulations on clusters like Narval, Rorqual, Fir, Nibi, and Trillium."
  },
  {
    "question": "Which version of GROMACS is configured in these whole node job scripts?",
    "answer": "GROMACS version 2024.4 is configured."
  },
  {
    "question": "What modules must be loaded to run GROMACS 2024.4 in the StdEnv/2023 environment as shown in these scripts?",
    "answer": "The modules `StdEnv/2023`, `gcc/12.3`, `openmpi/4.1.5`, and `gromacs/2024.4` must be loaded."
  },
  {
    "question": "How is the OMP_NUM_THREADS environment variable set in these job scripts?",
    "answer": "The OMP_NUM_THREADS environment variable is set using `export OMP_NUM_THREADS=\"${SLURM_CPUS_PER_TASK:-1}\"`."
  },
  {
    "question": "How many nodes are requested in the Narval GROMACS whole node job script?",
    "answer": "1 node is requested in the Narval GROMACS whole node job script."
  },
  {
    "question": "How many MPI tasks per node are requested for a GROMACS whole node job on Narval?",
    "answer": "32 MPI tasks per node are requested for a GROMACS whole node job on Narval."
  },
  {
    "question": "How many OpenMP threads per MPI task are specified for a GROMACS whole node job on Narval?",
    "answer": "2 OpenMP threads per MPI task are specified for a GROMACS whole node job on Narval."
  },
  {
    "question": "What is the total number of CPUs used per node in the Narval GROMACS whole node job script?",
    "answer": "A total of 64 CPUs per node (32 MPI tasks * 2 OpenMP threads) are utilized in the Narval GROMACS whole node job script."
  },
  {
    "question": "What is the memory limit per CPU set for a GROMACS whole node job on Narval?",
    "answer": "The memory limit per CPU is 2000 MB for a GROMACS whole node job on Narval."
  },
  {
    "question": "What is the time limit specified for a GROMACS whole node job on Narval?",
    "answer": "The time limit specified for a GROMACS whole node job on Narval is 0 days, 1 hour, and 0 minutes (0-01:00)."
  },
  {
    "question": "What is the `srun` command used to execute GROMACS on Narval for a whole node simulation?",
    "answer": "The `srun` command used is `srun --cpus-per-task=$OMP_NUM_THREADS gmx_mpi mdrun -deffnm md`."
  },
  {
    "question": "How many MPI tasks per node are requested for a GROMACS whole node job on Rorqual, Fir, Nibi, or Trillium?",
    "answer": "96 MPI tasks per node are requested for a GROMACS whole node job on Rorqual, Fir, Nibi, or Trillium."
  },
  {
    "question": "How many OpenMP threads per MPI task are used for GROMACS whole node simulations on Rorqual, Fir, Nibi, or Trillium?",
    "answer": "2 OpenMP threads per MPI task are used for GROMACS whole node simulations on Rorqual, Fir, Nibi, or Trillium."
  },
  {
    "question": "What is the total number of CPUs per node utilized in the Rorqual, Fir, Nibi, or Trillium GROMACS whole node job scripts?",
    "answer": "A total of 192 CPUs per node (96 MPI tasks * 2 OpenMP threads) are utilized in the Rorqual, Fir, Nibi, or Trillium GROMACS whole node job scripts."
  },
  {
    "question": "What is the memory limit per CPU set for a GROMACS whole node job on Rorqual, Fir, Nibi, or Trillium?",
    "answer": "The memory limit per CPU is 2000 MB for a GROMACS whole node job on Rorqual, Fir, Nibi, or Trillium."
  },
  {
    "question": "What is the time limit specified for a GROMACS whole node job on Rorqual, Fir, Nibi, or Trillium?",
    "answer": "The time limit specified for a GROMACS whole node job on Rorqual, Fir, Nibi, or Trillium is 0 days, 1 hour, and 0 minutes (0-01:00)."
  },
  {
    "question": "What is the `srun` command used to execute GROMACS on Rorqual, Fir, Nibi, or Trillium for a whole node simulation?",
    "answer": "The `srun` command used is `srun --cpus-per-task=$OMP_NUM_THREADS gmx_mpi mdrun -deffnm md`."
  }
]