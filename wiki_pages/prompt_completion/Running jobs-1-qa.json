[
  {
    "question": "Who is this page intended for?",
    "answer": "This page is intended for users already familiar with job scheduling and job scripts, who need guidance on submitting jobs to clusters."
  },
  {
    "question": "What should users new to large shared computer clusters read first?",
    "answer": "Users new to large shared computer clusters should first read 'What is a scheduler?'."
  },
  {
    "question": "How must all jobs be submitted?",
    "answer": "All jobs must be submitted via the scheduler."
  },
  {
    "question": "What are the exceptions for running tasks on a login node?",
    "answer": "Exceptions are made for compilation and other tasks not expected to consume more than about 10 CPU-minutes and about 4 gigabytes of RAM. Such tasks may be run on a login node."
  },
  {
    "question": "Is it ever permissible to run processes on compute nodes without using the scheduler?",
    "answer": "No, in no case should you run processes on compute nodes except via the scheduler."
  },
  {
    "question": "What is the job scheduler used on these clusters?",
    "answer": "The job scheduler on our clusters is the Slurm Workload Manager."
  },
  {
    "question": "Who maintains the comprehensive documentation for Slurm?",
    "answer": "Comprehensive documentation for Slurm is maintained by SchedMD."
  },
  {
    "question": "What resource is useful for users transitioning to Slurm from other schedulers like PBS/Torque, SGE, LSF, or LoadLeveler?",
    "answer": "Users coming to Slurm from PBS/Torque, SGE, LSF, or LoadLeveler might find a table of corresponding commands useful."
  },
  {
    "question": "What command is used to submit a job in Slurm?",
    "answer": "The command to submit a job is `sbatch`."
  },
  {
    "question": "How do you submit a job script named `simple_job.sh`?",
    "answer": "You can submit a job script named `simple_job.sh` by running `$ sbatch simple_job.sh`."
  },
  {
    "question": "What does a minimal Slurm job script look like?",
    "answer": "A minimal Slurm job script includes a shebang (`#!/bin/bash`), SBATCH directives (e.g., `--time`, `--account`), and executable commands like `echo 'Hello, world!'`."
  },
  {
    "question": "What resources does a minimal Slurm job script reserve by default on general-purpose (GP) clusters?",
    "answer": "On general-purpose (GP) clusters, a minimal job reserves 1 core and 256MB of memory for 15 minutes."
  },
  {
    "question": "What resources does a minimal Slurm job script reserve on Niagara?",
    "answer": "On Niagara, a minimal job reserves the whole node with all its memory."
  },
  {
    "question": "How are directives (options) prefixed in a Slurm job script?",
    "answer": "Directives (or options) in the job script are prefixed with `#SBATCH`."
  },
  {
    "question": "Where must SBATCH directives be placed within a job script?",
    "answer": "SBATCH directives must precede all executable commands in a job script."
  },
  {
    "question": "Where can users find descriptions of all available sbatch directives?",
    "answer": "All available directives are described on the sbatch online manual page."
  },
  {
    "question": "What are the minimum required directives for a job according to policies?",
    "answer": "Our policies require that you supply at least a time limit (`--time`) for each job, and you may also need to supply an account name (`--account`)."
  },
  {
    "question": "How can directives be specified as command-line arguments to `sbatch`?",
    "answer": "Directives can be specified as command-line arguments to `sbatch`, for example, `$ sbatch --time=00:30:00 simple_job.sh`."
  },
  {
    "question": "What are the acceptable time formats for the `--time` directive?",
    "answer": "Acceptable time formats include 'minutes', 'minutes:seconds', 'hours:minutes:seconds', 'days-hours', 'days-hours:minutes', and 'days-hours:minutes:seconds'."
  },
  {
    "question": "How does the time limit affect how quickly a job starts?",
    "answer": "The time limit strongly affects how quickly the job is started, as longer jobs are eligible to run on fewer nodes."
  },
  {
    "question": "What potential issue can arise from submitting multiple Slurm jobs rapidly in a short time?",
    "answer": "Submitting thousands of jobs at a time can cause Slurm to become unresponsive to other users."
  },
  {
    "question": "What are recommended ways to submit many Slurm jobs efficiently and safely?",
    "answer": "Consider using an array job instead, or use `sleep` to space out calls to `sbatch` by one second or more."
  },
  {
    "question": "What directives are used to request memory in Slurm?",
    "answer": "Memory may be requested with `--mem-per-cpu` (memory per core) or `--mem` (memory per node)."
  },
  {
    "question": "What is the default memory allocation per core on general-purpose (GP) clusters?",
    "answer": "On general-purpose (GP) clusters, a default memory amount of 256 MB per core will be allocated unless another request is made."
  },
  {
    "question": "Is a memory specification required for jobs submitted to Niagara?",
    "answer": "No, a memory specification is not required on Niagara, as only whole nodes are allocated along with all available memory."
  },
  {
    "question": "Why is some memory on a node not available to user jobs?",
    "answer": "Some memory on a node is not available to the job because it is reserved for the operating system and other system tasks."
  },
  {
    "question": "What happens if a job requests more memory than a node type provides?",
    "answer": "If a job requests more memory than a node type provides, it will be constrained to run on higher-memory nodes, which may be fewer in number."
  },
  {
    "question": "How does Slurm interpret memory unit prefixes like K, M, and G?",
    "answer": "Slurm interprets K, M, G, etc., as binary prefixes (e.g., `--mem=125G` is equivalent to `--mem=128000M`)."
  },
  {
    "question": "Where can users find the maximum memory they can request on each node type for general-purpose clusters?",
    "answer": "The maximum memory that can be requested on each node is listed in the 'Available memory' column of the 'Node characteristics' table for each GP cluster (e.g., B\u00e9luga, Cedar, Graham, Narval)."
  }
]