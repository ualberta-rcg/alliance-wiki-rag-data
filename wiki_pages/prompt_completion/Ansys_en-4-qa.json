[
  {
    "question": "What is the Fluent command for a single node job using Intel MPI with shared memory?",
    "answer": "For a single node job using Intel MPI with shared memory, the Fluent command is `fluent -g $MYVERSION -t $NCORES -affinity=0 -mpi=intel -pshmem -i $MYJOURNALFILE`."
  },
  {
    "question": "How does the Fluent command differ for multinode jobs on the Nibi cluster when using Intel MPI?",
    "answer": "For multinode jobs on the Nibi cluster using Intel MPI, the Fluent command is `fluent -g $MYVERSION -t $NCORES -affinity=0 -mpi=intel -peth -cnf=/tmp/machinefile-$SLURM_JOB_ID -i $MYJOURNALFILE`, utilizing `-peth` for Ethernet communication."
  },
  {
    "question": "What MPI communication method is used for multinode Fluent jobs on clusters other than Nibi when using Intel MPI?",
    "answer": "For multinode Fluent jobs on clusters other than Nibi using Intel MPI, InfiniBand (specified by `-pib`) is used for communication."
  },
  {
    "question": "What is the recommended Ansys Fluent module version to load in the provided Slurm scripts?",
    "answer": "The scripts recommend loading `ansys/2023R2` or newer versions of the Ansys Fluent module."
  },
  {
    "question": "What is the purpose of the `MYVERSION` variable in the Fluent Slurm scripts?",
    "answer": "The `MYVERSION` variable specifies the Fluent solver version, which can be `2d`, `2ddp`, `3d`, or `3ddp`."
  },
  {
    "question": "How is the `NCORES` variable calculated in a 'Multinode (by core)' Fluent Slurm script?",
    "answer": "In a 'Multinode (by core)' Fluent Slurm script, `NCORES` is calculated as `SLURM_NTASKS * SLURM_CPUS_PER_TASK`."
  },
  {
    "question": "Which Intel MPI modules should be loaded on Narval if `EBVERSIONGENTOO` is 2020?",
    "answer": "If `EBVERSIONGENTOO` is 2020 on Narval, you should load `intel/2021 intelmpi`."
  },
  {
    "question": "Which Intel MPI modules should be loaded on Narval if `EBVERSIONGENTOO` is 2023?",
    "answer": "If `EBVERSIONGENTOO` is 2023 on Narval, you should load `intel/2023 intelmpi`."
  },
  {
    "question": "What environment modules are exported for Open MPI in the 'Multinode (by node, narval)' script?",
    "answer": "The environment variables `OPENMPI_ROOT=$EBROOTOPENMPI` and `OMPI_MCA_hwloc_base_binding_policy=core` are exported for Open MPI in the 'Multinode (by node, narval)' script."
  },
  {
    "question": "What is the maximum number of cores per node specified for Narval in the 'Multinode (by node, narval)' Slurm script?",
    "answer": "The 'Multinode (by node, narval)' Slurm script specifies a maximum of 64 cores per node for Narval (`#SBATCH --ntasks-per-node=64`)."
  },
  {
    "question": "What is the common filename used for the journal file in the example Fluent Slurm scripts?",
    "answer": "The common filename for the journal file specified in the example Fluent Slurm scripts is `sample.jou`."
  },
  {
    "question": "What is the filename for the 'Multinode (by core)' Fluent Slurm script using Intel MPI?",
    "answer": "The filename for the 'Multinode (by core)' Fluent Slurm script using Intel MPI is `script-flu-bycore-intel.sh`."
  },
  {
    "question": "What is the filename for the 'Multinode (by node, narval)' Fluent Slurm script using Open MPI?",
    "answer": "The filename for the 'Multinode (by node, narval)' Fluent Slurm script using Open MPI is `script-flu-bynode-openmpi.sh`."
  }
]