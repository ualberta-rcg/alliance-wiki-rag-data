[
  {
    "question": "How is the PyTorch Lightning `Trainer` class initialized to use GPUs and the DDP strategy?",
    "answer": "The `Trainer` is initialized with `pl.Trainer(accelerator=\"gpu\", devices=2, num_nodes=1, strategy='ddp', max_epochs = args.max_epochs, enable_progress_bar=False)`."
  },
  {
    "question": "How can the PyTorch Lightning `Trainer` initialization be made more flexible for a generic number of GPUs and nodes?",
    "answer": "To make it more generic, `torch.cuda.device_count()` can be used to set the number of GPUs, and `int(os.environ.get(\"SLURM_JOB_NUM_NODES\"))` can be used to set the number of nodes."
  },
  {
    "question": "What is the purpose of setting `enable_progress_bar=False` in the PyTorch Lightning `Trainer`?",
    "answer": "It is set to avoid writing a progress bar to the logs, which can cause issues due to updating logs too frequently."
  },
  {
    "question": "When is it not recommended to use a GPU for model training?",
    "answer": "It is not advisable to use a GPU when a model is relatively small, does not utilize a large portion of GPU memory, and cannot effectively use its compute capacity."
  },
  {
    "question": "What alternative is suggested for training small models that do not efficiently utilize a GPU?",
    "answer": "For such models, it is strongly recommended to use one or more CPUs instead of a GPU."
  },
  {
    "question": "Under what conditions does Data Parallelism on a single GPU become a viable option for small models?",
    "answer": "It becomes a viable option when you have a small model but a very large dataset, and you wish to perform training with a small batch size while still increasing resource usage and achieving a speed-up."
  },
  {
    "question": "What technologies are used to efficiently place multiple model replicas on a single GPU for Data Parallelism?",
    "answer": "Nvidia's Multi-Process Service (MPS) along with MPI are used to efficiently place multiple model replicas on one GPU."
  },
  {
    "question": "How do you activate Nvidia Multi-Process Service (MPS) in a bash script?",
    "answer": "Nvidia MPS is activated by exporting `CUDA_MPS_PIPE_DIRECTORY` and `CUDA_MPS_LOG_DIRECTORY` environment variables, and then running `nvidia-cuda-mps-control -d`."
  },
  {
    "question": "Which SLURM parameter in the `pytorch-gpu-mps.sh` script specifies the number of model replicas to be placed on a single GPU?",
    "answer": "The `--tasks-per-node` parameter determines the number of model replicas placed on the GPU."
  },
  {
    "question": "Describe the architecture of the `Net` class defined in the PyTorch Lightning example for CIFAR10 classification.",
    "answer": "The `Net` class defines a convolutional neural network with two `Conv2d` layers (3 to 6 channels, 6 to 16 channels, both 5x5 kernel), each followed by a `MaxPool2d` layer. It then flattens the output and passes it through three fully connected layers: `nn.Linear(16 * 5 * 5, 120)`, `nn.Linear(120, 84)`, and `nn.Linear(84, 10)` for the final classification."
  }
]