[
  {
    "question": "What are the different types of nodes available?",
    "answer": "The available node types are 'base' (4 to 8GB of memory per core), 'large' (16 to 96GB of memory per core), and 'GPU base'."
  },
  {
    "question": "How are submitted tasks assigned to node categories?",
    "answer": "Submitted tasks are directed to the appropriate node category based on their required resources."
  },
  {
    "question": "What is the difference between 'by-node' and 'by-core' partitions?",
    "answer": "'By-node' partitions are reserved for tasks that use entire nodes, utilizing all available resources. 'By-core' partitions are for tasks that use only a few or even a single core on a node, allocating only a subset of nodes."
  },
  {
    "question": "How does execution time influence task scheduling?",
    "answer": "Execution time plays a role; shorter tasks generally have access to more resources. For example, a task requiring less than 3 hours can run on any node allowing 12-hour tasks, but some nodes that accept 3-hour tasks will not accept 12-hour tasks."
  },
  {
    "question": "What kind of information does the `partition-stats` utility display?",
    "answer": "The `partition-stats` utility shows the number of queued jobs, running jobs, idle nodes, and total assigned nodes within each partition."
  },
  {
    "question": "How are partitions organized in terms of execution time limits?",
    "answer": "Partitions are organized like Russian dolls, where a partition for a shorter duration (e.g., 3 hours) contains a subset of nodes from partitions allowing longer durations (e.g., 12 hours), and so on."
  },
  {
    "question": "What details are not provided by the `partition-stats` utility?",
    "answer": "The `partition-stats` utility does not provide information on the number of cores used by current or pending tasks, the number of free cores in partially assigned by-core partitions, or the memory available with those free cores."
  },
  {
    "question": "Is it advisable to frequently call `partition-stats` in automated scripts?",
    "answer": "No, it is not advisable, as executing `partition-stats` demands significant resources from the scheduler. Users should avoid repetitive automatic calls in their scripts."
  },
  {
    "question": "What is the maximum number of concurrent tasks a normal account can have on Graham and B\u00e9luga?",
    "answer": "A normal account on Graham and B\u00e9luga cannot have more than 1000 tasks in execution or pending simultaneously."
  },
  {
    "question": "How are tasks counted towards the limit in a job array on Graham and B\u00e9luga?",
    "answer": "In a job array, each individual task counts as one task towards the overall limit set by the Slurm `MaxSubmit` parameter."
  },
  {
    "question": "Based on the `partition-stats` example, how many tasks requesting whole nodes of 'base' type with a 3-hour walltime are waiting?",
    "answer": "According to the example, 12 tasks are waiting that requested whole nodes of the 'base' type (less than 8GB/core) with a maximum execution time of 3 hours."
  },
  {
    "question": "Based on the `partition-stats` example, how many tasks requesting individual cores of 'base' type with a 3-hour walltime are waiting?",
    "answer": "The example shows 170 tasks waiting that requested individual cores (less than whole nodes) of the 'base' type (less than 8GB/core) with a maximum execution time of 3 hours."
  },
  {
    "question": "In the `partition-stats` example, how many 'Regular' type nodes are available for whole-node tasks with a walltime of less than 3 hours?",
    "answer": "The example indicates there are 871 'Regular' type nodes available for whole-node tasks with a walltime of less than 3 hours."
  },
  {
    "question": "How many of the 'Regular' 3-hour whole-node tasks can also accept 'by-core' tasks of less than 3 hours?",
    "answer": "Out of the 871 'Regular' nodes that accept 3-hour whole-node tasks, 431 can also accept 'by-core' tasks of less than 3 hours."
  }
]