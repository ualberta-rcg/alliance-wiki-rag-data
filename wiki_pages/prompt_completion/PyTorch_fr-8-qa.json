[
  {
    "question": "What is the main benefit of using Data Parallelism on a single GPU for small models?",
    "answer": "Using Data Parallelism on a single GPU for small models allows fitting several replicas of the model on one GPU, which increases resource usage and provides a speed-up, especially for models with large datasets and small batch sizes."
  },
  {
    "question": "Which Nvidia technology is used to place multiple model replicas efficiently on one GPU?",
    "answer": "Nvidia's Multi-Process Service (MPS) is used to efficiently place multiple model replicas on one GPU."
  },
  {
    "question": "What is the recommended backend for `dist.init_process_group` when performing single GPU Data Parallelism with PyTorch?",
    "answer": "For single GPU Data Parallelism, the `backend` for `dist.init_process_group` should be 'mpi' or 'gloo', as NCCL does not work due to a hard-coded multi-GPU topology check."
  },
  {
    "question": "How do you activate Nvidia MPS in a bash script according to the provided example?",
    "answer": "Activate Nvidia MPS by exporting `CUDA_MPS_PIPE_DIRECTORY` and `CUDA_MPS_LOG_DIRECTORY` environment variables, then running `nvidia-cuda-mps-control -d`."
  },
  {
    "question": "How is the number of model replicas specified in the `pytorch-gpu-mps.sh` Slurm script?",
    "answer": "The number of model replicas is specified using the `#SBATCH --tasks-per-node` parameter in the Slurm script."
  },
  {
    "question": "What is the typical setup for wrapping a PyTorch model for DistributedDataParallel on a single GPU?",
    "answer": "The model should be wrapped with `torch.nn.parallel.DistributedDataParallel(net, device_ids=[current_device])`, where `current_device` is the GPU ID."
  },
  {
    "question": "When should one consider using Data Parallelism on a single GPU instead of multiple CPUs for a small model?",
    "answer": "It is not advisable to use a GPU for a small model unless it has a very large dataset and you wish to perform training with a small batch size, allowing Data Parallelism to increase resource usage and gain speed."
  }
]