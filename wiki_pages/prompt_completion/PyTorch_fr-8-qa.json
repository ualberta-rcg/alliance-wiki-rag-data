[
  {
    "question": "What are the characteristics of the small model task described for single-GPU Data Parallelism?",
    "answer": "The task involves a batch size of 512 images, with the model occupying approximately 1GB of GPU memory and utilizing only about 6% of the GPU's compute capacity during training."
  },
  {
    "question": "Is it advisable to train this specific small model on a GPU on clusters without using Data Parallelism?",
    "answer": "No, it is explicitly stated that this model should not be trained on a GPU on our clusters, as it would inefficiently use the GPU's capacity."
  },
  {
    "question": "How does Data Parallelism help when training a small model on a single GPU?",
    "answer": "Data Parallelism allows fitting several replicas of the small model onto a single GPU, thereby increasing resource usage and achieving a speed-up."
  },
  {
    "question": "What Nvidia technology is employed for efficiently placing multiple model replicas on one GPU?",
    "answer": "Nvidia's Multi-Process Service (MPS) is used for this purpose."
  },
  {
    "question": "What other technology, besides Nvidia MPS, is used in conjunction for single-GPU Data Parallelism?",
    "answer": "MPI (Message Passing Interface) is used along with Nvidia MPS."
  },
  {
    "question": "How many GPUs are requested in the `pytorch-gpu-mps.sh` SLURM script example?",
    "answer": "The script requests one GPU (`#SBATCH --gres=gpu:1`)."
  },
  {
    "question": "How is the number of model replicas configured in the `pytorch-gpu-mps.sh` SLURM script?",
    "answer": "The number of model replicas is set using the `#SBATCH --tasks-per-node` parameter, which is 8 in the example."
  },
  {
    "question": "What are the necessary commands to activate Nvidia MPS in the `pytorch-gpu-mps.sh` script?",
    "answer": "The commands are `export CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps`, `export CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log`, and `nvidia-cuda-mps-control -d`."
  },
  {
    "question": "Which backend is used for `dist.init_process_group` in the `cifar10-gpu-mps.py` script for single-GPU Data Parallelism?",
    "answer": "The 'mpi' backend is used (`backend=\"mpi\"`)."
  },
  {
    "question": "Why is NCCL not suitable as a backend for `dist.init_process_group` when using a single GPU?",
    "answer": "NCCL does not work on a single GPU because of a hard-coded multi-GPU topology check."
  },
  {
    "question": "How is the PyTorch model prepared for Data Parallelism in the `cifar10-gpu-mps.py` script?",
    "answer": "The model is wrapped using `torch.nn.parallel.DistributedDataParallel(net, device_ids=[current_device])`."
  },
  {
    "question": "What dataset is utilized in the `cifar10-gpu-mps.py` example for training?",
    "answer": "The CIFAR10 dataset is used for training."
  },
  {
    "question": "In the `cifar10-gpu-mps.py` script, how are the input data and targets transferred to the GPU?",
    "answer": "Inputs and targets are transferred to the GPU using `inputs = inputs.cuda()` and `targets = targets.cuda()` respectively."
  }
]