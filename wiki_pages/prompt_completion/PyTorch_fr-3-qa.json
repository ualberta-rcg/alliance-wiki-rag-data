[
  {
    "question": "What is the purpose of the `Net` class in the `cifar10-cpu.py` script?",
    "answer": "The `Net` class defines a convolutional neural network (CNN) model with convolutional layers, pooling layers, and fully connected layers for classification."
  },
  {
    "question": "How is the CIFAR10 dataset handled by default in the `cifar10-cpu.py` script?",
    "answer": "The `cifar10-cpu.py` script attempts to load the CIFAR10 dataset from the `./data` directory, with `download=False` set. Users are advised to download the data manually beforehand."
  },
  {
    "question": "What are the command-line steps to download the CIFAR10 dataset as instructed?",
    "answer": "First, create the directory with `mkdir -p data && cd data`, then download using `wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz`, extract with `tar zxf cifar-10-python.tar.gz`, and finally `cd ..`."
  },
  {
    "question": "What optimizer and loss function are configured in the `cifar10-cpu.py` example?",
    "answer": "The example uses `optim.SGD` (Stochastic Gradient Descent) as the optimizer and `nn.CrossEntropyLoss()` as the loss function."
  },
  {
    "question": "What is the `pytorch-multi-cpu.sh` script used for?",
    "answer": "The `pytorch-multi-cpu.sh` script is an example SLURM job submission script for running the `cifar10-cpu.py` application, specifically designed to observe the effect of CPU core allocation on performance."
  },
  {
    "question": "How does `pytorch-multi-cpu.sh` prepare the Python environment for the job?",
    "answer": "It loads a Python module, creates a virtual environment in `$SLURM_TMPDIR/env` without downloading, activates it, and then installs `torch` and `torchvision` using `pip install --no-index`."
  },
  {
    "question": "Which environment variable should be set to control the number of CPU threads in the `pytorch-multi-cpu.sh` script?",
    "answer": "The `OMP_NUM_THREADS` environment variable should be set to `$SLURM_CPUS_PER_TASK`."
  },
  {
    "question": "Is it always advisable to use a GPU for model training on clusters?",
    "answer": "No, it is not always advisable. For very small models, training might be faster with one or more CPUs, and users should not request a GPU if their code cannot make reasonable use of its compute capacity."
  },
  {
    "question": "What two factors contribute to the performance advantage of GPUs for deep learning tasks?",
    "answer": "GPUs offer performance advantages due to their ability to parallelize key operations on thousands of computing cores and their much larger memory bandwidth compared to CPUs."
  },
  {
    "question": "What specialized libraries does PyTorch use for parallel operations on GPUs?",
    "answer": "PyTorch uses specialized libraries such as CUDNN or MIOpen, depending on the hardware platform, for parallel implementations of deep learning operators like matrix product and convolution."
  },
  {
    "question": "Under what conditions is it beneficial to use a GPU for an AI task?",
    "answer": "It is beneficial to use a GPU when the task involves elements that can be massively parallelized, either in terms of the number of operations or the quantity of data to be processed, or ideally both, such as with large models or large input datasets."
  }
]