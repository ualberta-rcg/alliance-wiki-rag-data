[
  {
    "question": "What is the primary focus of this tutorial?",
    "answer": "This tutorial introduces the graphics processing unit (GPU) as a massively parallel computing device, the CUDA parallel programming language, and some of the CUDA numerical libraries for high performance computing."
  },
  {
    "question": "What programming languages are a prerequisite for this tutorial?",
    "answer": "A working knowledge of C or C++ is required, as this tutorial focuses on CUDA C/C++."
  },
  {
    "question": "What does the term 'CUDA C' refer to in this tutorial?",
    "answer": "The term 'CUDA C' refers to CUDA C/C++, which is essentially a C/C++ that allows one to execute functions on both GPUs and CPUs."
  },
  {
    "question": "What are the key learning objectives of this tutorial?",
    "answer": "The learning objectives include understanding GPU architecture, the workflow of a CUDA program, managing and understanding various types of GPU memories, and writing and compiling an example of CUDA code."
  },
  {
    "question": "What is a GPU?",
    "answer": "A GPU, or graphics processing unit, is a single-chip processor that performs rapid mathematical calculations for the purpose of rendering images."
  },
  {
    "question": "How have GPUs been utilized in recent years beyond image rendering?",
    "answer": "In recent years, the capability of GPUs has been harnessed more broadly to accelerate computational workloads in cutting-edge scientific research."
  },
  {
    "question": "What does CUDA stand for?",
    "answer": "CUDA stands for 'compute unified device architecture'."
  },
  {
    "question": "What is CUDA?",
    "answer": "CUDA is a scalable parallel programming model and software environment for parallel computing which provides access to instructions and memory of massively parallel elements in GPUs."
  },
  {
    "question": "What are the two main components of GPU architecture?",
    "answer": "The two main components of the GPU are Global memory and Streaming multiprocessors (SMs)."
  },
  {
    "question": "Describe Global memory within a GPU.",
    "answer": "Global memory is similar to CPU memory and is accessible by both CPUs and GPUs."
  },
  {
    "question": "What are Streaming multiprocessors (SMs) responsible for in a GPU?",
    "answer": "Streaming multiprocessors (SMs) consist of many streaming processors (SPs), perform actual computations, and each SM has its own control unit, registers, and execution pipelines."
  },
  {
    "question": "In the CUDA programming model, what is referred to as the 'Host'?",
    "answer": "The 'Host' refers to the CPU and its memory (host memory)."
  },
  {
    "question": "In the CUDA programming model, what is referred to as the 'Device'?",
    "answer": "The 'Device' refers to the GPU and its memory (device memory)."
  },
  {
    "question": "What kind of programming model does CUDA use?",
    "answer": "The CUDA programming model is a heterogeneous model in which both the CPU and GPU are used."
  },
  {
    "question": "What are 'kernels' in the context of CUDA programming?",
    "answer": "Kernels are GPU functions executed by many GPU threads in parallel, and CUDA code is capable of executing them."
  },
  {
    "question": "What are the five steps for a typical CUDA code recipe?",
    "answer": "A typical CUDA code recipe involves: declaring and allocating both host and device memories, initializing the host memory, transferring data from host memory to device memory, executing GPU functions (kernels), and transferring data back to the host memory."
  },
  {
    "question": "What is a 'kernel' in the CUDA execution model?",
    "answer": "Simple CUDA code executed on GPU is called a 'kernel'."
  },
  {
    "question": "What is a 'thread' in the GPU execution model?",
    "answer": "A 'thread' is the smallest set of instructions handled by the operating system's scheduler, executed sequentially by each GPU core (streaming processor)."
  },
  {
    "question": "How do all GPU cores execute a kernel?",
    "answer": "All GPU cores execute the kernel in a SIMT fashion (Single Instruction, Multiple Threads)."
  },
  {
    "question": "What is the recommended three-step procedure for executing on a GPU?",
    "answer": "The recommended procedure for executing on a GPU is: 1. Copy input data from CPU memory to GPU memory, 2. Load GPU program (kernel) and execute it, 3. Copy results from GPU memory back to CPU memory."
  },
  {
    "question": "How are threads organized in the CUDA block-threading model?",
    "answer": "In CUDA, all threads are structured in threading blocks, and these blocks are further organized into grids."
  },
  {
    "question": "What conditions must be met when distributing threads in CUDA?",
    "answer": "Threads within a block must cooperate via shared memory, and threads in different blocks cannot cooperate."
  },
  {
    "question": "How do threads within a block interact?",
    "answer": "Threads within a block work on the same set of instructions (but perhaps with different data sets) and exchange data between each other via shared memory."
  },
  {
    "question": "How do threads use IDs in the block-threading model to work on data?",
    "answer": "Each thread uses Block IDs (1D or 2D like blockIdx.x, blockIdx.y) and Thread IDs (1D, 2D, or 3D like threadIdx.x, threadIdx.y, threadIdx.z) to decide what data to work on."
  },
  {
    "question": "What is the benefit of using Block IDs and Thread IDs in the CUDA model?",
    "answer": "Such a model simplifies memory addressing when processing multi-dimensional data."
  },
  {
    "question": "How does a streaming microprocessor (SM) typically execute threading blocks?",
    "answer": "Usually, a streaming microprocessor (SM) executes one threading block at a time."
  },
  {
    "question": "What are 'warps' in CUDA thread scheduling?",
    "answer": "Code in CUDA is executed in groups of 32 threads, which are called warps."
  },
  {
    "question": "How does the hardware scheduler manage block execution in CUDA?",
    "answer": "A hardware scheduler is free to assign blocks to any SM at any time and can postpone or suspend execution of blocks, for example, when data becomes unavailable, to execute another ready threading block."
  },
  {
    "question": "What is 'zero-overhead scheduling' in CUDA?",
    "answer": "Zero-overhead scheduling makes execution more streamlined so that SMs are not idle, by having the scheduler execute another threading block when the current one is postponed or suspended due to data unavailability."
  },
  {
    "question": "Name the types of GPU memories available for CUDA operations.",
    "answer": "The types of GPU memories available for CUDA operations are Global memory, Shared memory, Registers and Local Memory, and Constant memory."
  },
  {
    "question": "What are the characteristics of Global memory for CUDA operations?",
    "answer": "Global memory is off-chip, good for I/O, but relatively slow."
  },
  {
    "question": "What are the characteristics of Shared memory for CUDA operations?",
    "answer": "Shared memory is on-chip, good for thread collaboration, and very fast."
  },
  {
    "question": "What are the characteristics of Registers and Local Memory for CUDA operations?",
    "answer": "Registers and Local Memory provide thread work space and are very fast."
  },
  {
    "question": "How do you allocate an object in device memory using CUDA?",
    "answer": "You allocate an object in device memory using `cudaMalloc((void**)&array, size)`, which requires the address of a pointer of the allocated array and its size."
  },
  {
    "question": "How do you deallocate an object from device memory using CUDA?",
    "answer": "You deallocate an object from device memory using `cudaFree(array)`, which requires just a pointer to the array."
  },
  {
    "question": "What is the purpose of the `cudaMemcpy` function?",
    "answer": "`cudaMemcpy` copies data from either device to host or host to device, requiring pointers to the arrays, size, and the direction type (e.g., cudaMemcpyHostToDevice, cudaMemcpyDeviceToHost)."
  },
  {
    "question": "How does `cudaMemcpyAsync` differ from `cudaMemcpy`?",
    "answer": "`cudaMemcpyAsync` is similar to `cudaMemcpy`, but it transfers data asynchronously, meaning it doesn't block the execution of other processes."
  },
  {
    "question": "How can parallelism be achieved by executing N different CUDA blocks at the same time?",
    "answer": "Parallelism can be achieved by calling the kernel with `add <<< N, 1 >>> (dev_a, dev_b, dev_c);` and modifying the kernel to use `blockIdx.x` for array indexing, such as `c[blockIdx.x] = a[blockIdx.x] + b[blockIdx.x];`."
  },
  {
    "question": "What is `blockIdx.x` used for when achieving parallelism with multiple CUDA blocks?",
    "answer": "`blockIdx.x` is the unique number identifying a CUDA block, allowing each block to perform operations on its specific portion of data, such as `a[blockIdx.x]` and `b[blockIdx.x]`."
  },
  {
    "question": "How can a job be distributed across parallel threads within a single block?",
    "answer": "A job can be distributed across parallel threads within a single block by calling the kernel with `add <<< 1, N >>> (dev_a, dev_b, dev_c);`."
  },
  {
    "question": "What is an advantage of parallelizing across multiple threads within a block?",
    "answer": "Unlike blocks, threads within a block can communicate with each other, which is advantageous when heavy communication is involved."
  },
  {
    "question": "When should code chunks be distributed across parallel blocks versus parallel threads?",
    "answer": "Chunks of code that can run independently (with little or no communication) are distributed across parallel blocks, whereas parallelizing across multiple threads in a block is used when heavy communication is involved."
  }
]