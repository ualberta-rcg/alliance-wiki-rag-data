[
  {
    "question": "What is the recommended form for requesting one or more GPUs for a Slurm job?",
    "answer": "The recommended form is `--gpus-per-node=[type:]number`."
  },
  {
    "question": "Is it mandatory to specify the GPU type when requesting GPUs using `--gpus-per-node`?",
    "answer": "No, specifying the GPU type is optional. The `[type:]` part of the `--gpus-per-node=[type:]number` notation means it may be optionally specified."
  },
  {
    "question": "Where can users find a list of valid GPU types for the `type` specifier?",
    "answer": "Valid GPU types are listed in the 'Available GPUs' table, in the column headed 'Slurm type specifier'."
  },
  {
    "question": "How do you request two GPUs per node of any available type?",
    "answer": "You can use the command `--gpus-per-node=2`."
  },
  {
    "question": "How do you request one V100 type GPU per node?",
    "answer": "You can use the command `--gpus-per-node=v100:1`."
  },
  {
    "question": "What is an older form for requesting GPUs, and is it still recommended?",
    "answer": "The older form is `--gres=gpu[[:type]:number]`. It is not recommended and is expected to be unsupported in future Slurm releases. Users should replace it with the `--gpus-per-node` form."
  },
  {
    "question": "What other directives can be used to request GPU resources in Slurm?",
    "answer": "Other directives include `--gpus`, `--gpus-per-socket`, `--gpus-per-task`, `--mem-per-gpu`, and `--ntasks-per-gpu`."
  },
  {
    "question": "What should a user do if they encounter unexpected results when requesting GPU resources?",
    "answer": "If you don't get the result you expect, you should contact technical support."
  },
  {
    "question": "How many nodes are in the B\u00e9luga cluster?",
    "answer": "The B\u00e9luga cluster has 172 nodes."
  },
  {
    "question": "What is the Slurm type specifier for GPUs on B\u00e9luga?",
    "answer": "The Slurm type specifier for B\u00e9luga is `v100`."
  },
  {
    "question": "What is the GPU model on B\u00e9luga nodes, and how much memory does it have?",
    "answer": "The GPU model on B\u00e9luga is V100-16gb, and it has 16 GiB of GPU memory."
  },
  {
    "question": "How many GPUs are available per node on B\u00e9luga?",
    "answer": "There are 4 GPUs per node on B\u00e9luga."
  },
  {
    "question": "How many CPU cores and CPU memory are on a B\u00e9luga node?",
    "answer": "A B\u00e9luga node has 40 CPU cores and 191000M CPU memory."
  },
  {
    "question": "What is the Compute Capability of V100-16gb GPUs on B\u00e9luga?",
    "answer": "The Compute Capability is 70."
  },
  {
    "question": "What are the characteristics of the GPU setup on B\u00e9luga?",
    "answer": "All GPUs are associated with the same CPU socket and connected via NVLink and SXM2."
  },
  {
    "question": "How many Cedar nodes use the `p100` Slurm type specifier?",
    "answer": "114 Cedar nodes use the `p100` Slurm type specifier."
  },
  {
    "question": "What is the GPU model and memory for the `p100` type on Cedar?",
    "answer": "The GPU model is P100-12gb with 12 GiB of GPU memory."
  },
  {
    "question": "How many GPUs per node are available for the `p100` type on Cedar?",
    "answer": "There are 4 GPUs per node for the `p100` type on Cedar."
  },
  {
    "question": "How are `p100` GPUs connected on Cedar (114 nodes)?",
    "answer": "Two GPUs per CPU socket, connected via PCIe."
  },
  {
    "question": "How many Cedar nodes use the `p100l` Slurm type specifier?",
    "answer": "32 Cedar nodes use the `p100l` Slurm type specifier."
  },
  {
    "question": "What is the GPU model and memory for the `p100l` type on Cedar?",
    "answer": "The GPU model is P100-16gb with 16 GiB of GPU memory."
  },
  {
    "question": "How much CPU memory is available per node for `p100l` type GPUs on Cedar?",
    "answer": "There is 257000M CPU memory per node for `p100l` type GPUs on Cedar."
  },
  {
    "question": "How are `p100l` GPUs connected on Cedar?",
    "answer": "All GPUs are associated with the same CPU socket and connected via PCIe."
  },
  {
    "question": "How many Cedar nodes use the `v100l` Slurm type specifier?",
    "answer": "192 Cedar nodes use the `v100l` Slurm type specifier."
  },
  {
    "question": "What is the GPU model and memory for the `v100l` type on Cedar?",
    "answer": "The GPU model is V100-32gb with 32 GiB of GPU memory."
  },
  {
    "question": "How are `v100l` GPUs connected on Cedar?",
    "answer": "Two GPUs per CPU socket; all GPUs connected via NVLink and SXM2."
  },
  {
    "question": "How many Graham nodes use the `p100` Slurm type specifier?",
    "answer": "160 Graham nodes use the `p100` Slurm type specifier."
  },
  {
    "question": "What is the GPU model for `p100` type on Graham, and how many GPUs are per node?",
    "answer": "The GPU model is P100-12gb, and there are 2 GPUs per node."
  },
  {
    "question": "How are `p100` GPUs connected on Graham (160 nodes)?",
    "answer": "One GPU per CPU socket, connected via PCIe."
  },
  {
    "question": "How many Graham nodes have V100-16gb GPUs?",
    "answer": "7 Graham nodes have V100-16gb GPUs."
  },
  {
    "question": "What is the Slurm type specifier for the 16GB V100 GPU on Graham, and how many GPUs are per node?",
    "answer": "The Slurm type specifier is `v100(**)`, and there are 8 GPUs per node."
  },
  {
    "question": "How many Graham nodes have V100-32gb GPUs?",
    "answer": "2 Graham nodes have V100-32gb GPUs."
  },
  {
    "question": "What is the Slurm type specifier for the 32GB V100 GPU on Graham, and how many GPUs are per node?",
    "answer": "The Slurm type specifier is `v100(***)`, and there are 8 GPUs per node."
  },
  {
    "question": "How many Graham nodes have T4 GPUs?",
    "answer": "There are 30 nodes and another 6 nodes that have T4 GPUs."
  },
  {
    "question": "What is the GPU model and memory for the `t4` type on Graham?",
    "answer": "The GPU model is T4-16gb with 16 GiB of GPU memory."
  },
  {
    "question": "How many GPUs are per node for the `t4` type on Graham?",
    "answer": "There are 4 GPUs per node for the `t4` type on Graham."
  },
  {
    "question": "How are T4 GPUs connected on Graham?",
    "answer": "Two GPUs per CPU socket."
  },
  {
    "question": "How many nodes are in the Mist cluster?",
    "answer": "The Mist cluster has 54 nodes."
  },
  {
    "question": "What is the Slurm type specifier for GPUs on Mist?",
    "answer": "Mist does not have a listed Slurm type specifier, indicated as `(none)`."
  },
  {
    "question": "What is the GPU model on Mist nodes, and how much memory does it have?",
    "answer": "The GPU model on Mist is V100-32gb, and it has 32 GiB of GPU memory."
  },
  {
    "question": "How many GPUs are available per node on Mist?",
    "answer": "There are 4 GPUs per node on Mist."
  },
  {
    "question": "How much CPU memory is on a Mist node?",
    "answer": "A Mist node has 256GiB of CPU memory."
  },
  {
    "question": "How many nodes are in the Narval cluster?",
    "answer": "The Narval cluster has 159 nodes."
  },
  {
    "question": "What is the Slurm type specifier for GPUs on Narval?",
    "answer": "The Slurm type specifier for Narval is `a100`."
  },
  {
    "question": "What is the GPU model and memory for the `a100` type on Narval?",
    "answer": "The GPU model is A100-40gb with 40 GiB of GPU memory."
  },
  {
    "question": "How many GPUs are available per node on Narval?",
    "answer": "There are 4 GPUs per node on Narval."
  },
  {
    "question": "What is the Compute Capability of A100-40gb GPUs on Narval?",
    "answer": "The Compute Capability is 80."
  },
  {
    "question": "How are A100 GPUs connected on Narval?",
    "answer": "Two GPUs per CPU socket; all GPUs connected via NVLink."
  },
  {
    "question": "What are the CPU resources on a Narval node with GPUs?",
    "answer": "Narval nodes have 48 CPU cores and 510000M CPU memory."
  },
  {
    "question": "Can Arbutus cloud resources be scheduled via Slurm?",
    "answer": "No, Arbutus cloud resources are not schedulable via Slurm."
  }
]