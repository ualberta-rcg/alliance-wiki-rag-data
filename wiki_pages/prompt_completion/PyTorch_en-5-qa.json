[
  {
    "question": "What is a key requirement for models when using multiple GPUs with Data Parallelism?",
    "answer": "For multiple-GPU data parallelism, the model must be small enough to fit inside the memory of a single GPU."
  },
  {
    "question": "What are the primary ways to perform Data Parallelism in PyTorch, as mentioned in the document?",
    "answer": "Data Parallelism in PyTorch can be performed using the `DistributedDataParallel` class directly with one or multiple GPUs, or by utilizing the `PyTorch Lightning` package."
  },
  {
    "question": "Which PyTorch class is recommended by PyTorch maintainers for multi-GPU data parallelism?",
    "answer": "The `DistributedDataParallel` class is recommended by PyTorch maintainers for using multiple GPUs, whether on a single node or distributed across multiple nodes."
  },
  {
    "question": "What is the purpose of the `pytorch-ddp-test.sh` script?",
    "answer": "The `pytorch-ddp-test.sh` script is an example job submission script for running a PyTorch Distributed Data Parallel (DDP) application on a cluster, specifically requesting 2 GPUs."
  },
  {
    "question": "How many GPUs and tasks per node does the `pytorch-ddp-test.sh` SLURM script request?",
    "answer": "The `pytorch-ddp-test.sh` SLURM script requests 2 GPUs (`--gres=gpu:2`) and 2 tasks per node (`--tasks-per-node=2`), implying one process per GPU."
  },
  {
    "question": "How is the `MASTER_ADDR` environment variable set within the `pytorch-ddp-test.sh` script?",
    "answer": "The `MASTER_ADDR` environment variable is set to the hostname (`export MASTER_ADDR=$(hostname)`) to store the master node's IP address."
  },
  {
    "question": "What is the role of the `srun` command in the `pytorch-ddp-test.sh` script?",
    "answer": "The `srun` command in `pytorch-ddp-test.sh` is used to execute the Python script `pytorch-ddp-test.py` multiple times, corresponding to the number of tasks per node times the number of nodes (`$((SLURM_NTASKS_PER_NODE * SLURM_JOB_NUM_NODES))`), distributing the processes across the allocated resources."
  },
  {
    "question": "What is the primary function of the `pytorch-ddp-test.py` Python script?",
    "answer": "The `pytorch-ddp-test.py` Python script demonstrates a distributed data parallel test for CIFAR10 classification models, utilizing PyTorch's `DistributedDataParallel` for multi-GPU training."
  },
  {
    "question": "How does `pytorch-ddp-test.py` initialize the distributed process group?",
    "answer": "The `pytorch-ddp-test.py` script initializes the distributed process group using `dist.init_process_group(backend=args.dist_backend, init_method=args.init_method, world_size=args.world_size, rank=rank)`."
  },
  {
    "question": "Which specific PyTorch module is used to wrap the neural network for distributed data parallelism in `pytorch-ddp-test.py`?",
    "answer": "The `torch.nn.parallel.DistributedDataParallel` module is used to wrap the neural network (e.g., `net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[current_device])`) for distributed training."
  }
]