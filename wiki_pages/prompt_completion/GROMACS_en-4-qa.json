[
  {
    "question": "How do you load a GPU-enabled version of GROMACS?",
    "answer": "To load a GPU-enabled version, the `cuda` module needs to be loaded first, along with the standard environment, compiler, MPI, and GROMACS modules. For example, `module load StdEnv/2023 gcc/12.3 openmpi/4.1.5 cuda/12.2 gromacs/2024.4`."
  },
  {
    "question": "What precision is supported for GPU-enabled GROMACS versions?",
    "answer": "GPU-enabled GROMACS versions are available only with single precision."
  },
  {
    "question": "What are the four binaries that GROMACS 5.x, 2016.x, and newer releases consist of?",
    "answer": "The four binaries are `gmx` (mixed/single precision, OpenMP, no MPI), `gmx_mpi` (mixed/single precision, OpenMP, MPI), `gmx_d` (double precision, OpenMP, no MPI), and `gmx_mpi_d` (double precision, OpenMP, MPI)."
  },
  {
    "question": "What does the `gmx_mpi` binary indicate for GROMACS 5.x and newer?",
    "answer": "The `gmx_mpi` binary signifies a mixed (\"single\") precision GROMACS version with OpenMP (threading) and MPI support."
  },
  {
    "question": "How are double precision binaries identified in GROMACS 4.6.7?",
    "answer": "In GROMACS 4.6.7, double precision binaries have the suffix `_d`."
  },
  {
    "question": "What are the names of the parallel `mdrun` binaries for GROMACS 4.6.7?",
    "answer": "The parallel `mdrun` binaries for GROMACS 4.6.7 are `mdrun_mpi` for single precision and `mdrun_mpi_d` for double precision."
  },
  {
    "question": "What is the recommended command to load GROMACS 2024.4 for a serial job in `StdEnv/2023`?",
    "answer": "The recommended command is `module load StdEnv/2023 gcc/12.3 openmpi/4.1.5 gromacs/2024.4`."
  },
  {
    "question": "What is the `mdrun` command used in the example serial job script?",
    "answer": "The `mdrun` command used is `gmx mdrun -nt 1 -deffnm em`."
  },
  {
    "question": "What general rule should be followed for `--ntasks-per-node` and `--cpus-per-task` when running GROMACS on whole nodes?",
    "answer": "Generally, the product of `--ntasks-per-node` and `--cpus-per-task` should match the total number of CPU cores available on the cluster's compute nodes."
  },
  {
    "question": "What action should be taken if GROMACS reports an error regarding domain decomposition being impossible in a whole node simulation?",
    "answer": "If GROMACS reports a domain decomposition error, you should halve `--ntasks-per-node` and double `--cpus-per-task`."
  },
  {
    "question": "How many MPI tasks and OpenMP threads are requested per node for a GROMACS whole node job on Narval, according to the example script?",
    "answer": "The example script for Narval requests 32 MPI tasks per node (`--ntasks-per-node=32`) and 2 OpenMP threads per MPI task (`--cpus-per-task=2`), totaling 64 CPUs per node."
  }
]