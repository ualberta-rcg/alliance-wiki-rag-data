[
  {
    "question": "How many cores are available per CPU node on Rorqual?",
    "answer": "Each CPU node has a total of 192 cores."
  },
  {
    "question": "How are cores grouped within a CPU node for L3 cache sharing?",
    "answer": "Cores are grouped in sets of 8, sharing the same L3 cache, which is ideal for multithreaded parallel programs."
  },
  {
    "question": "What is the composition of a NUMA node in a CPU node?",
    "answer": "A NUMA node consists of 24 cores (3x8) that share a trio of system memory channels."
  },
  {
    "question": "How can users maximize the benefits of the CPU node topology on Rorqual?",
    "answer": "To fully leverage the topology, users should reserve complete nodes (e.g., using `--ntasks-per-node=24 --cpus-per-task=8`) and explicitly control the placement of processes and execution threads."
  },
  {
    "question": "Describe the CPU socket architecture within a GPU node on Rorqual.",
    "answer": "Each GPU node contains two CPU sockets. Each socket has eight system memory channels, 60 MiB of L3 cache, and 32 equidistant cores, each with its own 2 MiB L2 cache and 32+48 KiB L1 cache, plus two NVidia H100 accelerators."
  },
  {
    "question": "How many NVidia H100 accelerators are present in a GPU node?",
    "answer": "A GPU node has a total of four NVidia H100 accelerators."
  },
  {
    "question": "How are the accelerators interconnected within a GPU node?",
    "answer": "The four accelerators in a GPU node are interconnected by SXM5."
  },
  {
    "question": "What are the available GPU instance models on Rorqual?",
    "answer": "The available GPU instance models are H100-80gb, H100-1g.10gb, H100-2g.20gb, and H100-3g.40gb."
  },
  {
    "question": "What is the full name for the H100-80gb GPU model?",
    "answer": "The full name for the H100-80gb GPU model is `nvidia_h100_80gb_hbm3`."
  },
  {
    "question": "What are the different ways to refer to the H100-1g.10gb MIG instance?",
    "answer": "It can be referred to as `h100_1g.10gb` (short name), `h100_1.10` (without unit), `h100_10gb` (by its memory), or `nvidia_h100_80gb_hbm3_1g.10gb` (full name)."
  },
  {
    "question": "How do you request a single H100-80gb GPU using Slurm?",
    "answer": "You can request a single H100-80gb GPU with Slurm options like `--gpus=h100:1` or `--gpus=h100_80gb:1`."
  },
  {
    "question": "What Slurm options are used to request multiple H100-80gb GPUs on a single node?",
    "answer": "To request multiple H100-80gb GPUs per node, you can use `--gpus-per-node=h100:2`, `--gpus-per-node=h100:3`, or `--gpus-per-node=h100:4`."
  },
  {
    "question": "How can you request a specific number of H100-80gb GPUs that can be scattered across nodes?",
    "answer": "You can request a specific number `n` of H100-80gb GPUs using the option `--gpus=h100:n`."
  },
  {
    "question": "Approximately how many Rorqual GPU nodes are configured with Multi-Instance GPU (MIG) technology?",
    "answer": "About half of the Rorqual GPU nodes are configured with MIG technology."
  },
  {
    "question": "What are the three available sizes for MIG instances on Rorqual?",
    "answer": "The three available MIG instance sizes are H100-1g.10gb, H100-2g.20gb, and H100-3g.40gb."
  },
  {
    "question": "What are the specifications of the H100-1g.10gb MIG instance?",
    "answer": "The H100-1g.10gb instance offers 1/8 of the computing power with 10 GB of GPU memory."
  },
  {
    "question": "What Slurm option is used to request a single H100-2g.20gb MIG instance for a computing task?",
    "answer": "To request a single H100-2g.20gb MIG instance, use the Slurm option `--gpus=h100_2g.20gb:1`."
  },
  {
    "question": "How much GPU memory does the H100-3g.40gb MIG instance provide?",
    "answer": "The H100-3g.40gb MIG instance provides 40 GB of GPU memory."
  }
]