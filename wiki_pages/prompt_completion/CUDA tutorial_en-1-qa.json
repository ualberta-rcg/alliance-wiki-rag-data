[
  {
    "question": "What is the primary focus of this tutorial?",
    "answer": "This tutorial introduces the graphics processing unit (GPU) as a massively parallel computing device, the CUDA parallel programming language, and CUDA numerical libraries for high performance computing."
  },
  {
    "question": "What programming language knowledge is required to gain the most benefit from this tutorial?",
    "answer": "A working knowledge of C or C++ is required, as the tutorial uses CUDA to accelerate C or C++ code (referred to as CUDA C)."
  },
  {
    "question": "What is CUDA C?",
    "answer": "CUDA C is essentially a C/C++ dialect that allows one to execute functions on both GPUs and CPUs."
  },
  {
    "question": "What are the learning objectives of this tutorial?",
    "answer": "The learning objectives include understanding GPU architecture, the workflow of a CUDA program, managing and understanding various types of GPU memories, and writing and compiling an example of CUDA code."
  },
  {
    "question": "What is a GPU?",
    "answer": "A GPU, or graphics processing unit, is a single-chip processor that performs rapid mathematical calculations primarily for rendering images, but also harnessed to accelerate computational workloads in scientific research."
  },
  {
    "question": "What does CUDA stand for?",
    "answer": "CUDA stands for 'compute unified device architecture'."
  },
  {
    "question": "What is CUDA?",
    "answer": "CUDA is a scalable parallel programming model and software environment for parallel computing that provides access to instructions and memory of massively parallel elements in GPUs."
  },
  {
    "question": "What are the two main components of a GPU's architecture?",
    "answer": "The two main components of a GPU are Global memory and Streaming multiprocessors (SMs)."
  },
  {
    "question": "How is Global memory similar to CPU memory?",
    "answer": "Global memory is similar to CPU memory and is accessible by both CPUs and GPUs."
  },
  {
    "question": "What are Streaming multiprocessors (SMs) and what do they contain?",
    "answer": "Streaming multiprocessors (SMs) consist of many streaming processors (SPs) that perform actual computations, and each SM has its own control unit, registers, and execution pipelines."
  },
  {
    "question": "In CUDA terminology, what is a 'Host'?",
    "answer": "In CUDA terminology, the 'Host' refers to the CPU and its memory (host memory)."
  },
  {
    "question": "In CUDA terminology, what is a 'Device'?",
    "answer": "In CUDA terminology, the 'Device' refers to the GPU and its memory (device memory)."
  },
  {
    "question": "What type of programming model is the CUDA programming model?",
    "answer": "The CUDA programming model is a heterogeneous model in which both the CPU and GPU are used."
  },
  {
    "question": "What are GPU functions called in CUDA?",
    "answer": "GPU functions in CUDA are called kernels."
  },
  {
    "question": "How are kernels executed in CUDA?",
    "answer": "Kernels are executed by many GPU threads in parallel."
  },
  {
    "question": "What is the five-step recipe for a typical CUDA code?",
    "answer": "A typical CUDA code involves: 1) Declaring and allocating both host and device memories, 2) Initializing the host memory, 3) Transferring data from Host memory to device memory, 4) Executing GPU functions (kernels), and 5) Transferring data back to the host memory."
  },
  {
    "question": "What is a 'kernel' in the context of CUDA execution?",
    "answer": "A kernel is simple CUDA code executed on a GPU."
  },
  {
    "question": "How does each GPU core execute a thread?",
    "answer": "Each GPU core (streaming processor) executes a sequential 'thread'."
  },
  {
    "question": "What is a 'thread' in the context of GPU execution?",
    "answer": "A 'thread' is the smallest set of instructions handled by the operating system's scheduler."
  },
  {
    "question": "How do all GPU cores execute a kernel?",
    "answer": "All GPU cores execute the kernel in a SIMT (Single Instruction, Multiple Threads) fashion."
  },
  {
    "question": "What is the recommended procedure for executing on a GPU?",
    "answer": "The recommended procedure is to 1) Copy input data from CPU memory to GPU memory, 2) Load the GPU program (kernel) and execute it, and 3) Copy results from GPU memory back to CPU memory."
  },
  {
    "question": "How are threads organized in a CUDA kernel to achieve massive parallelism?",
    "answer": "In CUDA, threads are structured in threading blocks, and these blocks are further organized into grids."
  },
  {
    "question": "What are the rules for thread cooperation in the CUDA block-threading model?",
    "answer": "Threads within a block cooperate via shared memory, but threads in different blocks cannot cooperate."
  },
  {
    "question": "How do threads within a block share data?",
    "answer": "Threads within a block work on the same set of instructions (though possibly with different data sets) and exchange data between each other via shared memory."
  },
  {
    "question": "How do threads use IDs in the block-threading model?",
    "answer": "Each thread uses IDs (Block IDs and Thread IDs) to decide what data to work on."
  },
  {
    "question": "What are the types of IDs used by CUDA threads?",
    "answer": "CUDA threads use Block IDs (1D or 2D, e.g., blockIdx.x, blockIdx.y) and Thread IDs (1D, 2D, or 3D, e.g., threadIdx.x, threadIdx.y, threadIdx.z)."
  },
  {
    "question": "What is the benefit of the CUDA ID model for threads?",
    "answer": "This model simplifies memory addressing when processing multi-dimensional data."
  },
  {
    "question": "How many threading blocks does a streaming microprocessor (SM) typically execute at a time?",
    "answer": "A streaming microprocessor (SM) usually executes one threading block at a time."
  },
  {
    "question": "What are 'warps' in CUDA, and how many threads do they contain?",
    "answer": "Code in CUDA is executed in groups of 32 threads, which are called warps."
  },
  {
    "question": "Who is responsible for assigning blocks to Streaming Multiprocessors (SMs)?",
    "answer": "A hardware scheduler is free to assign blocks to any SM at any time."
  },
  {
    "question": "Can an assigned threading block be executed non-stop by an SM?",
    "answer": "No, the scheduler can postpone or suspend the execution of blocks under certain conditions, such as when data becomes unavailable."
  },
  {
    "question": "What happens if a threading block's execution is postponed?",
    "answer": "When a block's execution is postponed, the scheduler executes another threading block which is ready for execution."
  },
  {
    "question": "What is 'zero-overhead scheduling' in CUDA?",
    "answer": "Zero-overhead scheduling refers to the mechanism where the scheduler executes another ready threading block when an active one is postponed, making execution more streamlined and preventing SMs from being idle."
  }
]