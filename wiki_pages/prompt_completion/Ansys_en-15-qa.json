[
  {
    "question": "What is the recommended memory allocation for all cores in a Shared Memory Parallel (SMP) Mechanical job using GPUs?",
    "answer": "The recommended memory allocation is `#SBATCH --mem=32G` for all cores."
  },
  {
    "question": "How many cores are typically specified for a Shared Memory Parallel (SMP) Mechanical job with GPUs?",
    "answer": "Eight cores (`#SBATCH --ntasks=8`) are specified for an SMP Mechanical job with GPUs."
  },
  {
    "question": "How many nodes are used for a Shared Memory Parallel (SMP) Mechanical job with GPUs?",
    "answer": "Only one node (`#SBATCH --nodes=1`) is used for an SMP Mechanical job with GPUs."
  },
  {
    "question": "How many GPUs per node are specified for a Shared Memory Parallel (SMP) Mechanical job?",
    "answer": "One GPU per node (`#SBATCH --gpus-per-node=1`) is specified."
  },
  {
    "question": "Which Ansys module is loaded for a Shared Memory Parallel (SMP) Mechanical job with StdEnv/2023?",
    "answer": "`ansys/2024R1.03` is loaded."
  },
  {
    "question": "What environment variable needs to be exported to print GPU devices in a Mechanical GPU job?",
    "answer": "`export ANSGPU_PRINTDEVICES=1` needs to be exported."
  },
  {
    "question": "How is a Shared Memory Parallel (SMP) Mechanical job with GPU acceleration run?",
    "answer": "It's run using `mapdl -smp -acc nvidia -na $SLURM_GPUS_ON_NODE -b nolist -np $SLURM_NTASKS -dir outdir-$SLURM_JOBID -i YOURAPDLFILE.inp`."
  },
  {
    "question": "What is the memory per CPU specified for a Distributed Memory Parallel (DMP) Mechanical job using GPUs?",
    "answer": "The memory per CPU is specified as `#SBATCH --mem-per-cpu=4G`."
  },
  {
    "question": "How many nodes are specified for a Distributed Memory Parallel (DMP) Mechanical job using GPUs?",
    "answer": "One node (`#SBATCH --nodes=1`) is specified for a DMP Mechanical job with GPUs."
  },
  {
    "question": "How many cores per node are specified for a Distributed Memory Parallel (DMP) Mechanical job using GPUs?",
    "answer": "Eight cores per node (`#SBATCH --ntasks-per-node=8`) are specified."
  },
  {
    "question": "What specific library path adjustment is made for Cedar clusters when running Distributed Memory Parallel (DMP) Mechanical jobs with GPUs?",
    "answer": "If the cluster is Cedar, a symbolic link to `libstdc++.so.6.0.29` is created in `outdir-$SLURM_JOBID`, and `LD_LIBRARY_PATH` is exported to point to this directory."
  },
  {
    "question": "How is a Distributed Memory Parallel (DMP) Mechanical GPU job executed on Beluga?",
    "answer": "On Beluga, `export KMP_AFFINITY=none` is set, and the job is run with `mapdl -dis -acc nvidia -na $SLURM_GPUS_ON_NODE -mpi intelmpi -b nolist -np $SLURM_NTASKS -dir outdir-$SLURM_JOBID -i YOURAPDLFILE.inp`."
  },
  {
    "question": "How is a Distributed Memory Parallel (DMP) Mechanical GPU job executed on clusters other than Beluga?",
    "answer": "It is executed with `mapdl -dis -acc nvidia -na $SLURM_GPUS_ON_NODE -mpi openmpi -b nolist -np $SLURM_NTASKS -dir outdir-$SLURM_JOBID -i YOURAPDLFILE.inp`."
  },
  {
    "question": "What is the default total memory and database memory allocated for APDL jobs by Ansys?",
    "answer": "Ansys allocates 1024 MB total memory and 1024 MB database memory by default for APDL jobs."
  },
  {
    "question": "How can the default memory allocation for APDL jobs be manually adjusted?",
    "answer": "The values can be changed by adding arguments `-m <total_memory_MB>` and/or `-db <database_memory_MB>` to the `mapdl` command line."
  },
  {
    "question": "What arguments might be required when using a remote institutional license server with multiple Ansys licenses for APDL jobs?",
    "answer": "It may be necessary to add `-p aa_r` or `-ppf anshpc` to the `mapdl` command, depending on the Ansys module used."
  },
  {
    "question": "Which type of Mechanical parallel scripts typically offer better performance?",
    "answer": "The single node (SMP Shared Memory Parallel) scripts typically perform better than the multinode (DIS Distributed Memory Parallel) scripts."
  },
  {
    "question": "What is a best practice for Ansys module version compatibility with APDL input files?",
    "answer": "The Ansys module loaded in the script should ideally match the version used to generate the input file."
  },
  {
    "question": "How can one check the Workbench version used to generate an APDL input file?",
    "answer": "By running `cat YOURAPDLFILE.inp | grep version` on the command line, you can find a line like `! ANSYS input file written by Workbench version 2019 R3`."
  },
  {
    "question": "Can Ansys Rocky be run in non-GUI mode?",
    "answer": "Yes, Ansys Rocky can run simulations in non-GUI mode."
  },
  {
    "question": "What hardware configurations does Ansys Rocky support for simulations?",
    "answer": "Both CPUs only and CPUs with GPUs are supported."
  },
  {
    "question": "Where are the provided Ansys Rocky Slurm scripts currently usable?",
    "answer": "The scripts are currently only usable on Graham, as the Rocky module is installed there locally."
  },
  {
    "question": "How can a user get a full listing of command-line options for Ansys Rocky?",
    "answer": "By loading any Rocky module and running `Rocky -h` on the command line."
  },
  {
    "question": "When running GPU-based Ansys Rocky for coupled problems, how many CPUs should be requested?",
    "answer": "The number of CPUs should be increased to a maximum until the scalability limit of the coupled application is reached."
  },
  {
    "question": "How many CPUs should be requested when running GPU-based Ansys Rocky for standalone uncoupled problems?",
    "answer": "Only a minimal number of CPUs, possibly 2 or 3, should be requested for optimal performance."
  },
  {
    "question": "Under what condition are 'rocky_hpc' licenses required for Ansys Rocky?",
    "answer": "`rocky_hpc` licenses are required when Rocky is run with 4 or more CPUs."
  }
]