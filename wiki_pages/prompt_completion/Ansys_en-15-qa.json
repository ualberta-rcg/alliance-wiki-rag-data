[
  {
    "question": "What are the default memory allocations for APDL jobs in Ansys?",
    "answer": "Ansys allocates 1024 MB total memory and 1024 MB database memory by default for APDL jobs."
  },
  {
    "question": "How can the default memory allocations for APDL jobs be changed?",
    "answer": "These values can be manually specified (or changed) by adding arguments `-m 1024` and/or `-db 1024` to the mapdl command line in the submission scripts."
  },
  {
    "question": "What license server arguments might be necessary when using a remote institutional license server with multiple Ansys licenses for APDL jobs?",
    "answer": "It may be necessary to add `-p aa_r` or `-ppf anshpc` to the mapdl command line, depending on which Ansys module you are using."
  },
  {
    "question": "Which type of Mechanical scripts (SMP or multinode DMP) generally performs better and should be used whenever possible?",
    "answer": "The single node (SMP Shared Memory Parallel) scripts will typically perform better than the multinode (DIS Distributed Memory Parallel) scripts and therefore should be used whenever possible."
  },
  {
    "question": "How can users ensure compatibility between the Ansys module loaded and the version used to generate an APDL input file?",
    "answer": "To help avoid compatibility issues, the Ansys module loaded in your script should ideally match the version used to generate the input file, which can be checked by grepping the input file for 'version'."
  },
  {
    "question": "What are the two modes in which Ansys Rocky can run simulations?",
    "answer": "Ansys Rocky can run simulations in GUI mode or non-GUI mode."
  },
  {
    "question": "What compute resources does Ansys Rocky support for its simulations?",
    "answer": "Ansys Rocky supports running simulations with CPUs only or with CPUs and GPUs."
  },
  {
    "question": "On which cluster are the provided Ansys Rocky Slurm scripts currently usable?",
    "answer": "The provided Ansys Rocky Slurm scripts are only usable on Graham, as the Rocky module is currently only installed there locally."
  },
  {
    "question": "How can a user get a full listing of command-line options for Ansys Rocky?",
    "answer": "To get a full listing of command-line options, run `Rocky -h` on the command line after loading any Rocky module."
  },
  {
    "question": "When running GPU-based Rocky for coupled problems, how many CPUs should be requested from Slurm?",
    "answer": "When Rocky is being run with GPUs to solve coupled problems, the number of CPUs requested from Slurm should be increased to a maximum until the scalability limit of the coupled application is reached."
  },
  {
    "question": "When running GPU-based Rocky for standalone uncoupled problems, what is the recommendation for CPU requests?",
    "answer": "If Rocky is run with GPUs to solve standalone uncoupled problems, only a minimal number of CPUs (e.g., 2 or possibly 3) should be requested that will allow Rocky to still run optimally."
  },
  {
    "question": "What type of licenses are required if Ansys Rocky is run with 4 or more CPUs?",
    "answer": "If Rocky is run with 4 or more CPUs, `rocky_hpc` licenses will be required."
  },
  {
    "question": "Which license server provides the `rocky_hpc` licenses?",
    "answer": "The SHARCNET license provides `rocky_hpc` licenses."
  },
  {
    "question": "Which Ansys Rocky module is currently available on Graham for StdEnv/2023?",
    "answer": "The `ansysrocky/2024R2.0` module is available on Graham for StdEnv/2023."
  },
  {
    "question": "What is the SBATCH directive to specify the number of GPUs per node for a Mechanical GPU job?",
    "answer": "The SBATCH directive for specifying GPUs per node is `--gpus-per-node=1`, or `--gpus-per-node=h100:1` for specific types like H100."
  },
  {
    "question": "How do you enable printing of detected GPU devices in a Mechanical GPU job script?",
    "answer": "You can enable printing of detected GPU devices by setting `export ANSGPU_PRINTDEVICES=1` in the script."
  },
  {
    "question": "What is the `mapdl` command for running a Shared Memory Parallel (SMP) Mechanical job with GPU acceleration?",
    "answer": "The `mapdl` command is `mapdl -smp -acc nvidia -na $SLURM_GPUS_ON_NODE -b nolist -np $SLURM_NTASKS -dir outdir-$SLURM_JOBID -i YOURAPDLFILE.inp`."
  },
  {
    "question": "How is the `LD_LIBRARY_PATH` handled for a Distributed Memory Parallel (DMP) GPU Mechanical job on the Cedar cluster?",
    "answer": "On the Cedar cluster, a symbolic link to `libstdc++.so.6.0.29` from the GCC library is created in the job's output directory, and this directory is then added to the `LD_LIBRARY_PATH`."
  },
  {
    "question": "Which MPI implementation is used for Distributed Memory Parallel (DMP) GPU Mechanical jobs on Beluga versus other clusters?",
    "answer": "On Beluga, `intelmpi` is used, while on other clusters, `openmpi` is used for DMP GPU Mechanical jobs."
  }
]