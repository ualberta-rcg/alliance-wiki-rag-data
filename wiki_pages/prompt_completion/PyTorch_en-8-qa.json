[
  {
    "question": "When is it advisable to use Data Parallelism on a single GPU?",
    "answer": "It is advisable when a model is fairly small but you have a very large dataset and wish to perform training with a small batch size, as it can increase resource usage and provide a speed-up."
  },
  {
    "question": "What Nvidia technology is used with MPI for efficient single GPU data parallelism?",
    "answer": "Nvidia's Multi-Process Service (MPS) is used along with MPI to efficiently place multiple model replicas on one GPU."
  },
  {
    "question": "What environment variables need to be set to activate Nvidia MPS?",
    "answer": "You need to export `CUDA_MPS_PIPE_DIRECTORY` and `CUDA_MPS_LOG_DIRECTORY`."
  },
  {
    "question": "What command activates the Nvidia Multi-Process Service (MPS)?",
    "answer": "The command `nvidia-cuda-mps-control -d` activates Nvidia MPS."
  },
  {
    "question": "In the context of single GPU data parallelism, why does NCCL not work as a backend for `dist.init_process_group`?",
    "answer": "NCCL does not work on a single GPU due to a hard-coded multi-GPU topology check."
  },
  {
    "question": "What backends are recommended for `dist.init_process_group` when performing single GPU data parallelism with MPS?",
    "answer": "The recommended backends are \"mpi\" or \"gloo\"."
  },
  {
    "question": "What is Fully Sharded Data Parallelism (FSDP)?",
    "answer": "Fully Sharded Data Parallelism (FSDP) enables distributed storage and computing of different elements of a training task, such as optimizer states, model weights, model gradients, and model activations, across multiple devices like GPUs, CPUs, and local hard disks."
  },
  {
    "question": "What is the primary benefit of using FSDP?",
    "answer": "FSDP's 'pooling' of resources, especially for storage, allows models with massive amounts of parameters to be trained efficiently across multiple nodes."
  },
  {
    "question": "When should FSDP not be used?",
    "answer": "You should not use FSDP if your model has layers that do not fit entirely in the memory of a single GPU, because a sharded layer may be collected inside a single device during a forward or backward pass."
  },
  {
    "question": "What is Tensor Parallelism (TP)?",
    "answer": "Tensor Parallelism (TP) is a model sharding approach where the computation of a forward or backward pass through a model layer is split along with the layers' weights across multiple devices."
  },
  {
    "question": "How does Tensor Parallelism (TP) differ from Fully Sharded Data Parallelism (FSDP) in terms of computation?",
    "answer": "With TP, computation steps are done locally in the device where a model shard is placed, whereas FSDP may collect sharded layers into a single device during computation, introducing overhead."
  },
  {
    "question": "What is Pipeline Parallelism (PP)?",
    "answer": "Pipeline Parallelism (PP) is a model sharding approach where the shards are groups of consecutive layers of a model, with each block placed on a different device."
  },
  {
    "question": "How does Pipeline Parallelism mitigate the waiting time for devices during sequential computations?",
    "answer": "To mitigate waiting time, every input batch is broken into 'micro-batches' which are fed to the model in sequence, ensuring all devices stay busy as the first micro-batch reaches the last model block."
  }
]