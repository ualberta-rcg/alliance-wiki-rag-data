[
  {
    "question": "What is Data Parallelism in the context of model training?",
    "answer": "Data Parallelism refers to methods used to train a model by running multiple replicas of it in parallel, where each replica processes a different chunk of training data during each iteration."
  },
  {
    "question": "How are gradients and parameters handled when using Data Parallelism?",
    "answer": "At the end of an iteration, gradients are aggregated, and the parameters of all model replicas are updated either synchronously or asynchronously, depending on the specific method."
  },
  {
    "question": "What is a significant advantage of using Data Parallelism?",
    "answer": "Data Parallelism can provide a significant speed-up by iterating through all examples in a large dataset approximately N times faster, where N is the number of model replicas."
  },
  {
    "question": "What is an important consideration for achieving equivalent training results when using Data Parallelism compared to single-model training?",
    "answer": "To get a trained model equivalent to one without Data Parallelism, the user must scale either the learning rate or the desired batch size based on the number of replicas."
  },
  {
    "question": "What is a constraint on model size when using Data Parallelism with multiple GPUs?",
    "answer": "In the multiple-GPU case, each GPU hosts a replica of the model, meaning the model must be small enough to fit inside the memory of a single GPU."
  },
  {
    "question": "Which PyTorch class is recommended by developers for multi-GPU Data Parallelism?",
    "answer": "The `DistributedDataParallel` class is recommended by PyTorch developers for multi-GPU Data Parallelism, applicable for both single and multiple nodes."
  },
  {
    "question": "What is PyTorch Lightning?",
    "answer": "PyTorch Lightning is a Python package that provides interfaces to PyTorch, making common, code-heavy tasks like multi-GPU training more straightforward."
  },
  {
    "question": "What does the `pytorch-ddp-test.sh` script demonstrate?",
    "answer": "The `pytorch-ddp-test.sh` script provides an example of submitting a job using the Python wheel with a virtual environment for multi-GPU Data Parallelism, specifically with several GPUs distributed across two nodes."
  },
  {
    "question": "What is the purpose of the `pytorch-ddp-test.py` script?",
    "answer": "The `pytorch-ddp-test.py` script demonstrates how to implement distributed data parallel training, including initializing a process group, making a neural network model, and preparing data for a CIFAR10 classification task."
  }
]