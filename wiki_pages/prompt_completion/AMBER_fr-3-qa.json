[
  {
    "question": "What applications are offered by CPU modules using AmberTools/20?",
    "answer": "CPU modules using AmberTools/20 offer pmemd (sequential) and pmemd.MPI (parallel) applications."
  },
  {
    "question": "What additional applications are provided by GPU-enabled modules in AmberTools/20?",
    "answer": "GPU-enabled modules in AmberTools/20 add pmemd.cuda (for a single GPU) and pmemd.cuda.MPI (for multiple GPUs)."
  },
  {
    "question": "What is a known limitation of the amber/20.12-20.15 module?",
    "answer": "The amber/20.12-20.15 module does not include the MMPBSA.py.MPI executable."
  },
  {
    "question": "Which Amber modules are problematic for MMPBSA.py PB calculations?",
    "answer": "MMPBSA.py from amber/18-10-18.11 and amber/18.14-18.17 modules cannot perform PB calculations."
  },
  {
    "question": "Which Amber modules should be used for MMPBSA.py PB calculations instead of older versions?",
    "answer": "Users should utilize more recent amber/20 modules for MMPBSA.py PB calculations."
  },
  {
    "question": "Which Amber module is recommended for simulations with a single GPU on Narval?",
    "answer": "For simulations with a single GPU on Narval, it is recommended to use amber/20.12-20.15."
  },
  {
    "question": "Why might older CUDA-compiled modules not work on an A100 GPU?",
    "answer": "Modules compiled with a CUDA version less than 11.4 do not function on an A100 GPU."
  },
  {
    "question": "What are the required SLURM parameters for a single GPU job example?",
    "answer": "An example single GPU job script specifies `--ntasks=1`, `--gpus-per-node=1`, `--mem-per-cpu=2000`, and `--time=10:00:00`."
  },
  {
    "question": "What modules should be loaded for a single GPU job according to the example script?",
    "answer": "For a single GPU job, the example script loads `StdEnv/2023 gcc/12.3 openmpi/4.1.5 cuda/12.6 amber-pmemd/24.3`."
  },
  {
    "question": "What executable is run for a single GPU job in the provided example?",
    "answer": "The `pmemd.cuda` executable is run for a single GPU job in the example script."
  },
  {
    "question": "What are the SLURM resource requests for a parallel MPI CPU job on Narval?",
    "answer": "For a parallel MPI CPU job on Narval, the SLURM resource requests are `--nodes=4`, `--ntasks-per-node=64`, `--mem-per-cpu=2000`, and `--time=1:00:00`."
  },
  {
    "question": "Which modules are loaded for a parallel MPI CPU job on Narval?",
    "answer": "For a parallel MPI CPU job on Narval, the loaded modules are `StdEnv/2023 gcc/12.3 openmpi/4.1.5 cuda/12.6 amber-pmemd/24.3`."
  },
  {
    "question": "What command is used to execute a parallel MPI CPU job on Narval?",
    "answer": "The command `srun pmemd.MPI -O -i input.in -p topol.parm7 -c coord.rst7 -o output.mdout -r restart.rst7` is used for a parallel MPI CPU job on Narval."
  },
  {
    "question": "What are the SLURM resource requests for a parallel MPI CPU job on Rorqual, Fir, Nibi, or Trillium?",
    "answer": "For a parallel MPI CPU job on Rorqual, Fir, Nibi, or Trillium, the SLURM resource requests are `--nodes=1`, `--ntasks-per-node=192`, `--mem-per-cpu=2000`, and `--time=1:00:00`."
  },
  {
    "question": "Which modules are loaded for a parallel MPI CPU job on Rorqual, Fir, Nibi, or Trillium?",
    "answer": "For a parallel MPI CPU job on Rorqual, Fir, Nibi, or Trillium, the loaded modules are `StdEnv/2023 gcc/12.3 openmpi/4.1.5 cuda/12.6 amber-pmemd/24.3`."
  }
]