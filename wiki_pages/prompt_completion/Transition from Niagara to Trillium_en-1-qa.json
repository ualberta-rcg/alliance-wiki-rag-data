[
  {
    "question": "How do users gain access to the Trillium cluster?",
    "answer": "Users must opt-in on CCDB, specifically on the webpage [https://ccdb.alliancecan.ca/me/access_systems](https://ccdb.alliancecan.ca/me/access_systems)."
  },
  {
    "question": "Will users who previously opted into Niagara or Mist automatically get access to Trillium?",
    "answer": "Yes, users who had opted into Niagara/Mist on August 5, 2025, will have automatically been opted-in to Trillium."
  },
  {
    "question": "What does opting into the Trillium cluster provide access to?",
    "answer": "Opting into Trillium gives access to both the CPU and GPU subclusters of Trillium."
  },
  {
    "question": "Is there a separate opt-in required for HPSS on Trillium?",
    "answer": "Yes, to get access to HPSS, the nearline system for Trillium, there is a separate opt-in on [https://ccdb.alliancecan.ca/me/access_systems](https://ccdb.alliancecan.ca/me/access_systems)."
  },
  {
    "question": "If I had HPSS access through Niagara before August 5th, will I be automatically opted-in to Trillium's HPSS?",
    "answer": "Yes, if you had access to HPSS through Niagara before August 5th, you will be opted-in automatically to Trillium's HPSS."
  },
  {
    "question": "Is HPSS fully integrated with Trillium yet?",
    "answer": "For the moment, HPSS has not been integrated into Trillium yet."
  },
  {
    "question": "How do I log in to the CPU subcluster of Trillium via terminal?",
    "answer": "You can log in to the CPU subcluster of Trillium by SSH-ing to `trillium.alliancecan.ca` using the command `$ ssh USERNAME@trillium.alliancecan.ca`."
  },
  {
    "question": "What authentication methods are required for logging into Trillium?",
    "answer": "You will need to use SSH Keys and have MFA enabled on your CCDB account."
  },
  {
    "question": "What type of login nodes are available for the Trillium CPU subcluster?",
    "answer": "Logging into the CPU subcluster will land you on one of six CPU login nodes called `tri-login01-6`. These nodes do not have GPUs and can only submit jobs to CPU compute nodes."
  },
  {
    "question": "How do I log in to the GPU subcluster of Trillium?",
    "answer": "To access the GPU subcluster of Trillium, you should log into `trillium-gpu.alliancecan.ca` using the command `$ ssh USERNAME@trillium-gpu.alliancecan.ca`."
  },
  {
    "question": "What are the specifications of the GPU login node on Trillium?",
    "answer": "The GPU login node, `trig-login01`, has 4 NVIDIA H100 GPUs and allows submission only to the compute nodes of the GPU subcluster."
  },
  {
    "question": "What is the recommended first action after logging into Trillium for the first time?",
    "answer": "It is highly recommended that you run the command `$ trisetup` after logging in for the first time."
  },
  {
    "question": "What files and directories does the `trisetup` command create or populate?",
    "answer": "The `trisetup` command populates `.bashrc`, `.bash_profile`, `.chsrc`, and `.Xauthority` in your home directory. It also creates the directories `.licenses`, `.local`, `.ssh`, and `links`, with the `links` directory containing symbolic links to your scratch and project directories."
  },
  {
    "question": "What actions should users take after running the `trisetup` command?",
    "answer": "After running `trisetup`, users can recompile their code and reinstall virtual environments."
  },
  {
    "question": "What is the current status of the SciNet OnDemand website regarding Trillium access?",
    "answer": "The SciNet OnDemand website will remain connected to Niagara for now, but will eventually also offer a way to log into Trillium and run web-based apps like Jupyter with access to the Trillium file system."
  },
  {
    "question": "What are the hardware specifications of a single compute node in Trillium's CPU subcluster?",
    "answer": "Each compute node of the CPU subcluster has 192 cores, 755 GB of available memory, and uses AMD Zen 5 (Turin) chips."
  },
  {
    "question": "How many compute nodes are in the Trillium CPU subcluster and what is the total core count?",
    "answer": "There are 1224 compute nodes in the Trillium CPU subcluster, for a total of 235,008 cores."
  },
  {
    "question": "What are the suggested alternatives for Intel MKL if I am compiling code on Trillium's CPU subcluster?",
    "answer": "If you were compiling code that used math and linear algebra routines from the Intel MKL, it is suggested to switch to Flexiblas or use the AMD AOCL libraries directly, which are available in the `aocl-blas` and `aocl-lapack` modules."
  },
  {
    "question": "What are the hardware specifications of a single compute node in Trillium's GPU subcluster?",
    "answer": "Each compute node in the GPU subcluster has 96 cores, 755 GB of available memory, 4 NVIDIA H100 GPUs, and uses AMD Zen 4 (Genoa) chips."
  },
  {
    "question": "How many GPU compute nodes are in Trillium, and what is the total number of GPUs?",
    "answer": "There are 61 GPU compute nodes, so in total Trillium has 244 GPUs."
  },
  {
    "question": "What data from Niagara was automatically copied over to Trillium?",
    "answer": "Any data that was present on your home, scratch, and project directories on Niagara on July 31st and that you did not change after that, will have been copied over to the corresponding directory on Trillium."
  },
  {
    "question": "What are the default quotas for home and scratch directories on Trillium?",
    "answer": "For now, the quotas of home and scratch remain the same default values as they were on Niagara, i.e., 100 GB for home and 25 TB for scratch."
  }
]