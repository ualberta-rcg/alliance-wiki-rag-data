[
  {
    "question": "What potential issues might users encounter when running GPU-enabled PyTorch code with `torch >= 1.12.0` compared to `torch < 1.12.0` on clusters equipped with A100, H100, or newer Nvidia GPUs?",
    "answer": "Users may notice a significant slowdown and/or get different results when running the exact same GPU-enabled code with `torch >= 1.12.0` compared to `torch < 1.12.0`."
  },
  {
    "question": "How can TF32 mode be enabled or disabled for matrix multiplications in PyTorch version 1.12.0 and higher?",
    "answer": "To enable or disable TF32 for matrix multiplications on `torch >= 1.12.0`, set the flag `torch.backends.cuda.matmul.allow_tf32` to `True` or `False` accordingly."
  },
  {
    "question": "How can TF32 mode be enabled or disabled for convolutions in PyTorch version 1.12.0 and higher?",
    "answer": "To enable or disable TF32 for convolutions on `torch >= 1.12.0`, set the flag `torch.backends.cudnn.allow_tf32` to `True` or `False` accordingly."
  },
  {
    "question": "Where can one find more detailed information about TF32 on Ampere GPUs in PyTorch?",
    "answer": "More information can be found in PyTorch's official documentation at `https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere`."
  },
  {
    "question": "What are the two ways PyTorch natively supports parallelizing work across multiple CPUs?",
    "answer": "PyTorch natively supports parallelizing work across multiple CPUs through intra-op parallelism and inter-op parallelism."
  },
  {
    "question": "Describe intra-op parallelism in PyTorch for CPU operations.",
    "answer": "Intra-op parallelism refers to PyTorch's parallel implementations of operators commonly used in Deep Learning, such as matrix multiplication and convolution, using OpenMP directly or through low-level libraries like MKL and OneDNN, automatically leveraging multi-threading over available CPU cores."
  },
  {
    "question": "What is inter-op parallelism in PyTorch and what does it typically require?",
    "answer": "Inter-op parallelism refers to PyTorch's ability to execute different parts of your code concurrently. This modality typically requires users to explicitly design their program such that different parts can run in parallel, for example, using PyTorch's Just-In-Time compiler `torch.jit` for asynchronous tasks in a TorchScript program."
  },
  {
    "question": "For small-scale models, what is the recommended approach regarding CPU vs GPU usage?",
    "answer": "With small scale models, it is strongly recommended to use multiple CPUs instead of a GPU."
  },
  {
    "question": "Why is it generally not advisable to use a GPU for small models on HPC clusters, even if training might be faster on a GPU?",
    "answer": "For small models, the speed up relative to CPU will likely not be very significant, and the job will use only a small portion of the GPU's compute capabilities, unnecessarily blocking a resource for other users and consuming the group's allocation."
  }
]