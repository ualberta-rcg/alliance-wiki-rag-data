[
  {
    "question": "How are GPUs scheduled on the Trillium GPU subcluster?",
    "answer": "The scheduler allows users to request either a whole number of nodes or a single GPU on the Trillium GPU subcluster."
  },
  {
    "question": "What resources are allocated when requesting a single GPU on Trillium?",
    "answer": "Requesting a single GPU amounts to a quarter node, with 24 cores and about 188 GiB of RAM."
  },
  {
    "question": "Does Trillium support NVIDIA's MIG technology?",
    "answer": "No, Trillium does not support NVIDIA's MIG (Multi-Instance GPU) technology."
  },
  {
    "question": "How can users share a GPU among processes within a Trillium job?",
    "answer": "Users can utilize Hyper-Q / MPS (Multi-Process Service) within their jobs to share a GPU among processes."
  },
  {
    "question": "Are memory requests honored on Trillium compute nodes?",
    "answer": "Memory requests are ignored on Trillium compute nodes."
  },
  {
    "question": "How much RAM is allocated to CPU jobs on Trillium?",
    "answer": "CPU jobs always receive N \u00d7 768 GB of RAM, where N is the number of nodes requested."
  },
  {
    "question": "How much RAM do GPU full-node jobs receive on Trillium?",
    "answer": "GPU full-node jobs receive N \u00d7 768 GB of RAM, where N is the number of nodes."
  },
  {
    "question": "What is the memory allocation for single-GPU jobs on Trillium?",
    "answer": "Single-GPU jobs receive 1/4 of a node's memory, which is 188 GiB."
  },
  {
    "question": "How should common job options be included in a SLURM job script?",
    "answer": "These options should be put in separate comment lines at the top of the job script (after `#!/bin/bash`), prefixed with `#SBATCH`. They can also be used as command line options for `salloc`."
  },
  {
    "question": "What is the purpose of the `--nodes` option in a SLURM job script?",
    "answer": "The `--nodes` (or `-N`) option specifies the number of nodes requested for a job."
  },
  {
    "question": "Which option is preferred for specifying tasks per node: `--ntasks-per-node` or `--ntasks`?",
    "answer": "`--ntasks-per-node` is preferred over `--ntasks` for launching tasks per node."
  },
  {
    "question": "What does the `--cpus-per-task` option typically specify?",
    "answer": "The `--cpus-per-task` (or `-c`) option typically specifies the number of cores per task, often for OpenMP threads."
  },
  {
    "question": "How do you define the duration of a job using SLURM options?",
    "answer": "The duration of a job is specified using the `--time` (or `-t`) option."
  },
  {
    "question": "What option allows you to name your SLURM job?",
    "answer": "The `--job-name` (or `-J`) option allows you to specify a name for your job."
  },
  {
    "question": "How can you redirect standard output of a job using SLURM options?",
    "answer": "Standard output can be redirected to a file using the `--output` (or `-o`) option, which can also use patterns like `%j` for the job ID."
  },
  {
    "question": "Which option is used to configure email notifications for job status?",
    "answer": "The `--mail-type` option is used to specify when to send email notifications (e.g., BEGIN, END, FAIL, ALL)."
  },
  {
    "question": "What are the allowed values for `--gpus-per-node` on the GPU subcluster?",
    "answer": "On the GPU subcluster, either 1 or 4 GPUs are allowed to be specified with the `--gpus-per-node` option."
  },
  {
    "question": "What is the function of the `--partition` option in SLURM?",
    "answer": "The `--partition` (or `-p`) option specifies the SLURM partition to which the job should be submitted."
  },
  {
    "question": "What is the default partition for CPU compute jobs on Trillium?",
    "answer": "The default partition for CPU compute jobs on Trillium is `compute`."
  },
  {
    "question": "What is the limit on running jobs in the CPU `compute` partition?",
    "answer": "The limit on running jobs in the CPU `compute` partition is 150."
  },
  {
    "question": "What is the minimum job size in the CPU `compute` partition?",
    "answer": "The minimum job size in the CPU `compute` partition is 1 node, which has 192 cores."
  },
  {
    "question": "What is the default maximum job size in the CPU `compute` partition without a specific allocation?",
    "answer": "Without a specific allocation, the default maximum job size in the CPU `compute` partition is 10 nodes (1920 cores)."
  },
  {
    "question": "What is the maximum job size in the CPU `compute` partition if a user has an allocation?",
    "answer": "With an allocation, the maximum job size in the CPU `compute` partition is 128 nodes (24576 cores)."
  },
  {
    "question": "What are the walltime limits for jobs in the CPU `compute` partition?",
    "answer": "Jobs in the CPU `compute` partition have a minimum walltime of 15 minutes and a maximum walltime of 24 hours."
  },
  {
    "question": "What is the limit on running jobs in the CPU `debug` partition?",
    "answer": "The limit on running jobs in the CPU `debug` partition is 1."
  },
  {
    "question": "What is the maximum job size allowed in the CPU `debug` partition?",
    "answer": "The maximum job size allowed in the CPU `debug` partition is 2 nodes (384 cores)."
  },
  {
    "question": "What is the maximum walltime for jobs in the CPU `debug` partition?",
    "answer": "The maximum walltime for jobs in the CPU `debug` partition is 1 hour."
  },
  {
    "question": "What happens if a CPU job underutilizes its allocated cores?",
    "answer": "If a CPU job underutilizes its cores, the support team may reach out to assist with workflow optimization, or the user can contact them for assistance."
  }
]