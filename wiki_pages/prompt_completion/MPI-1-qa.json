[
  {
    "question": "What analogy is used to explain the concept of parallel programming?",
    "answer": "The document uses the analogy of adding more oxen to pull a bigger wagon, and hiring multiple people to build a house as quickly as possible, to illustrate parallel work."
  },
  {
    "question": "Why is dividing up computational problems and assigning work in parallel important?",
    "answer": "This approach is important for speeding up computations and for tackling problems that require large amounts of memory."
  },
  {
    "question": "What is identified as the most significant concept to master in designing parallel applications?",
    "answer": "Communication is the most significant concept to master in designing and building parallel applications."
  },
  {
    "question": "What are the two major models of computational parallelism?",
    "answer": "The two major models of computational parallelism are shared memory and distributed memory."
  },
  {
    "question": "How does shared memory parallelism (SMP) work?",
    "answer": "In shared memory parallelism (SMP), all processors see the same memory image, meaning all memory is globally addressable and all processes can ultimately access it."
  },
  {
    "question": "How is communication handled between processes in a shared memory parallelism environment?",
    "answer": "Communication between processes on an SMP machine is implicit, as any process can read and write values directly to memory that can be accessed and manipulated by others."
  },
  {
    "question": "What is the main challenge when writing programs for shared memory parallelism?",
    "answer": "The main challenge is data consistency, which requires extra care to ensure data is not modified by more than one process at a time."
  },
  {
    "question": "What is distributed memory parallelism equivalent to?",
    "answer": "Distributed memory parallelism is equivalent to a collection of workstations linked by a dedicated network for communication, commonly known as a cluster."
  },
  {
    "question": "How do processes communicate in the distributed memory model?",
    "answer": "In the distributed memory model, processes each have their own private memory and communicate by sending messages, typically by invoking functions to send and receive data."
  },
  {
    "question": "What is a major challenge in distributed memory programming?",
    "answer": "A major challenge in distributed memory programming is how to minimize communication overhead, because networks transmit data significantly slower than within a single machine."
  },
  {
    "question": "How do memory access times compare to network latency?",
    "answer": "Memory access times are typically measured in ones to hundreds of nanoseconds, while network latency is typically expressed in microseconds."
  },
  {
    "question": "What is MPI?",
    "answer": "MPI, or the Message Passing Interface, is a standard describing a set of subroutines, functions, and objects used for writing parallel programs in a distributed memory environment."
  },
  {
    "question": "Can you name some implementations of the MPI standard?",
    "answer": "Some implementations of the MPI standard include Open MPI, Intel MPI, MPICH, and MVAPICH."
  },
  {
    "question": "Which programming languages are officially supported by the MPI standard?",
    "answer": "The MPI standard officially describes how MPI should be called from Fortran, C, and C++ languages."
  },
  {
    "question": "What is the situation regarding C++ bindings in MPI 3.0?",
    "answer": "MPI 3.0 dropped official C++ bindings, but users can still use the C bindings from C++ or Boost MPI."
  },
  {
    "question": "What Python package is used for MPI examples?",
    "answer": "The MPI for Python package, MPI4py, is used for Python examples."
  },
  {
    "question": "What are the advantages of using MPI for parallel programs?",
    "answer": "MPI is an open, non-proprietary standard, allowing programs to be easily ported to many computers, run efficiently on many cores, and simplifying debugging due to local memory for each process."
  },
  {
    "question": "What are the complexities or challenges of designing MPI programs?",
    "answer": "MPI programs may appear more complex due to the need for explicit communication and synchronization management, and designers must minimize communication overhead to achieve good speed-up."
  }
]