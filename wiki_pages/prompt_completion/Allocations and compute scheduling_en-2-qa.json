[
  {
    "question": "What are the three evaluation criteria and their respective weights used to rank different GPU models for Reference GPU Units (RGUs)?",
    "answer": "The evaluation criteria are FP32 score (40%), FP16 score (40%), and GPU memory score (20%)."
  },
  {
    "question": "Which NVidia GPU model is currently used as the reference model for RGU calculations, and what is its assigned RGU value?",
    "answer": "The NVidia A100-40gb GPU is used as the reference model and is assigned an RGU value of 4.0 for historical reasons."
  },
  {
    "question": "How are the FP16 performance, FP32 performance, and memory size of the A100-40gb defined for RGU calculation purposes?",
    "answer": "For the A100-40gb GPU, its FP16 performance, FP32 performance, and memory size are each defined as 1.0."
  },
  {
    "question": "What are the coefficients applied to FP32 score, FP16 score, and Memory score when calculating RGU values for other GPU models, based on the A100-40gb reference?",
    "answer": "The coefficients are 1.6 for FP32 score, 1.6 for FP16 score, and 0.8 for Memory score."
  },
  {
    "question": "What is the combined RGU score for the H100-80gb GPU model?",
    "answer": "The H100-80gb GPU model has a combined RGU score of 12.2."
  },
  {
    "question": "What is the RGU value for the A100-80gb GPU model?",
    "answer": "The A100-80gb GPU model has a combined RGU score of 4.8."
  },
  {
    "question": "What is Multi-Instance GPU (MIG) technology and when was it introduced?",
    "answer": "Multi-Instance GPU (MIG) technology, introduced with the 2025 infrastructure renewal, makes it possible to schedule a fraction of a GPU, allowing different jobs, potentially from different users, to run on the same GPU simultaneously."
  },
  {
    "question": "What is a 'GPU instance' or 'MIG instance' in the context of Multi-Instance GPU technology?",
    "answer": "A GPU instance, also sometimes called a MIG instance, is a fraction of a GPU allocated to a single job."
  },
  {
    "question": "What is the RGU value for an A100-3g.20gb GPU instance?",
    "answer": "An A100-3g.20gb GPU instance has an RGU value of 2.0."
  },
  {
    "question": "What is the RGU value for an H100-3g.40gb GPU instance?",
    "answer": "An H100-3g.40gb GPU instance has an RGU value of 6.1."
  },
  {
    "question": "What does a GPU instance of profile '1g' represent for an A100 or H100 GPU?",
    "answer": "A GPU instance of profile '1g' is worth 1/7 of an A100 or H100 GPU."
  },
  {
    "question": "What is the general principle governing priority calculation for compute-based jobs on national clusters?",
    "answer": "The overarching principle is that compute-based jobs are considered based on the resources that others are prevented from using, not on the resources actually used."
  },
  {
    "question": "How is resource usage calculated if a submitted job requests more cores than it actually uses?",
    "answer": "The usage that will affect the priority of future jobs is the number of cores requested, not the number of cores the application actually used, because the unused cores were unavailable to others."
  },
  {
    "question": "How is resource usage calculated if a job requests significantly more memory than is typically associated with its requested cores?",
    "answer": "If a job requests memory beyond what is associated with the cores requested (e.g., 8GB for one core on a 4GB/core system), the job will be deemed to have used additional cores (e.g., two cores) because other researchers were effectively prevented from using the second core's memory."
  },
  {
    "question": "What constitutes a 'core equivalent' on most clusters?",
    "answer": "On most clusters, a core equivalent is defined as a single core bundled with 4GB of associated memory."
  },
  {
    "question": "How are research groups charged for core equivalents if they use more cores than memory relative to the 1 core/4GB ratio?",
    "answer": "Research groups using more cores than memory (e.g., requesting two cores and 4 GB total memory) will be charged by cores, counted as 2 core equivalents."
  },
  {
    "question": "How are research groups charged for core equivalents if they use more memory than cores relative to the 1 core/4GB ratio?",
    "answer": "Research groups using more memory than the 1 core/4GB ratio (e.g., requesting two cores and 10 GB total memory) will be charged by memory, counted as 2.5 core equivalents."
  },
  {
    "question": "What must a proposal specify during the Resource Allocation Competition (RAC) when asking for GPUs?",
    "answer": "During the RAC, any proposal asking for GPUs must specify the preferred GPU model for the project."
  },
  {
    "question": "How is the amount of Reference GPU Units (RGUs) calculated in the CCDB form for a RAC GPU proposal?",
    "answer": "In the CCDB form, the amount of RGUs is automatically calculated by multiplying the requested amount of gpu-years per year of the project by the RGU value of the specified GPU model."
  },
  {
    "question": "What happens if a RAC GPU allocation needs to be moved to a different cluster?",
    "answer": "If a RAC GPU allocation must be moved to a different cluster, the committee will allocate gpu-years at that cluster in a way that keeps the total amount of RGUs the same."
  },
  {
    "question": "For FP32 operations, how does an A100-40gb GPU compare in performance and resource usage to a P100-12gb GPU?",
    "answer": "For primarily FP32 operations, an A100-40gb GPU is expected to be twice as fast as a P100-12gb GPU, but the recorded usage will be four times the resources. For an equal amount of RGUs, P100-12gb GPUs should allow double the computations."
  },
  {
    "question": "For FP16 operations, how does an A100-40gb GPU compare in computational capacity to a P100-12gb GPU for the same resource evaluation?",
    "answer": "For primarily FP16 operations, an A100-40gb GPU, while evaluated as using four times the resources of a P100-12gb, is capable of computing approximately 30 times the calculations for the same amount of time, allowing completion of about 7.5 times the computations."
  }
]