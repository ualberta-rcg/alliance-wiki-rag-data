[
  {
    "question": "What limits the number of filesystem objects on clusters?",
    "answer": "Filesystem quotas on the clusters limit the number of filesystem objects."
  },
  {
    "question": "What can significantly slow down software on a cluster when dealing with small files?",
    "answer": "Streaming lots of small files from `/project` (or `/scratch`) to a compute node can significantly slow down software."
  },
  {
    "question": "How should data be stored on a distributed filesystem, especially with many small files?",
    "answer": "On a distributed filesystem, data should be stored in large single-file archives."
  },
  {
    "question": "Where can I find more information about handling large collections of files?",
    "answer": "Please refer to the documentation on 'Handling large collections of files'."
  },
  {
    "question": "What strategy is recommended for long computations?",
    "answer": "For long computations, you should use checkpointing. For example, a 3-day training could be split into three 24-hour chunks."
  },
  {
    "question": "What are the advantages of using checkpointing for long computations?",
    "answer": "Checkpointing prevents loss of work in case of an outage and can provide an edge in job priority, as more nodes are available for short jobs."
  },
  {
    "question": "Do machine learning libraries typically support checkpointing?",
    "answer": "Most machine learning libraries natively support checkpointing."
  },
  {
    "question": "Is there a general checkpointing solution available if a program does not natively support it?",
    "answer": "Yes, a general checkpointing solution is provided for programs that do not natively support it."
  },
  {
    "question": "Where can I find examples of checkpointing for PyTorch and TensorFlow?",
    "answer": "Examples can be found under 'Checkpointing with PyTorch' and 'Checkpointing with TensorFlow'."
  },
  {
    "question": "When should I consider grouping many jobs into one?",
    "answer": "You should consider grouping many jobs into one for situations like hyperparameter search, training many variants of the same method, or running many optimization processes of similar duration."
  },
  {
    "question": "What tools are available to help group many similar jobs?",
    "answer": "META, GLOST, and GNU Parallel are available to help with grouping many similar jobs."
  },
  {
    "question": "How can Weights & Biases (wandb) and Comet.ml help with compute allocation?",
    "answer": "They can help by allowing easier tracking and analysis of training runs, and by providing Bayesian hyperparameter search."
  },
  {
    "question": "Are Comet.ml and Weights & Biases (wandb) available on Graham?",
    "answer": "No, Comet and Wandb are not currently available on Graham."
  },
  {
    "question": "Do modern deep learning packages support large-scale training?",
    "answer": "Yes, modern deep learning packages like Pytorch and TensorFlow include utilities to handle large-scale training natively."
  },
  {
    "question": "Is scaling classic machine learning methods widely discussed?",
    "answer": "Scaling classic machine learning (i.e., not deep learning) methods is not as widely discussed and can often be a frustrating problem to solve."
  },
  {
    "question": "Where can I find guidance for tackling large-scale classic machine learning problems?",
    "answer": "A guide on 'Large-Scale Machine Learning (Big Data)' contains ideas, practical options, and tutorials for training classic ML models on very large datasets."
  },
  {
    "question": "What can cause non-deterministic behavior in RNN and multi-head attention API calls with cuDNN?",
    "answer": "RNN and multi-head attention API calls may exhibit non-deterministic behavior when the cuDNN library is built with CUDA Toolkit 10.2 or higher."
  },
  {
    "question": "How can non-deterministic behavior in cuDNN RNN and multi-head attention APIs be eliminated?",
    "answer": "It can be eliminated by setting a single buffer size in the `CUBLAS_WORKSPACE_CONFIG` environmental variable, for example, `:16:8` or `:4096:2`."
  }
]