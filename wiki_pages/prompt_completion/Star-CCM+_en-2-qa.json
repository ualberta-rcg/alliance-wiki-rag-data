[
  {
    "question": "What command generates the machinefile for STAR-CCM+ jobs?",
    "answer": "The command `slurm_hl2hl.py --format STAR-CCM+ > $SLURM_TMPDIR/machinefile` generates the machinefile."
  },
  {
    "question": "How is the total number of cores (`NCORE`) calculated in the STAR-CCM+ Slurm scripts?",
    "answer": "NCORE is calculated as `SLURM_NNODES * SLURM_CPUS_PER_TASK * SLURM_NTASKS_PER_NODE`."
  },
  {
    "question": "What is the primary difference in the `starccm+` command invocation for a Siemens PoD license server compared to an institutional license server?",
    "answer": "For a Siemens PoD license server, the `starccm+` command includes `-power -podkey $LM_PROJECT`, which is omitted for an institutional license server."
  },
  {
    "question": "How do you specify the input simulation file in the STAR-CCM+ Slurm job script examples?",
    "answer": "The input simulation file is specified by setting the `SIM_FILE` variable, for example, `SIM_FILE='mysample.sim'`."
  },
  {
    "question": "How can an input Java filename be included in a STAR-CCM+ job on the cluster?",
    "answer": "An input Java filename can be specified by uncommenting and setting the `JAVA_FILE` variable, for example, `#JAVA_FILE='mymacros.java'`."
  },
  {
    "question": "What is the `LM_PROJECT` variable used for in the provided Slurm scripts?",
    "answer": "`LM_PROJECT` is used to specify your Siemens Power on Demand (PoD) Key."
  },
  {
    "question": "Which `StdEnv` module version is loaded in the example Slurm scripts for Beluga, Cedar, and Graham?",
    "answer": "`module load StdEnv/2023` is loaded in the example Slurm scripts."
  },
  {
    "question": "Which `starccm-mixed` module version is loaded in the example Slurm scripts for Beluga, Cedar, and Graham?",
    "answer": "`module load starccm-mixed/18.06.006` is loaded in the example Slurm scripts."
  },
  {
    "question": "What are the arguments `-jvmargs -Xmx4G -jvmargs -Djava.io.tmpdir=$SLURM_TMPDIR` for in the `starccm+` command?",
    "answer": "They are JVM arguments for setting the maximum heap size to 4GB (`-Xmx4G`) and the temporary directory to `$SLURM_TMPDIR` (`-Djava.io.tmpdir=$SLURM_TMPDIR`)."
  },
  {
    "question": "What is the purpose of the `-batch` option in the `starccm+` command?",
    "answer": "The `-batch` option is used to run STAR-CCM+ in batch mode."
  },
  {
    "question": "What does the `-np $NCORE` option signify in the `starccm+` command?",
    "answer": "The `-np $NCORE` option specifies the number of processes to use, which is set to the calculated total number of cores."
  },
  {
    "question": "What is the function of `-nbuserdir $SLURM_TMPDIR` in the `starccm+` command?",
    "answer": "It sets the user directory for STAR-CCM+ to `$SLURM_TMPDIR`."
  },
  {
    "question": "What is the `starccm+` command option `-machinefile $SLURM_TMPDIR/machinefile` used for?",
    "answer": "It specifies the machinefile to be used by STAR-CCM+, located at `$SLURM_TMPDIR/machinefile`."
  },
  {
    "question": "What MPI implementation and fabric are specified for STAR-CCM+ jobs on Cedar and Graham clusters?",
    "answer": "On Cedar and Graham, the jobs specify `-mpi intel -fabric psm2`."
  },
  {
    "question": "What is the recommended number of CPUs per task (`--cpus-per-task`) for Beluga?",
    "answer": "For Beluga, the recommended `--cpus-per-task` is 40."
  },
  {
    "question": "What are the recommended values for `cpus-per-task` on the Cedar cluster?",
    "answer": "On Cedar, the recommended values for `cpus-per-task` are 32 or 48."
  },
  {
    "question": "What are the recommended values for `cpus-per-task` on the Graham cluster?",
    "answer": "On Graham, the recommended values for `cpus-per-task` are 32 or 44."
  },
  {
    "question": "What Slurm setting ensures all memory per node is requested for STAR-CCM+ jobs?",
    "answer": "The `#SBATCH --mem=0` setting requests all memory per node."
  },
  {
    "question": "What Slurm setting ensures that only one task runs per node for STAR-CCM+ jobs?",
    "answer": "The `#SBATCH --ntasks-per-node=1` setting ensures one task per node."
  },
  {
    "question": "How do you specify the account for Slurm jobs?",
    "answer": "The account is specified using `#SBATCH --account=def-group`."
  }
]