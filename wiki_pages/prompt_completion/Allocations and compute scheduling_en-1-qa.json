[
  {
    "question": "What is an allocation in high-performance computing?",
    "answer": "An allocation is an amount of resources that a research group can target for use for a period of time, usually a year."
  },
  {
    "question": "How is the amount of resources determined for an allocation?",
    "answer": "The amount is either a maximum amount, as is the case for storage, or an average amount of usage over the period, as is the case for shared resources like computation cores."
  },
  {
    "question": "In what terms are high-performance computing allocations usually made?",
    "answer": "Allocations are usually made in terms of core years, GPU years, or storage space."
  },
  {
    "question": "What is the difference in understanding between storage allocations and core year or GPU year allocations?",
    "answer": "Storage allocations are maximum amounts for exclusive use, while core year and GPU year allocations capture average use over time on shared resources."
  },
  {
    "question": "If a cluster experiences downtime for maintenance, does a research group receive an extension on their resource usage allocation?",
    "answer": "No, a research group would not be entitled to an additional week of resource usage if the clusters were down for maintenance, as the allocation period is a reference value."
  },
  {
    "question": "How can a research group maximize their chances of hitting or exceeding their core year or GPU year allocation targets?",
    "answer": "A research group is more likely to hit (or exceed) its target(s) if the resources are used evenly over the allocation period than if the resources are used in bursts or if use is put off until later in the allocation period."
  },
  {
    "question": "What is a 'job' in the context of compute allocations?",
    "answer": "A job is a combination of a computer program (an application) and a list of resources that the application is expected to use."
  },
  {
    "question": "What is the primary function of a scheduler in high-performance computing?",
    "answer": "The scheduler is a program that calculates the priority of each job submitted and provides the needed resources based on the priority of each job and the available resources."
  },
  {
    "question": "How does the scheduler determine job priority?",
    "answer": "The scheduler uses prioritization algorithms based on a research group\u2019s recent usage of the system as compared to their allocated usage on that system, with most weight on recent usage."
  },
  {
    "question": "What is the goal of the scheduler's prioritization strategy?",
    "answer": "The point of this is to allow a research group that matches their actual usage with their allocated amounts to operate roughly continuously at that level, smoothing resource usage over time across all groups and resources."
  },
  {
    "question": "What is the main consequence of overusing a CPU or GPU allocation?",
    "answer": "The only consequence is that succeeding jobs of yours may have lower priority for a time while the scheduler prioritizes other groups which were below their target."
  },
  {
    "question": "Does overusing a CPU or GPU allocation prevent a user from submitting new jobs?",
    "answer": "No, you are not prevented from submitting or running new jobs, and the average of your usage over time should still be close to your target."
  },
  {
    "question": "When were Reference GPU Units (RGUs) introduced and for what purpose?",
    "answer": "Reference GPU Units (RGUs) were introduced in the 2024 RAC year to rank all GPU models in production and alleviate problems caused by treating all GPUs as equivalent for allocation purposes, due to dramatically increasing GPU performance."
  },
  {
    "question": "What new complexity was introduced for RGUs in the 2025 RAC year?",
    "answer": "Since the 2025 RAC year, new complexity involving multi-instance GPU technology has to be dealt with."
  },
  {
    "question": "What evaluation criteria and weights are used to rank GPU models for RGU calculation?",
    "answer": "The evaluation criteria are FP32 score (40%), FP16 score (40%), and GPU memory score (20%)."
  }
]