[
  {
    "question": "What is the primary focus of this tutorial?",
    "answer": "This tutorial presents the highly parallel computing component that is the graphics processing unit (GPU), the parallel programming language CUDA, and some of the CUDA numerical libraries used in high-performance computing."
  },
  {
    "question": "What programming languages are prerequisites for this CUDA tutorial?",
    "answer": "A good knowledge of C or C++ will allow you to get the most out of it."
  },
  {
    "question": "What is the specific term used for CUDA when programming with C/C++?",
    "answer": "We will limit ourselves here to CUDA for C/C++ and use the term CUDA C."
  },
  {
    "question": "What is the essential goal of CUDA C?",
    "answer": "It is essentially about producing C/C++ functions that can be executed by CPUs and GPUs."
  },
  {
    "question": "What are the learning objectives of this tutorial?",
    "answer": "Understand GPU architecture, understand the flow of a CUDA program, understand and manage different types of GPU memories, write and compile a CUDA code example."
  },
  {
    "question": "What does GPU stand for and what is its primary function?",
    "answer": "GPU (for 'graphics processing unit') is a single-chip processor capable of performing mathematical calculations quickly to produce image renderings."
  },
  {
    "question": "Besides image rendering, what else is the power of a GPU used for?",
    "answer": "Since a few years, the power of the GPU is also used to accelerate the execution of intensive calculations in several areas of cutting-edge scientific research."
  },
  {
    "question": "What does CUDA stand for and what is its definition?",
    "answer": "CUDA ('compute unified device architecture') is a software environment and a scalable programming model for processing intensive parallel calculations on GPUs."
  },
  {
    "question": "What are the main components of a GPU's architecture?",
    "answer": "A GPU is composed of a global memory and streaming multiprocessors (SM for 'streaming multiprocessors')."
  },
  {
    "question": "Describe the global memory of a GPU.",
    "answer": "It is similar to CPU memory and accessible by both CPU and GPU."
  },
  {
    "question": "What are streaming multiprocessors (SMs) composed of?",
    "answer": "Each SM is composed of several streaming processors (SP for 'streaming processors')."
  },
  {
    "question": "What is the role of streaming processors (SPs) within an SM?",
    "answer": "They perform the calculations."
  },
  {
    "question": "What are the characteristics of each Streaming Multiprocessor (SM)?",
    "answer": "Each SM is equipped with its own control unit, registers, execution pipelines, etc."
  },
  {
    "question": "In the CUDA programming model, what does 'H\u00f4te' (Host) refer to?",
    "answer": "'H\u00f4te' designates the CPU and its memory (host memory)."
  },
  {
    "question": "In the CUDA programming model, what does 'Carte graphique' (Graphics card) refer to?",
    "answer": "'Carte graphique' designates the GPU and its memory (graphics card memory)."
  },
  {
    "question": "What type of model is the CUDA model?",
    "answer": "The CUDA model is a heterogeneous model where both the CPU and the GPU are used."
  },
  {
    "question": "What types of memory can CUDA code manage?",
    "answer": "The CUDA code can manage both types of memories: host memory and graphics card memory."
  },
  {
    "question": "What are 'kernels' in the context of CUDA?",
    "answer": "The code also executes GPU functions called 'kernels' (noyaux)."
  },
  {
    "question": "How are kernel functions executed?",
    "answer": "These functions are executed in parallel by several GPU threads."
  },
  {
    "question": "What are the five steps of a CUDA program process?",
    "answer": "1. Declaration and allocation of host and graphics card memory. 2. Initialization of host memory. 3. Transfer of data from host memory to graphics card memory. 4. Execution of GPU functions ('kernels'). 5. Return of data to host memory."
  },
  {
    "question": "What is a 'kernel' in the CUDA execution model?",
    "answer": "The simple CUDA code executed in a GPU is called 'kernel'."
  },
  {
    "question": "How does a GPU core execute a kernel?",
    "answer": "Each GPU core (streaming processor) executes a sequential thread, which is the smallest discrete set of instructions managed by the operating system's scheduler."
  },
  {
    "question": "How do all GPU cores execute the kernel simultaneously?",
    "answer": "All GPU cores execute the 'kernel' simultaneously according to the SIMT ('single instruction, multiple threads') model."
  },
  {
    "question": "What is the recommended procedure for CUDA execution?",
    "answer": "1. Copy input data from CPU memory to GPU memory. 2. Load and launch the GPU program (the 'kernel'). 3. Copy results from GPU memory to CPU memory."
  },
  {
    "question": "How are threads organized in CUDA to achieve intensive parallelism?",
    "answer": "With CUDA, threads are grouped into blocks of threads, which themselves form a grid."
  },
  {
    "question": "How do threads within the same block cooperate?",
    "answer": "The grouped threads cooperate via shared memory."
  },
  {
    "question": "Do threads from different blocks cooperate?",
    "answer": "The threads of one block do not cooperate with the threads of the other blocks."
  },
  {
    "question": "How do threads within a block work together?",
    "answer": "The threads in a block work on the same group of instructions (but perhaps with different data sets) and exchange data via shared memory."
  },
  {
    "question": "How do threads use IDs in CUDA?",
    "answer": "Each thread uses identifiers (IDs) to decide which data to use."
  },
  {
    "question": "What are the types of IDs used by threads in CUDA?",
    "answer": "IDs des blocs : 1D ou 2D (blockIdx.x, blockIdx.y) et IDs des fils : 1D, 2D, ou 3D (threadIdx.x, threadIdx.y, threadIdx.z)."
  },
  {
    "question": "What does the CUDA ID model simplify?",
    "answer": "This model simplifies memory addressing when processing multidimensional data."
  },
  {
    "question": "How does a streaming multiprocessor (SM) typically execute blocks of threads?",
    "answer": "Un processeur en continu (SM) ex\u00e9cute habituellement un bloc de fils \u00e0 la fois."
  },
  {
    "question": "What are 'warps' in CUDA?",
    "answer": "The code is executed in groups of 32 threads (called 'warps')."
  },
  {
    "question": "Can a block of threads, once assigned to an SM, be executed without interruption?",
    "answer": "No, when an SM receives the block assigned to it, this does not mean that this particular block will be executed without interruption. In fact, the scheduler can delay/suspend the execution of such blocks under certain conditions, for example, if the data is no longer available."
  },
  {
    "question": "What happens if an SM delays a block's execution?",
    "answer": "When this occurs, the scheduler executes another block of threads that is ready to be executed."
  },
  {
    "question": "What is 'zero-overhead' scheduling?",
    "answer": "It is a kind of 'zero-overhead' scheduling that favors a more regular execution flow so that the SMs do not remain inactive."
  },
  {
    "question": "What are the different types of memory available for CUDA operations?",
    "answer": "Global memory, shared memory, registers and local memory, constant memory."
  },
  {
    "question": "Describe global memory in CUDA.",
    "answer": "Global memory is non on-chip ('off-chip'), efficient for I/O operations, but relatively slow."
  },
  {
    "question": "Describe shared memory in CUDA.",
    "answer": "Shared memory is on-chip ('on-chip'), allows good thread collaboration, very fast."
  },
  {
    "question": "Describe registers and local memory in CUDA.",
    "answer": "Registers and local memory are the workspace for threads, very fast."
  },
  {
    "question": "What CUDA function is used to allocate memory on the graphics card?",
    "answer": "cudaMalloc((void**)&array, size)."
  },
  {
    "question": "What parameters does `cudaMalloc` require?",
    "answer": "It requires the address of a pointer to the allocated data and its size."
  },
  {
    "question": "What CUDA function is used to deallocate an object from memory?",
    "answer": "cudaFree(array)."
  },
  {
    "question": "What parameter does `cudaFree` require?",
    "answer": "It requires only the pointer to the data."
  },
  {
    "question": "What CUDA function is used to copy data between the graphics card and the host?",
    "answer": "cudaMemcpy(array_dest, array_orig, size, direction)."
  },
  {
    "question": "What parameters does `cudaMemcpy` require?",
    "answer": "It requires the pointers to the data, the size, and the type of direction (cudaMemcpyHostToDevice, cudaMemcpyDeviceToHost, cudaMemcpyDeviceToDevice, etc.)."
  },
  {
    "question": "What is `cudaMemcpyAsync` and how does it differ from `cudaMemcpy`?",
    "answer": "cudaMemcpyAsync is identical to cudaMemcpy, but transfers data asynchronously, which means that the execution of other processes is not blocked."
  },
  {
    "question": "How is a kernel function declared in CUDA C?",
    "answer": "With the `___global__` keyword, as seen in `___global__ void add (int *a, int *b, int *c)`."
  },
  {
    "question": "In the simple CUDA C example, how are variables `a`, `b`, and `c` allocated on the device?",
    "answer": "Using `cudaMalloc ( (void**) &dev_a, size);`, `cudaMalloc ( (void**) &dev_b, size);`, and `cudaMalloc ( (void**) &dev_c, size);`."
  },
  {
    "question": "How are input values copied from the host to the device in the CUDA example?",
    "answer": "Using `cudaMemcpy (dev_a, &a, size, cudaMemcpyHostToDevice);` and `cudaMemcpy (dev_b, &b, size, cudaMemcpyHostToDevice);`."
  }
]