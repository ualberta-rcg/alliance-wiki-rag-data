[
  {
    "question": "What is the primary purpose of the PROJECT file system?",
    "answer": "The PROJECT file system is designed for sharing data among group members and for storing large amounts of data."
  },
  {
    "question": "How are quotas handled for the PROJECT storage space?",
    "answer": "The PROJECT storage space offers large quotas that are adjustable per project."
  },
  {
    "question": "Does the PROJECT file system include automatic backups?",
    "answer": "Yes, the PROJECT file system has an automatic backup once per day."
  },
  {
    "question": "Which specific address should be used for data transfers via Globus?",
    "answer": "For data transfers via Globus, you should use the 'Point de chute Globus' (Globus Drop Point)."
  },
  {
    "question": "Which address is recommended for data transfer tools like rsync and scp?",
    "answer": "For data transfer tools such as rsync and scp, you should use the address of the 'N\u0153ud de copie' (Copy Node)."
  },
  {
    "question": "What type of high-performance network connects all nodes in the cluster?",
    "answer": "The cluster uses an InfiniBand NDR (Nvidia) high-performance network to connect all its nodes."
  },
  {
    "question": "How are the H100 GPUs connected to the InfiniBand network?",
    "answer": "Each H100 GPU is connected to a NDR200 port via an Nvidia ConnectX-7 card. Each server has 4 NDR200 ports connected to the InfiniBand fabric."
  },
  {
    "question": "What is the topology of the InfiniBand network?",
    "answer": "The InfiniBand network is non-blocking for compute servers and consists of 2 stages of switches arranged in a 'fat-tree' topology."
  },
  {
    "question": "How many compute nodes are equipped with GPUs, and what are their specifications?",
    "answer": "There are 42 compute nodes with GPUs. Each has 48 cores, 512GB of memory, 2 x Intel Xeon Gold 6442Y 2.6 GHz (24C) CPUs, 1 x 7.68TB SSD, and 4 x NVIDIA HGX H100 SXM 80GB HBM3 700W GPUs connected via NVLink."
  },
  {
    "question": "What are the specifications of the nodes without GPUs?",
    "answer": "There are 4 nodes without GPUs, each having 64 cores, 512GB of memory, 2 x Intel Xeon Gold 6438M 2.2G (32C/64T) CPUs, and 1 x 7.68TB SSD for storage."
  },
  {
    "question": "What is the default software environment available on tamIA?",
    "answer": "The default software environment on tamIA is `StdEnv/2023`."
  },
  {
    "question": "How can users track their computational tasks on tamIA?",
    "answer": "Users can track their GPU and CPU tasks in real-time or view past tasks through the tamIA portal at https://portail.tamia.ecpia.ca/."
  },
  {
    "question": "What metrics can be monitored for a task on the tamIA portal?",
    "answer": "On the tamIA portal, you can visualize the utilization of compute cores, memory used, and GPU usage for a task."
  },
  {
    "question": "Why is it important to adjust resource requests for tasks based on their actual usage?",
    "answer": "It is important to adjust resource requests to maximize the utilization of allocated resources, decrease waiting times, and rectify demands when compute resources are underutilized."
  }
]