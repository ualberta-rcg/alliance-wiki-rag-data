[
  {
    "question": "How often is automatic backup performed for the PROJECT storage?",
    "answer": "Automatic backup for the PROJECT storage is performed once a day."
  },
  {
    "question": "Which address should be used for data transfers via Globus?",
    "answer": "For Globus data transfers, the 'Point de chute Globus' (Globus Drop Point) should be used."
  },
  {
    "question": "Which address should be used for data transfers with tools like rsync or scp?",
    "answer": "For tools like rsync and scp, the 'N\u0153ud de copie' (copy node) address should be used."
  },
  {
    "question": "What type of high-performance network connects all nodes of the tamIA cluster?",
    "answer": "The InfiniBand NDR network from Nvidia connects all nodes of the cluster."
  },
  {
    "question": "How are H100 GPUs connected to the InfiniBand network?",
    "answer": "Each H100 GPU is connected to an NDR200 port via an Nvidia ConnectX-7 card."
  },
  {
    "question": "How many NDR200 ports does each server have connected to the InfiniBand fabric?",
    "answer": "Each server has 4 NDR200 ports connected to the InfiniBand fabric."
  },
  {
    "question": "What is the topology of the InfiniBand network?",
    "answer": "The InfiniBand network is composed of 2 layers of switches arranged in a 'fat-tree' topology."
  },
  {
    "question": "How are storage and management nodes connected to the core of the InfiniBand network?",
    "answer": "Storage and management nodes are connected to the core of the network via 4 connections at 400Gb/s."
  },
  {
    "question": "How many nodes in the tamIA cluster are equipped with GPUs?",
    "answer": "There are 42 nodes equipped with GPUs."
  },
  {
    "question": "What are the CPU specifications for the 42 GPU-enabled nodes?",
    "answer": "The 42 GPU-enabled nodes use 2 x Intel Xeon Gold 6442Y 2.6 GHz, 24C CPUs."
  },
  {
    "question": "How much available memory do the GPU-enabled nodes have?",
    "answer": "The GPU-enabled nodes have 512GB of available memory."
  },
  {
    "question": "What storage is included in the 42 GPU-enabled nodes?",
    "answer": "Each of the 42 GPU-enabled nodes has 1 x 7.68TB SSD."
  },
  {
    "question": "What GPUs are installed in the 42 nodes?",
    "answer": "The 42 nodes have 4 x NVIDIA HGX H100 SXM 80GB HBM3 700W GPUs, connected via NVLink."
  },
  {
    "question": "How many nodes in the cluster do not have GPUs?",
    "answer": "There are 4 nodes that do not have GPUs."
  },
  {
    "question": "What are the CPU specifications for the 4 nodes without GPUs?",
    "answer": "The 4 nodes without GPUs use 2 x Intel Xeon Gold 6438M 2.2G, 32C/64T CPUs."
  },
  {
    "question": "What is the available memory on the nodes without GPUs?",
    "answer": "The nodes without GPUs have 512GB of available memory."
  },
  {
    "question": "What is the default software environment on tamIA?",
    "answer": "The default software environment on tamIA is StdEnv/2023."
  },
  {
    "question": "Where can users monitor their computing tasks on tamIA?",
    "answer": "Users can monitor their computing tasks (GPU and CPU) in real-time or past tasks from the tamIA portal at https://portail.tamia.ecpia.ca/."
  },
  {
    "question": "What information can be visualized for a task on the tamIA portal?",
    "answer": "For a task, users can visualize the utilization of compute cores, used memory, and GPU usage."
  },
  {
    "question": "What should a user do if they are not fully utilizing the allocated compute resources?",
    "answer": "Users should adjust their submission file accordingly if they are not fully utilizing the allocated resources."
  }
]