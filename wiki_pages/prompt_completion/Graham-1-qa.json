[
  {
    "question": "What is the future plan for the Graham system?",
    "answer": "Graham will soon be retired and replaced by a new system called Nibi."
  },
  {
    "question": "When did the Graham system become available?",
    "answer": "The Graham system has been in production since June 2017."
  },
  {
    "question": "What is the login node for the Graham cluster?",
    "answer": "The login node for Graham is graham.alliancecan.ca."
  },
  {
    "question": "What is the Globus collection name for Graham?",
    "answer": "The Globus collection name for Graham is computecanada#graham-globus."
  },
  {
    "question": "How can users transfer data to or from Graham using tools like rsync or scp?",
    "answer": "Users should utilize robot or login nodes for data transfer methods such as rsync, scp, or sftp."
  },
  {
    "question": "Where is the Graham cluster located?",
    "answer": "The Graham cluster is located at the University of Waterloo."
  },
  {
    "question": "Who is the Graham cluster named after?",
    "answer": "The Graham cluster is named after Wes Graham, the first director of the Computing Centre at Waterloo."
  },
  {
    "question": "What type of cooling system does Graham use?",
    "answer": "Graham is entirely liquid cooled, using rear-door heat exchangers."
  },
  {
    "question": "Can Graham's compute nodes access the internet by default?",
    "answer": "No, by policy, Graham's compute nodes cannot access the internet, but exceptions can be requested from technical support."
  },
  {
    "question": "Is Crontab offered on Graham?",
    "answer": "No, Crontab is not offered on Graham."
  },
  {
    "question": "What are the allowed job durations on Graham?",
    "answer": "Each job on Graham should have a duration of at least one hour (five minutes for test jobs) and no more than 168 hours (seven days)."
  },
  {
    "question": "What is the maximum number of jobs a user can have running and queued on Graham?",
    "answer": "A user cannot have more than 1000 jobs, running and queued, at any given moment, with an array job counting as the number of tasks in the array."
  },
  {
    "question": "What is the total volume of Home space on Graham?",
    "answer": "The total volume for Home space on Graham is 133TB."
  },
  {
    "question": "Is Home space on Graham allocated through RAS or RAC?",
    "answer": "Home space is not allocated via RAS or RAC; larger requests should go to Project space."
  },
  {
    "question": "Does Home space on Graham have daily backups?",
    "answer": "Yes, Home space on Graham has daily backup."
  },
  {
    "question": "What is the total volume of Scratch space on Graham?",
    "answer": "The total volume for Scratch space on Graham is 3.2PB."
  },
  {
    "question": "What is Scratch space on Graham used for?",
    "answer": "Scratch space is for active or temporary storage and functions as a parallel high-performance filesystem."
  },
  {
    "question": "What happens to inactive data in Scratch space on Graham?",
    "answer": "Inactive data in Scratch space will be purged."
  },
  {
    "question": "What is the total volume of Project space on Graham?",
    "answer": "The total volume for Project space on Graham is 16PB."
  },
  {
    "question": "How is Project space allocated on Graham?",
    "answer": "Project space is allocated via RAS (Rapid Access Service) or RAC (Resource Allocation Competition)."
  },
  {
    "question": "Is Project space suitable for parallel I/O workloads?",
    "answer": "No, Project space is not designed for parallel I/O workloads; Scratch space should be used instead."
  },
  {
    "question": "What types of InfiniBand interconnects are used on Graham?",
    "answer": "Graham uses Mellanox FDR (56Gb/s) and EDR (100Gb/s) InfiniBand interconnects."
  },
  {
    "question": "Which InfiniBand interconnect is used for GPU and cloud nodes on Graham?",
    "answer": "FDR (56Gb/s) InfiniBand is used for GPU and cloud nodes."
  },
  {
    "question": "What connects all nodes and scratch storage on Graham?",
    "answer": "A low-latency high-bandwidth Infiniband fabric connects all nodes and scratch storage."
  },
  {
    "question": "What network features do cloud-provisionable nodes on Graham have?",
    "answer": "Nodes configurable for cloud provisioning have a 10Gb/s Ethernet network with 40Gb/s uplinks to scratch storage."
  },
  {
    "question": "What is the maximum number of cores Graham is designed to support for simultaneous parallel jobs in a non-blocking manner?",
    "answer": "The design of Graham is to support multiple simultaneous parallel jobs of up to 1024 cores in a fully non-blocking manner."
  },
  {
    "question": "What is the address for Graham's dedicated visualization nodes?",
    "answer": "Graham's dedicated visualization nodes are available at gra-vdi.alliancecan.ca."
  },
  {
    "question": "What type of connections are allowed to Graham's visualization nodes?",
    "answer": "Only VNC connections are allowed to Graham's dedicated visualization nodes."
  },
  {
    "question": "When was Graham's capacity reduced, and why?",
    "answer": "Graham's capacity was reduced in early 2025 to make space for the installation of the new Nibi cluster."
  },
  {
    "question": "Is Intel Turbo Boost enabled on Graham nodes?",
    "answer": "Yes, Turbo Boost is enabled for all Graham nodes."
  },
  {
    "question": "What is the best practice for local on-node storage on Graham?",
    "answer": "The best practice for local on-node storage is to use the temporary directory generated by Slurm, `$SLURM_TMPDIR`."
  },
  {
    "question": "Which Tesla GPU generations are available on Graham?",
    "answer": "Graham contains V100 Volta, T4 Turing, and A100 Ampere Tesla GPUs."
  },
  {
    "question": "Which GPU type has been decommissioned from Graham?",
    "answer": "P100 GPUs have been decommissioned."
  },
  {
    "question": "How many Volta GPU nodes does Graham have?",
    "answer": "Graham has a total of 2 Volta nodes."
  },
  {
    "question": "What is the maximum job duration for Volta GPU nodes on Graham?",
    "answer": "The maximum job duration for Volta GPU nodes is seven days."
  },
  {
    "question": "What CPU to GPU ratio should be maintained on 28-core Volta nodes?",
    "answer": "On 28-core Volta nodes, you should scale the number of CPUs requested, keeping the ratio of CPUs to GPUs at 3.5 or less."
  },
  {
    "question": "How many CPU cores should be requested for a job using 4 GPUs on a 28-core Volta node?",
    "answer": "For a job using 4 GPUs on a 28-core Volta node, you should request at most 14 CPU cores."
  },
  {
    "question": "What is the recommended CPU cores per GPU ratio for the newest 40-core Volta nodes with NVLINK?",
    "answer": "For the newest 40-core Volta nodes, you can use 5 CPU cores per GPU."
  },
  {
    "question": "How can a user request a specific NVLINK-enabled Volta node on Graham?",
    "answer": "To request an NVLINK Volta node, the `--constraint=cascade,v100` parameter should be added to the job submission script."
  },
  {
    "question": "How should users utilize the fast local disk on Volta nodes for I/O intensive jobs?",
    "answer": "Users should utilize the `$SLURM_TMPDIR` environment variable, which points to the temporary directory on the fast local disk, copying input files there at the start and output files out at the end of the job."
  },
  {
    "question": "What parameter should be used to request two T4 Turing GPUs per node?",
    "answer": "To request two T4 Turing GPUs per node, you should specify `--gres=gpu:t4:2`."
  },
  {
    "question": "What parameter should be used to request two A100 Ampere GPUs per node?",
    "answer": "To request two A100 Ampere GPUs per node, you should specify `--gres=gpu:a100:2`."
  },
  {
    "question": "What parameter should be used to request two RTX A5000 Ampere GPUs per node?",
    "answer": "To request two RTX A5000 Ampere GPUs per node, you should specify `--gres=gpu:a5000:2`."
  },
  {
    "question": "What percentage of its capacity is the Graham cluster operating at from January 13, 2025?",
    "answer": "From January 13, 2025, the Graham cluster will operate at approximately 25% capacity."
  }
]