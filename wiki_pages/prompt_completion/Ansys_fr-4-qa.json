[
  {
    "question": "After a Fluent job is completed, how can a user access the results and bring them back into Fluent?",
    "answer": "When the job is finished, users can download the data file and import it back into Fluent using `File > Import > Data\u2026`."
  },
  {
    "question": "Which type of Fluent Slurm script is generally recommended for minimizing wait times and maximizing performance?",
    "answer": "Most Fluent tasks should use the 'par n\u0153ud' (per node) script to minimize wait time and maximize performance by using the fewest possible nodes."
  },
  {
    "question": "What is a potential trade-off when using the 'par c\u0153ur' (per core) script for Fluent tasks requiring many CPU cores?",
    "answer": "While the 'par c\u0153ur' script might reduce queue time for tasks demanding many CPU cores, starting a multi-node task can take significantly longer, which diminishes its overall benefit."
  },
  {
    "question": "What risk is associated with running intensive Fluent tasks across a large, indeterminate number of nodes?",
    "answer": "Running intensive tasks on a very large, indeterminate number of nodes makes them much more susceptible to crashing if one of the compute nodes fails during the simulation."
  },
  {
    "question": "How do the Fluent Slurm scripts handle memory for single-node versus multi-node tasks?",
    "answer": "The Fluent Slurm scripts use shared memory for single-node tasks and distributed memory (with MPI and the appropriate CHP interconnect) for multi-node tasks."
  },
  {
    "question": "What are the suggested alternatives if Fluent crashes during automatic mesh partitioning on Narval when using standard Intel scripts?",
    "answer": "If Fluent crashes during automatic mesh partitioning on Narval with standard Intel scripts, an alternative is to use the OpenMPI scripts, or perform manual mesh partitioning in the Fluent graphical interface and then re-run the task on the cluster with Intel scripts."
  },
  {
    "question": "What are the requirements for mesh partitioning to achieve optimal efficiency in Fluent?",
    "answer": "For optimal mesh partitioning efficiency, the number of mesh partitions must be an integer multiple of the number of cores, and there should be at least 10,000 cells per core."
  },
  {
    "question": "What is the function of the `#SBATCH --mem=0` directive in the `script-flu-bynode-intel.sh` Slurm script?",
    "answer": "The `#SBATCH --mem=0` directive in the `script-flu-bynode-intel.sh` Slurm script ensures that all available memory per compute node is allocated."
  },
  {
    "question": "Which standard environment and Ansys module versions are loaded in the `script-flu-bynode-intel.sh` example?",
    "answer": "The `script-flu-bynode-intel.sh` example loads `StdEnv/2023` and `ansys/2023R2` (or newer versions)."
  },
  {
    "question": "How is the total number of cores (`NCORES`) determined in the `script-flu-bynode-intel.sh` Slurm script?",
    "answer": "The total number of cores (`NCORES`) is calculated as `SLURM_NNODES * SLURM_NTASKS_PER_NODE * SLURM_CPUS_PER_TASK`."
  },
  {
    "question": "What Fluent command is used for single-node tasks in the `script-flu-bynode-intel.sh` script?",
    "answer": "For single-node tasks, the command is `fluent -g $MYVERSION -t $NCORES -affinity=0 -mpi=intel -pshmem -i $MYJOURNALFILE`, with `export I_MPI_HYDRA_BOOTSTRAP=ssh` optionally uncommented on Beluga or Cedar."
  },
  {
    "question": "What Fluent command is used for multi-node tasks in the `script-flu-bynode-intel.sh` script?",
    "answer": "For multi-node tasks, the command is `fluent -g $MYVERSION -t $NCORES -affinity=0 -mpi=intel -pib -cnf=/tmp/machinefile-$SLURM_JOB_ID -i $MYJOURNALFILE`."
  },
  {
    "question": "What is the maximum number of compute nodes that can be specified for Fluent jobs on Narval using the `script-flu-bynode-intel.sh` script?",
    "answer": "The `script-flu-bynode-intel.sh` script specifies a maximum of 1 compute node (`#SBATCH --nodes=1`) for Narval."
  },
  {
    "question": "What are the recommended `ntasks-per-node` values for Graham, Cedar, Beluga, and Narval in the `script-flu-bynode-intel.sh` script?",
    "answer": "The `script-flu-bynode-intel.sh` script recommends `ntasks-per-node` values of 32 for Graham, 48 for Cedar, 40 for Beluga, and 64 for Narval, or fewer."
  },
  {
    "question": "What user-defined variables need to be set in the `script-flu-bynode-intel.sh` submission script?",
    "answer": "Users need to specify `MYJOURNALFILE` (journal file name) and `MYVERSION` (2d, 2ddp, 3d, or 3ddp) in the submission script."
  }
]