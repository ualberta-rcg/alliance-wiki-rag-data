[
  {
    "question": "Quelle est la seule grappe qui offre des GPU Ampere en octobre 2022?",
    "answer": "En date d'octobre 2022, la seule grappe qui offre des GPU Ampere est Narval."
  },
  {
    "question": "Quels effets peut-on observer lors de l'utilisation de PyTorch sur Narval avec des versions de `torch` diff\u00e9rentes (ant\u00e9rieures et sup\u00e9rieures \u00e0 1.12.0)?",
    "answer": "Lors de l'utilisation de PyTorch sur Narval, vous pourriez remarquer un fort ralentissement ou obtenir des r\u00e9sultats diff\u00e9rents dans l'ex\u00e9cution sur GPU du m\u00eame code avec `torch < 1.12.0` et `torch >= 1.12.0`."
  },
  {
    "question": "Comment peut-on activer ou d\u00e9sactiver le mode TF32 pour les multiplications matricielles avec `torch >= 1.12.0`?",
    "answer": "Pour activer ou d\u00e9sactiver TF32 pour les multiplications matricielles avec `torch >= 1.12.0`, il faut donner la valeur `True` ou `False` \u00e0 l'indicateur `torch.backends.cuda.matmul.allow_tf32`."
  },
  {
    "question": "Quel indicateur utilise-t-on pour activer ou d\u00e9sactiver le mode TF32 pour les convolutions avec `torch >= 1.12.0`?",
    "answer": "Pour les convolutions avec `torch >= 1.12.0`, on utilise l'indicateur `torch.backends.cudnn.allow_tf32` pour activer ou d\u00e9sactiver le mode TF32."
  },
  {
    "question": "O\u00f9 trouver plus d'informations sur le mode TF32 dans la documentation PyTorch?",
    "answer": "Pour plus d'information sur le mode TF32, consultez le paragraphe https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere de la documentation PyTorch."
  },
  {
    "question": "Quelles sont les deux m\u00e9thodes de parall\u00e9lisme par d\u00e9faut offertes par PyTorch avec plusieurs CPU?",
    "answer": "PyTorch permet le parall\u00e9lisme avec plusieurs CPU de deux fa\u00e7ons : intra-op et inter-op."
  },
  {
    "question": "Qu'est-ce que le parall\u00e9lisme intra-op dans PyTorch avec les CPU?",
    "answer": "Le parall\u00e9lisme intra-op dans PyTorch permet l'impl\u00e9mentation parall\u00e8le d'op\u00e9rateurs d'apprentissage profond comme le produit matriciel ou le produit de convolution, en utilisant OpenMP ou des biblioth\u00e8ques comme MKL et OneDNN, et utilise automatiquement de multiples fils avec tous les c\u0153urs CPU disponibles."
  },
  {
    "question": "Comment fonctionne le parall\u00e9lisme inter-op avec PyTorch sur des CPU?",
    "answer": "Le parall\u00e9lisme inter-op permet d'ex\u00e9cuter diff\u00e9rentes parties de code de mani\u00e8re concurrente, n\u00e9cessitant un programme con\u00e7u pour la parall\u00e9lisation, par exemple avec le compilateur `torch.jit` pour des t\u00e2ches asynchrones dans un programme TorchScript."
  },
  {
    "question": "Est-il recommand\u00e9 d'utiliser un GPU pour l'entra\u00eenement de petits mod\u00e8les avec PyTorch?",
    "answer": "Pour les petits mod\u00e8les, il est fortement recommand\u00e9 d'utiliser plusieurs CPU plut\u00f4t qu'un GPU."
  },
  {
    "question": "Pourquoi l'utilisation d'un GPU pour les petits mod\u00e8les n'est-elle pas toujours optimale, m\u00eame si elle est plus rapide?",
    "answer": "Pour les petits mod\u00e8les et jeux de donn\u00e9es, m\u00eame si un GPU est plus rapide, la vitesse gagn\u00e9e ne sera pas tr\u00e8s importante et la t\u00e2che n\u2019utilisera qu\u2019une petite part de la capacit\u00e9 de calcul du GPU."
  },
  {
    "question": "Quelles sont les implications de l'utilisation d'un GPU pour de petits mod\u00e8les dans un environnement de grappe partag\u00e9?",
    "answer": "Dans un environnement de grappe partag\u00e9, l'utilisation d'un GPU pour de petits mod\u00e8les bloquerait une ressource pour d'autres projets, diminuerait l'allocation de votre groupe et impacterait la priorit\u00e9 des t\u00e2ches de vos coll\u00e8gues."
  },
  {
    "question": "Quel type de parall\u00e9lisme est mis en \u00e9vidence dans l'exemple de code `cifar10-cpu.py`?",
    "answer": "L'exemple de code `cifar10-cpu.py` pr\u00e9sente plusieurs occasions d'utiliser le parall\u00e9lisme intra-op."
  }
]