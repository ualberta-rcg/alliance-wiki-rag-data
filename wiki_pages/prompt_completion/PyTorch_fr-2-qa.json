[
  {
    "question": "What potential issues might arise when running PyTorch code on A100, H100, or newer Nvidia GPUs, specifically concerning PyTorch versions before and after 1.12.0?",
    "answer": "On A100, H100, or newer Nvidia GPUs, you might observe a significant slowdown and different computational results when running the same code with `torch < 1.12.0` compared to `torch >= 1.12.0`."
  },
  {
    "question": "How can TensorFloat-32 (TF32) be explicitly enabled or disabled for matrix multiplications in PyTorch version 1.12.0 or higher?",
    "answer": "For `torch >= 1.12.0`, TF32 can be enabled or disabled for matrix multiplications by setting `torch.backends.cuda.matmul.allow_tf32` to `True` or `False`."
  },
  {
    "question": "How can TensorFloat-32 (TF32) be explicitly enabled or disabled for convolutions in PyTorch version 1.12.0 or higher?",
    "answer": "For `torch >= 1.12.0`, TF32 can be enabled or disabled for convolutions by setting `torch.backends.cudnn.allow_tf32` to `True` or `False`."
  },
  {
    "question": "Where can users find more information about TF32 in the PyTorch documentation?",
    "answer": "More information about TF32 can be found in the PyTorch documentation at [https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere](https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere)."
  },
  {
    "question": "What are the two primary methods PyTorch employs for parallelism with multiple CPUs?",
    "answer": "PyTorch enables parallelism with multiple CPUs through 'intra-op' and 'inter-op' methods."
  },
  {
    "question": "Describe intra-op parallelism in PyTorch when working with CPUs.",
    "answer": "Intra-op parallelism involves the parallel implementation of frequently used deep learning operators, such as matrix or convolution products, using OpenMP or low-level libraries like MKL and OneDNN, which automatically utilize multiple threads across all available CPU cores."
  },
  {
    "question": "Describe inter-op parallelism in PyTorch when working with CPUs.",
    "answer": "Inter-op parallelism allows different parts of code to execute concurrently. This typically requires designing the program to run multiple parts in parallel, for instance, by using the `torch.jit` real-time compiler to perform asynchronous tasks in a TorchScript program."
  },
  {
    "question": "For small models, should a GPU or multiple CPUs be used for training, and what is the reasoning?",
    "answer": "For small models, it is strongly recommended to use multiple CPUs rather than a GPU. While GPU training might be faster, if the model and dataset are not sufficiently large, the speed gain from a GPU will be negligible, and the task will only use a small fraction of the GPU's capacity."
  },
  {
    "question": "Why is it discouraged to use a GPU for small models on shared computing clusters?",
    "answer": "Using a GPU for small models on shared clusters blocks a valuable resource that could be used by other projects for large-scale computations, contributes to a decrease in your group's allocation, and negatively impacts the priority of tasks for your colleagues."
  },
  {
    "question": "Does the provided `cifar10-cpu.py` script utilize intra-op parallelism?",
    "answer": "Yes, the `cifar10-cpu.py` script offers several opportunities to leverage intra-op parallelism."
  }
]