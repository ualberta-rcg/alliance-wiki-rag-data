[
  {
    "question": "What is Hugging Face?",
    "answer": "Hugging Face is an organization that builds and maintains several popular open-source software packages widely used in Artificial Intelligence research."
  },
  {
    "question": "What is the Transformers package?",
    "answer": "Transformers is a python package that provides APIs and tools to easily download and train state-of-the-art models, pre-trained on various tasks in multiple domains."
  },
  {
    "question": "How do you install the Transformers package?",
    "answer": "The recommended way to install Transformers is by loading a Python module, creating and starting a virtual environment, and then installing it with `pip install --no-index transformers`."
  },
  {
    "question": "What are the recommended steps to install Transformers?",
    "answer": "1. Load a Python module (`module load python`). 2. Create and start a virtual environment. 3. Install Transformers in the virtual environment with `pip install --no-index transformers`."
  },
  {
    "question": "Where should pre-trained models be downloaded?",
    "answer": "Pre-trained models must be downloaded on the login node of the cluster you are working on."
  },
  {
    "question": "Why should models be downloaded on a login node?",
    "answer": "Models must be downloaded on a login node to avoid idle compute while waiting for resources to download."
  },
  {
    "question": "How are pre-trained models made available for download by Hugging Face?",
    "answer": "Pre-trained models are usually made up of fairly large binary files, which Hugging Face makes available for download via Git Large File Storage (Git LFS)."
  },
  {
    "question": "How do you download a model using Git LFS?",
    "answer": "To download a model using Git LFS, load the `git-lfs` module (`module load git-lfs/3.4.0`) and then clone your chosen model repository from the model hub (e.g., `git clone --depth 1 --jobs 1 https://huggingface.co/bert-base-uncased`)."
  },
  {
    "question": "How do you load a locally saved pre-trained model in a Python script within a job?",
    "answer": "You can load a locally saved pre-trained model by using `from transformers import AutoModel, AutoTokenizer` and then specifying the local path and `local_files_only=True` (e.g., `model = AutoModel.from_pretrained('/path/to/where/you/cloned/the/model', local_files_only=True)`)."
  },
  {
    "question": "What option should be used when loading a local model to prevent web downloads?",
    "answer": "The `local_files_only=True` option should be used when loading a local model to avoid attempts to download it from the web."
  },
  {
    "question": "What package contains the Hugging Face CLI for downloading models?",
    "answer": "The `huggingface_hub` package contains a command line interface (CLI) which can be used to download models."
  },
  {
    "question": "How do you download a model like Zephyr-7b-beta using the Hugging Face CLI?",
    "answer": "After installing `huggingface_hub` in a virtual environment, run `HF_HUB_DISABLE_XET=1 hf download --max-workers=1 HuggingFaceH4/zephyr-7b-beta` on a login node."
  },
  {
    "question": "Why is HF_HUB_DISABLE_XET set when using hf download?",
    "answer": "The variable `HF_HUB_DISABLE_XET` is set to avoid using the `hf_xet` package, which currently leads to failures on the systems."
  },
  {
    "question": "How can I download pre-trained models using Python?",
    "answer": "You can download pre-trained models using Python by executing `from transformers import AutoModel, AutoTokenizer` and then `model = AutoModel.from_pretrained('bert-base-uncased')` on a login node."
  },
  {
    "question": "Where must Python-based model downloads be executed?",
    "answer": "Python-based model downloads must be executed on a login node, as an internet connection is required."
  },
  {
    "question": "What is the default cache directory for pre-trained models downloaded via Python?",
    "answer": "The default cache directory for pre-trained model files downloaded via Python is `$HOME/.cache/huggingface/hub`."
  },
  {
    "question": "How can I change the default cache directory for Transformers models?",
    "answer": "You can change the cache directory by setting the environment variable `TRANSFORMERS_CACHE` before importing anything from the transformers package in your Python script (e.g., `os.environ['TRANSFORMERS_CACHE']='./'`)."
  },
  {
    "question": "What environment variable changes the Transformers cache directory?",
    "answer": "The `TRANSFORMERS_CACHE` environment variable changes the Transformers cache directory."
  },
  {
    "question": "What is another frequently used way of loading a pre-trained model?",
    "answer": "Another frequently used way of loading a pre-trained model is via a pipeline."
  },
  {
    "question": "How do you download a model using a pipeline on a login node?",
    "answer": "On a login node, you can download a model using a pipeline by calling `from transformers import pipeline` and then `pipe = pipeline('text-classification')`, passing a model name or task type."
  },
  {
    "question": "Where does `pipeline` store downloaded models by default?",
    "answer": "`pipeline` stores downloaded models at the default cache location."
  }
]