[
  {
    "question": "How many cores should a serial job utilize on a Niagara node?",
    "answer": "If you run serial jobs on Niagara, you must still use all 40 cores available on the node."
  },
  {
    "question": "What happens if a job on Niagara does not use the recommended N x 40 cores?",
    "answer": "If your job does not use N x 40 cores (where N is the number of nodes), the support team will contact you to help optimize your workflow. You can also contact support for assistance."
  },
  {
    "question": "Where can I find examples for running serial jobs on Niagara?",
    "answer": "You can find examples for running serial jobs on Niagara by visiting the 'Running Serial Jobs on Niagara' page at https://docs.scinet.utoronto.ca/index.php/Running_Serial_Jobs_on_Niagara."
  },
  {
    "question": "What factors determine job limits on Niagara?",
    "answer": "Job limits on Niagara, including size and duration, depend on whether a user's group has a Resources for Research Group allocation and which 'partition' the job runs in."
  },
  {
    "question": "What is a 'partition' in the context of SLURM on Niagara?",
    "answer": "In SLURM-speak on Niagara, 'partitions' refer to different use cases for job submission."
  },
  {
    "question": "How do you specify a partition for a job on Niagara?",
    "answer": "You specify a partition for a job using the `-p` parameter with the `sbatch` or `salloc` commands."
  },
  {
    "question": "Which partition is used if no partition is explicitly specified for a job?",
    "answer": "If you do not specify a partition, your job will run in the `compute` partition, which is the most common case."
  },
  {
    "question": "What are the job limits for compute jobs with an allocation in the 'compute' partition?",
    "answer": "For compute jobs with an allocation in the 'compute' partition, users can have 50 running jobs and 1000 submitted jobs (including running). The minimum job size is 1 node (40 cores) and the maximum is 1000 nodes (40000 cores), with a walltime between 15 minutes and 24 hours."
  },
  {
    "question": "What are the job limits for compute jobs without an allocation (default) in the 'compute' partition?",
    "answer": "For compute jobs without an allocation in the 'compute' partition, users can have 50 running jobs and 200 submitted jobs (including running). The minimum job size is 1 node (40 cores) and the maximum is 20 nodes (800 cores), with a walltime between 15 minutes and 24 hours."
  },
  {
    "question": "What are the job limits for testing or troubleshooting in the 'debug' partition?",
    "answer": "For testing or troubleshooting jobs in the 'debug' partition, users can have 1 running job and 1 submitted job. The minimum job size is 1 node (40 cores) and the maximum is 4 nodes (160 cores), with a maximum walltime of 1 hour."
  },
  {
    "question": "What are the job limits for archiving or retrieving data in HPSS using the 'archivelong' partition?",
    "answer": "For archiving or retrieving data in HPSS using the 'archivelong' partition, users can have 2 running jobs (with a maximum of 5 total across all users) and 10 submitted jobs. The walltime can range from 15 minutes to 72 hours."
  },
  {
    "question": "What are the job limits for inspecting archived data or small archival actions in HPSS using the 'archiveshort' partition?",
    "answer": "For inspecting archived data or small archival actions in HPSS using the 'archiveshort' partition, users can have 2 running jobs and 10 submitted jobs. The walltime can range from 15 minutes to 1 hour."
  },
  {
    "question": "What factors influence the waiting time for jobs in the Niagara queue?",
    "answer": "The waiting time for jobs in the Niagara queue depends on factors such as the allocation amount, how much allocation was used recently, the number of nodes requested, the walltime, and the number of other jobs waiting."
  },
  {
    "question": "Which file systems on Niagara use GPFS?",
    "answer": "The `$HOME`, `$SCRATCH`, and `$PROJECT` directories all use the parallel file system called GPFS."
  },
  {
    "question": "What is GPFS optimized for on Niagara?",
    "answer": "GPFS is a high-performance file system optimized to provide rapid reads and writes to large data sets in parallel from many nodes."
  },
  {
    "question": "What type of file access can lead to poor performance on GPFS?",
    "answer": "Accessing data sets that consist of many, small files leads to poor performance on GPFS."
  },
  {
    "question": "What should be used if a job must write many small files?",
    "answer": "If you must write many small files, it is recommended to use ramdisk."
  },
  {
    "question": "What is a recommended practice for writing data to disk to improve performance and save space?",
    "answer": "It is recommended to write data out in a binary format, as this is faster and takes less space."
  },
  {
    "question": "When is the Burst Buffer useful on Niagara?",
    "answer": "The Burst Buffer is better for I/O heavy jobs and for speeding up checkpoints."
  },
  {
    "question": "What is the purpose of `#SBATCH` directives in a job submission script?",
    "answer": "Lines starting with `#SBATCH` are directives sent to SLURM, specifying job requests like number of nodes, tasks, walltime, job name, output file, and mail notifications."
  },
  {
    "question": "How many nodes and tasks are requested in the example MPI job script?",
    "answer": "The example MPI job script requests 2 nodes and a total of 80 tasks."
  },
  {
    "question": "What is the walltime specified in the example MPI job script?",
    "answer": "The example MPI job script specifies a walltime of 1 hour (1:00:00)."
  },
  {
    "question": "How are software modules loaded within an MPI job submission script?",
    "answer": "Software modules are loaded using `module load <module-name>` commands within the script, such as `module load intel/2018.2` and `module load openmpi/3.1.0`."
  },
  {
    "question": "What commands can be used to run the MPI application in the example script?",
    "answer": "The `mpirun ./mpi_example` or `srun ./mpi_example` commands can be used to run the MPI application in the example script."
  },
  {
    "question": "How do you submit an MPI job script named `mpi_job.sh` on Niagara?",
    "answer": "You submit the MPI job script from your scratch directory using the command `sbatch mpi_job.sh`."
  },
  {
    "question": "What is an alternative way to specify the number of tasks per node in an MPI job script?",
    "answer": "Instead of specifying `--ntasks=80`, you can also specify `--ntasks-per-node=40` to achieve the same allocation."
  },
  {
    "question": "How can hyperthreading be enabled for an OpenMPI job on Niagara?",
    "answer": "To use hyperthreading with OpenMPI, you should change `--ntasks=80` to `--ntasks=160` in your SLURM directives and add `--bind-to none` to the `mpirun` command."
  },
  {
    "question": "How many nodes and CPUs per task are requested in the example OpenMP job script?",
    "answer": "The example OpenMP job script requests 1 node and 40 CPUs per task."
  },
  {
    "question": "How is the number of OpenMP threads set in the example OpenMP script?",
    "answer": "The number of OpenMP threads is set by exporting the `OMP_NUM_THREADS` environment variable to the value of `$SLURM_CPUS_PER_TASK`."
  },
  {
    "question": "How can hyperthreading be enabled for an OpenMP job on Niagara?",
    "answer": "To use hyperthreading for an OpenMP job, you just need to change `--cpus-per-task=40` to `--cpus-per-task=80` in your SBATCH directives."
  },
  {
    "question": "Why do modules need to be loaded again in the job submission script on Niagara?",
    "answer": "Modules must be loaded again in the submission script on Niagara because the execution environment on compute nodes is separate from the login nodes."
  },
  {
    "question": "What command is used to display the job queue on Niagara?",
    "answer": "You can use the `squeue` or `sqc` command to show the job queue."
  },
  {
    "question": "How can a user view only their own jobs in the queue?",
    "answer": "To view only your own jobs in the queue, use the command `squeue -u $USER`."
  },
  {
    "question": "What command provides a summary of the queue by user?",
    "answer": "The `qsum` command shows a summary of the queue by user."
  },
  {
    "question": "How can a user get detailed information on a specific job ID?",
    "answer": "You can get information on a specific job ID using `squeue -j JOBID` or, for more verbose details, `scontrol show job JOBID`."
  },
  {
    "question": "How can a user get an estimated start time for a queued job?",
    "answer": "You can get an estimate for when a job will run using `squeue --start -j JOBID`; however, these predictions tend not to be very accurate."
  },
  {
    "question": "What command is used to cancel a job on Niagara?",
    "answer": "To cancel a job, use the command `scancel -i JOBID`."
  },
  {
    "question": "How can a user monitor the instantaneous CPU and memory usage of a running job's nodes?",
    "answer": "The `jobperf JOBID` command can be used to get an instantaneous view of the CPU and memory usage of the nodes of the job while it is running."
  },
  {
    "question": "What command provides information on recent jobs?",
    "answer": "The `sacct` command can be used to get information on your recent jobs."
  },
  {
    "question": "What online resource is useful for monitoring current and past job usage on Niagara?",
    "answer": "The my.SciNet site (https://my.scinet.utoronto.ca) is a very useful tool for monitoring your current and past usage."
  },
  {
    "question": "Where can users find information about using visualization tools on Niagara?",
    "answer": "Information about how to use visualization tools on Niagara is available on the 'Visualization' page."
  },
  {
    "question": "Where can users find the current system status for Niagara?",
    "answer": "Users can find the system status at https://docs.scinet.utoronto.ca/index.php/Main_Page."
  },
  {
    "question": "Where can users find training resources for Niagara?",
    "answer": "Training resources for Niagara are available at https://support.scinet.utoronto.ca/education."
  },
  {
    "question": "How can users contact support for Niagara?",
    "answer": "Users can contact 'Technical support' for assistance."
  }
]