[
  {
    "question": "What is the purpose of the Shared Memory Parallel (CPU) Slurm script for Ansys Mechanical?",
    "answer": "The `script-smp-2023-cpu.sh` is designed for running Ansys Mechanical (APDL) jobs using Shared Memory Parallel (SMP) on CPU resources. It is typically used for single-node computations."
  },
  {
    "question": "How do you specify the number of cores for a Shared Memory Parallel (CPU) Mechanical job?",
    "answer": "In the `script-smp-2023-cpu.sh`, you specify the number of cores using `#SBATCH --tasks=8`. The example shows 8 cores, but this value should be adjusted as needed."
  },
  {
    "question": "What memory is allocated by default for APDL jobs, and how can it be changed?",
    "answer": "Ansys allocates 1024 MB total memory and 1024 MB database memory by default for APDL jobs. These values can be manually specified or changed by adding arguments `-m <total_memory_MB>` and/or `-db <database_memory_MB>` to the `mapdl` command line in your Slurm scripts."
  },
  {
    "question": "Which Ansys module is loaded in the example Mechanical Slurm scripts for 2023/2024 releases?",
    "answer": "The example Mechanical Slurm scripts load `module load StdEnv/2023` and `module load ansys/2024R1.03` for the 2024R1.03 version, though `ansys/2023R2` is commented out as an alternative."
  },
  {
    "question": "How is a unique output directory created for Mechanical jobs?",
    "answer": "A unique output directory is created using `mkdir outdir-$SLURM_JOBID`. The `$SLURM_JOBID` variable ensures the directory name is unique to the specific job."
  },
  {
    "question": "What is the command to execute an SMP CPU Mechanical job?",
    "answer": "The command to execute an SMP CPU Mechanical job is `mapdl -smp -b nolist -np $SLURM_NTASKS -dir outdir-$SLURM_JOBID -i YOURAPDLFILE.inp`."
  },
  {
    "question": "What is the purpose of the Distributed Memory Parallel (CPU) Slurm script for Ansys Mechanical?",
    "answer": "The `script-dmp-2023-cpu.sh` is used for running Ansys Mechanical (APDL) jobs that utilize Distributed Memory Parallel (DMP) with CPU resources, typically across multiple compute nodes."
  },
  {
    "question": "How do you specify memory for a Distributed Memory Parallel (CPU) Mechanical job?",
    "answer": "For a Distributed Memory Parallel (CPU) Mechanical job, memory is specified using `#SBATCH --mem-per-cpu=4G`, which allocates 4 gigabytes of memory per core."
  },
  {
    "question": "What specific setup is required for Cedar cluster when running DMP Mechanical CPU jobs?",
    "answer": "For Cedar cluster, when running DMP Mechanical CPU jobs, you need to create a symbolic link for `libstdc++.so.6.0.29` within the job's output directory and then export that directory to `LD_LIBRARY_PATH`. This ensures the necessary library is found during execution."
  },
  {
    "question": "Which MPI implementation is used for DMP Mechanical CPU jobs on Beluga cluster?",
    "answer": "On the Beluga cluster, DMP Mechanical CPU jobs use `intelmpi` for distributed parallel processing. The `export KMP_AFFINITY=none` setting is also applied."
  },
  {
    "question": "What is the purpose of the Shared Memory Parallel (GPU) Slurm script for Ansys Mechanical?",
    "answer": "The `script-smp-2023-gpu.sh` is for running Ansys Mechanical (APDL) jobs using Shared Memory Parallel (SMP) while leveraging GPU acceleration on a single compute node."
  },
  {
    "question": "How do you request GPUs for an SMP GPU Mechanical job?",
    "answer": "GPUs are requested using `#SBATCH --gpus-per-node=1` (or a specific type like `#SBATCH --gpus-per-node=h100:1`). This specifies the quantity of GPUs to be allocated per node."
  },
  {
    "question": "What environment variable is set to print information about GPU devices in Mechanical GPU jobs?",
    "answer": "The environment variable `export ANSGPU_PRINTDEVICES=1` is set to print information about the GPU devices being used by Mechanical jobs."
  },
  {
    "question": "What is the command to execute an SMP GPU Mechanical job?",
    "answer": "The command to execute an SMP GPU Mechanical job is `mapdl -smp -acc nvidia -na $SLURM_GPUS_ON_NODE -b nolist -np $SLURM_NTASKS -dir outdir-$SLURM_JOBID -i YOURAPDLFILE.inp`."
  },
  {
    "question": "What is the purpose of the Distributed Memory Parallel (GPU) Slurm script for Ansys Mechanical?",
    "answer": "The `script-dmp-2023-gpu.sh` is designed for running Ansys Mechanical (APDL) jobs with Distributed Memory Parallel (DMP) using GPU acceleration, potentially across multiple nodes."
  },
  {
    "question": "Which MPI implementation is used for DMP Mechanical GPU jobs on clusters other than Beluga?",
    "answer": "On clusters other than Beluga, DMP Mechanical GPU jobs use `openmpi` for distributed parallel processing."
  },
  {
    "question": "Why are scaling tests important for Ansys Mechanical jobs?",
    "answer": "Scaling tests should always be performed before running production jobs to ensure that the optimal number of cores and the minimum required amount of memory are specified in your scripts. This helps in efficient resource utilization."
  },
  {
    "question": "Should SMP or DMP scripts be preferred for Ansys Mechanical jobs?",
    "answer": "The single-node (SMP - Shared Memory Parallel) scripts will typically perform better than the multi-node (DMP - Distributed Memory Parallel) scripts and should therefore be used whenever possible."
  },
  {
    "question": "How can you check the Ansys Workbench version used to generate an APDL input file?",
    "answer": "You can check the Ansys Workbench version used to generate an APDL input file by inspecting the file content. For example, `cat YOURAPDLFILE.inp | grep version` might show output like `! ANSYS input file written by Workbench version 2019 R3`."
  },
  {
    "question": "Can Ansys Rocky run simulations in both GUI and non-GUI modes?",
    "answer": "Yes, Ansys Rocky can run simulations in both GUI mode (as discussed in the Graphical usage section) and non-GUI mode."
  },
  {
    "question": "On which cluster are the provided Ansys Rocky Slurm scripts currently usable?",
    "answer": "The provided Ansys Rocky Slurm scripts are currently only usable on the Graham cluster, as the Rocky module is exclusively installed there at present."
  },
  {
    "question": "How can you get a full listing of command-line options for Ansys Rocky?",
    "answer": "To get a full listing of command-line options for Ansys Rocky, you should first load any Rocky module (e.g., `ansysrocky/2023R2` on Graham) and then run `Rocky -h` on the command line."
  },
  {
    "question": "What is the recommended number of CPUs to request for Ansys Rocky when running standalone uncoupled problems with GPUs?",
    "answer": "When Rocky is run with GPUs to solve standalone uncoupled problems, only a minimal number of CPUs (e.g., 2 or possibly 3) should be requested that will allow Rocky to run optimally."
  },
  {
    "question": "When are `rocky_hpc` licenses required for Ansys Rocky, and which license provides them?",
    "answer": "`rocky_hpc` licenses are required when Ansys Rocky is run with 4 or more CPUs. The SHARCNET license provides these licenses."
  },
  {
    "question": "What is the command to execute an Ansys Rocky CPU-only simulation from a Slurm script?",
    "answer": "The command to execute an Ansys Rocky CPU-only simulation is `Rocky --simulate \u201cmysim.rocky\u201d --resume=1 --ncpus=$SLURM_CPUS_PER_TASK --use-gpu=0`."
  },
  {
    "question": "What is the command to execute an Ansys Rocky GPU-based simulation from a Slurm script?",
    "answer": "The command to execute an Ansys Rocky GPU-based simulation is `Rocky --simulate \u201cmysim.rocky\u201d --resume=1 --ncpus=$SLURM_CPUS_PER_TASK --use-gpu=1 --gpu-num=$SLURM_GPUS_ON_NODE`."
  },
  {
    "question": "What modules are loaded in the example Ansys Rocky Slurm scripts for 2024R2.0?",
    "answer": "The example Ansys Rocky Slurm scripts load `ansysrocky/2024R2.0 StdEnv/2023 ansys/2024R2.04`."
  }
]