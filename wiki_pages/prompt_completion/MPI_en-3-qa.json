[
  {
    "question": "What information can each MPI process output?",
    "answer": "Each process can output its rank and size information."
  },
  {
    "question": "Are conditional blocks required in an MPI program when all processes perform identical operations?",
    "answer": "No, conditional blocks are not required when all processes perform identical operations."
  },
  {
    "question": "How do you obtain the rank of an MPI process in C?",
    "answer": "In C, after calling `MPI_Init`, you can obtain the rank using `MPI_Comm_rank(MPI_COMM_WORLD, &rank)`."
  },
  {
    "question": "How do you obtain the total number of MPI processes (size) in C?",
    "answer": "In C, after calling `MPI_Init`, you can obtain the total number of processes using `MPI_Comm_size(MPI_COMM_WORLD, &size)`."
  },
  {
    "question": "How do you get the rank and size of an MPI process using Boost C++ MPI?",
    "answer": "In Boost C++ MPI, after initializing `mpi::environment env(argc, argv);` and `mpi::communicator world;`, you can get the rank with `world.rank()` and the size with `world.size()`."
  },
  {
    "question": "How do you get the rank and size of an MPI process in Fortran?",
    "answer": "In Fortran, after `call MPI_INIT(ierror)`, you can get the rank by `call MPI_COMM_RANK(MPI_COMM_WORLD, rank, ierror)` and the size by `call MPI_COMM_SIZE(MPI_COMM_WORLD, size, ierror)`."
  },
  {
    "question": "How do you get the rank and size of an MPI process using mpi4py in Python?",
    "answer": "In mpi4py Python, after `comm = MPI.COMM_WORLD`, you can get the rank with `comm.Get_rank()` and the size with `comm.Get_size()`."
  },
  {
    "question": "What happens when multiple MPI processes print output to standard out?",
    "answer": "The stdout of all running processes is concatenated together, and the output from different processes may not appear in order of rank, so no assumptions should be made about the order."
  },
  {
    "question": "How do you compile a basic C MPI program?",
    "answer": "A basic C MPI program like `phello1.c` can be compiled using `mpicc -Wall phello1.c -o phello1`."
  },
  {
    "question": "How do you run an MPI program with a specified number of processes?",
    "answer": "An MPI program can be run using `mpirun -np [number_of_processes] [executable_name]`, for example, `mpirun -np 4 ./phello1`."
  },
  {
    "question": "How do you compile a Boost C++ MPI program?",
    "answer": "A Boost C++ MPI program like `phello1.cpp` can be compiled using `mpic++ --std=c++11 phello1.cpp -lboost_mpi-mt -lboost_serialization-mt -o phello1`."
  },
  {
    "question": "How do you run a Python MPI program using mpi4py?",
    "answer": "A Python MPI program using mpi4py can be run using `mpirun -np [number_of_processes] python [script_name.py]`, for example, `mpirun -np 4 python phello1.py`."
  },
  {
    "question": "What is a simple communication pattern described for MPI processes in a ring?",
    "answer": "A simple communication ring pattern involves each process `i` sending a message to the process with the next higher rank number, `(i+1)%N`, where `N` is the total number of processes. The last process `N-1` sends its message back to process `0`."
  },
  {
    "question": "What kind of communication patterns does MPI support?",
    "answer": "MPI supports a large number of communication patterns, including one-to-one, one-to-many, many-to-one, and many-to-many."
  },
  {
    "question": "What are the simplest MPI functions for one-to-one data transfer between processes?",
    "answer": "The simplest MPI functions for one-to-one data transfer are `MPI_Send` for sending and `MPI_Recv` for receiving."
  }
]