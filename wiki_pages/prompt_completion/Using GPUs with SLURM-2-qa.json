[
  {
    "question": "Are cloud resources on the Arbutus cluster schedulable via Slurm?",
    "answer": "No, cloud resources on Arbutus are not schedulable via Slurm."
  },
  {
    "question": "Where can I find details about available hardware for Arbutus cloud resources?",
    "answer": "See [[Cloud resources]] for details of available hardware for Arbutus."
  },
  {
    "question": "What is NVIDIA's \"Compute Capability\"?",
    "answer": "Compute Capability is a technical term created by NVIDIA to describe what hardware functions are available on some models of GPU and not on others."
  },
  {
    "question": "Is \"Compute Capability\" a measure of performance?",
    "answer": "No, Compute Capability is not a measure of performance."
  },
  {
    "question": "When is \"Compute Capability\" relevant for users?",
    "answer": "Compute Capability is relevant only if you are compiling your own GPU programs."
  },
  {
    "question": "How do you access the 16GB V100 GPU flavor on Graham?",
    "answer": "To access the 16GB V100 GPU flavor on Graham, use the arguments `--constraint=skylake,v100` in your sbatch/salloc command."
  },
  {
    "question": "How do you access the 32GB V100 GPU flavor on Graham?",
    "answer": "To access the 32GB V100 GPU flavor on Graham, use the arguments `--constraint=cascade,v100` in your sbatch/salloc command."
  },
  {
    "question": "What type of CPUs and GPUs comprise the Mist cluster?",
    "answer": "Mist is a cluster comprised of IBM Power9 CPUs (not Intel x86!) and NVIDIA V100 GPUs."
  },
  {
    "question": "Who can access the Mist cluster?",
    "answer": "Users with access to Niagara can also access Mist."
  },
  {
    "question": "Where can I find specific instructions for submitting jobs on Mist?",
    "answer": "To specify job requirements on Mist, please see the specific instructions on the [https://docs.scinet.utoronto.ca/index.php/Mist#Submitting_jobs SciNet website]."
  },
  {
    "question": "Is Multi-Instance GPU (MIG) activated on the Narval cluster?",
    "answer": "Yes, MIG is currently activated on the Narval cluster as a pilot project."
  },
  {
    "question": "Where can I find more information on how to use MIGs on Narval?",
    "answer": "For more information on how to use the MIGs on Narval please see [[Multi-Instance_GPU]]."
  },
  {
    "question": "Do all clusters have only one GPU type available?",
    "answer": "No, some clusters like Cedar and Graham have more than one GPU type available."
  },
  {
    "question": "What happens if I don't specify a GPU type for my Slurm job?",
    "answer": "If you do not supply a type specifier, Slurm may send your job to a node equipped with any type of GPU."
  },
  {
    "question": "Why is it important to include a GPU type specifier for certain workflows?",
    "answer": "It is important because for certain workflows, such as molecular dynamics code requiring high double-precision performance, specific GPU types (like T4 GPUs) may not be appropriate, making an explicit type specifier desirable."
  },
  {
    "question": "What is an example Slurm script for a single-core job requesting one GPU?",
    "answer": "A single-core job requesting one GPU can use `#SBATCH --gpus-per-node=1` and include `./program` as its executable, with other standard Slurm directives for account, memory, and time."
  },
  {
    "question": "How do you request a single GPU and multiple CPU cores for a multi-threaded job in Slurm?",
    "answer": "For a multi-threaded job, you would use `#SBATCH --gpus-per-node=1` and `#SBATCH --cpus-per-task=6` (or other desired core count) in your Slurm script, along with `export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK`."
  },
  {
    "question": "What is the recommended maximum number of CPU cores per GPU on B\u00e9luga?",
    "answer": "On B\u00e9luga, it is recommended to use no more than 10 CPU cores per GPU."
  },
  {
    "question": "What is the recommended maximum number of CPU cores per P100 GPU (p100 and p100l) on Cedar?",
    "answer": "On Cedar, it is recommended to use no more than 6 CPU cores per P100 GPU (p100 and p100l)."
  },
  {
    "question": "What is the recommended maximum number of CPU cores per V100 GPU (v100l) on Cedar?",
    "answer": "On Cedar, it is recommended to use no more than 8 CPU cores per V100 GPU (v100l)."
  },
  {
    "question": "What is the recommended maximum number of CPU cores per GPU on Graham?",
    "answer": "On Graham, it is recommended to use no more than 16 CPU cores per GPU."
  },
  {
    "question": "How would you request 8 total GPUs and 1 MPI process per GPU for an MPI job?",
    "answer": "You would use `#SBATCH --gpus=8` to request 8 total GPUs and `#SBATCH --ntasks-per-gpu=1` to specify 1 MPI process per GPU, along with `srun --cpus-per-task=$SLURM_CPUS_PER_TASK ./program`."
  },
  {
    "question": "What is the advantage of requesting whole nodes for GPU applications?",
    "answer": "If your application can efficiently use an entire node and its associated GPUs, you will probably experience shorter wait times by requesting a whole node."
  },
  {
    "question": "What are the key Slurm directives for requesting a P100 GPU node on Graham?",
    "answer": "For requesting a P100 GPU node on Graham, key Slurm directives include `#SBATCH --nodes=1`, `#SBATCH --gpus-per-node=p100:2`, `#SBATCH --ntasks-per-node=32`, and `#SBATCH --mem=127000M`."
  },
  {
    "question": "What are the key Slurm directives for requesting a P100 GPU node on Cedar?",
    "answer": "For requesting a P100 GPU node on Cedar, key Slurm directives include `#SBATCH --nodes=1`, `#SBATCH --gpus-per-node=p100:4`, `#SBATCH --ntasks-per-node=24`, and `#SBATCH --exclusive`, along with `#SBATCH --mem=125G`."
  }
]