[
  {
    "question": "What problem do tools like GNU Parallel, Job Arrays, GLOST, and META help solve?",
    "answer": "These tools help users avoid creating and submitting thousands of job scripts when processing multiple data files with many different combinations of parameters."
  },
  {
    "question": "What is GNU Parallel used for?",
    "answer": "GNU Parallel is used to fill an entire compute node with serial compute tasks, addressing multiple combinations of arguments."
  },
  {
    "question": "How do Job Arrays function?",
    "answer": "Job Arrays allow a single script to represent multiple Slurm jobs, with each job identified by an integer value."
  },
  {
    "question": "What is GLOST (Greedy Launcher Of Small Tasks)?",
    "answer": "GLOST is described as a sophisticated mix of GNU Parallel and Job Arrays."
  },
  {
    "question": "What is META designed for?",
    "answer": "META is a suite of scripts developed in SHARCNET to automate high-throughput computing, managing a large number of related serial, parallel, or GPU calculations."
  },
  {
    "question": "What are inter-job dependencies in the context of Slurm?",
    "answer": "Inter-job dependencies are the links and relationships between individual Slurm jobs, which act as building blocks of pipelines. They define the order in which jobs should run."
  },
  {
    "question": "How can you make a Slurm job dependent on the successful completion of another job?",
    "answer": "You can use the `--dependency=afterok:$JOBID` option when submitting the second job, where `$JOBID` is the ID of the first job. For example, `sbatch --dependency=afterok:$JOBID1 job2.sh`."
  },
  {
    "question": "Can multiple Slurm jobs depend on a single job?",
    "answer": "Yes, multiple jobs can be configured to have the same dependency, meaning they all wait for one specific job to complete."
  },
  {
    "question": "Can a single Slurm job have multiple dependencies?",
    "answer": "Yes, a single job can be configured to wait after multiple other jobs before starting."
  },
  {
    "question": "What are some types of dependencies available for Slurm jobs?",
    "answer": "There are multiple types of dependencies available, including `after`, `afterany`, `afterok`, and `afternotok`."
  },
  {
    "question": "What are heterogeneous jobs in Slurm?",
    "answer": "Heterogeneous jobs are a Slurm scheduler feature that allows different parts of a single job to request varying amounts of resources, such as cores and memory."
  },
  {
    "question": "When are heterogeneous jobs particularly useful?",
    "answer": "They are very useful for MPI applications where the main process might require more cores and memory than other processes."
  },
  {
    "question": "How can resource requirements for a heterogeneous job be specified within a job script?",
    "answer": "Resource requirements can be specified using separate `#SBATCH` lines for each part of the job, with `#SBATCH hetjob` separating the definitions. For example: `#SBATCH --ntasks=1 --cpus-per-task=5 --mem-per-cpu=16000M` followed by `#SBATCH hetjob` and then `#SBATCH --ntasks=15 --cpus-per-task=1 --mem-per-cpu=1000M`."
  },
  {
    "question": "How can resource requests for a heterogeneous job be defined on the `sbatch` command line?",
    "answer": "Resource requests for a heterogeneous job can be separated by a colon (`:`) on the `sbatch` command line. For example: `sbatch --ntasks=1 --cpus-per-task=5 --mem-per-cpu=16000M : --ntasks=15 --cpus-per-task=1 --mem-per-cpu=1000M mpi_job.sh`."
  }
]