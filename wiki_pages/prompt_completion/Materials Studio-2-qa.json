[
  {
    "question": "What commands must be run after a team installation of Materials Studio but before the software can be used?",
    "answer": "First, you must run `module use $PI_PROJECT_DIR/MatStudio2018/modules/2017/Core/`. Afterwards, you need to load the materialsstudio module using `module load materialsstudio`."
  },
  {
    "question": "Where might team members add the `module use` command for a team installation of Materials Studio?",
    "answer": "Team members may wish to add the `module use $PI_PROJECT_DIR/MatStudio2018/modules/2017/Core/` command to their `~/.bashrc` file."
  },
  {
    "question": "What variables should always be replaced with their appropriate values when following team installation instructions?",
    "answer": "You should always replace the `PI_GROUP` and `PI_PROJECT_DIR` variables with their appropriate values."
  },
  {
    "question": "What is a prerequisite for using the provided Slurm job submission script examples for Materials Studio 2018?",
    "answer": "The examples assume that Materials Studio 2018 has been installed according to the instructions provided earlier in the document."
  },
  {
    "question": "Which module needs to be loaded in a Slurm script to use Materials Studio 2018?",
    "answer": "You need to load the module `materialsstudio/2018`."
  },
  {
    "question": "How do you create a list of nodes for a `RunDMol3.sh` job formatted for HP-MPI?",
    "answer": "You can create a list of nodes by using the command `slurm_hl2hl.py --format HP-MPI > $DSD_MachineList`."
  },
  {
    "question": "What are the Slurm directives for a `RunDMol3.sh` job requesting one node, one task per node, 32 CPUs per task, and a 12-hour time limit?",
    "answer": "The Slurm directives are `#SBATCH --nodes=1`, `#SBATCH --ntasks-per-node=1`, `#SBATCH --cpus-per-task=32`, and `#SBATCH --time=12:00:00`."
  },
  {
    "question": "How is the `DSD_MachineList` prepared for a `RunCASTEP.sh` job using the MPIHOSTLIST format?",
    "answer": "The `DSD_MachineList` is prepared by running `slurm_hl2hl.py --format MPIHOSTLIST >$DSD_MachineList` and then exporting it."
  },
  {
    "question": "What are the Slurm directives for a `RunCASTEP.sh` job requesting one node, one task, 2 CPUs per task, 1MB memory per CPU, and a 12-hour time limit?",
    "answer": "The Slurm directives are `#SBATCH --nodes=1`, `#SBATCH --ntasks=1`, `#SBATCH --cpus-per-task=2`, `#SBATCH --mem-per-cpu=1M`, and `#SBATCH --time=0-12:00`."
  },
  {
    "question": "How should versions of Materials Studio earlier than 2018 be installed?",
    "answer": "Versions of Materials Studio earlier than 2018 need to be installed into an Apptainer container."
  },
  {
    "question": "What are the general steps involved in installing an older version of Materials Studio into an Apptainer container?",
    "answer": "The process involves creating an Apptainer container with a compatible Linux distribution, installing Materials Studio into that container, and then uploading the container to your account for use."
  },
  {
    "question": "What action is necessary for an Apptainer container running an older Materials Studio version to access a license server from compute nodes?",
    "answer": "You will need to contact technical support so that they can configure firewall(s) to allow the software in the container to connect to your license server."
  },
  {
    "question": "What is a potential limitation when running older Materials Studio versions in an Apptainer container across multiple nodes?",
    "answer": "You might be restricted to whole-node (single-node) jobs because the version of MPI inside the container might not be compatible for use across nodes."
  }
]