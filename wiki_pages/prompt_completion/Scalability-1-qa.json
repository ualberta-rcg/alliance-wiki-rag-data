[
  {
    "question": "What is scalability in the context of parallel programming?",
    "answer": "Scalability refers to the capacity of a program to efficiently use added computing resources, such as CPU cores, in parallel programming."
  },
  {
    "question": "What factors determine the performance gain when using more CPU cores in parallel programming?",
    "answer": "The gain in performance depends on the nature of the problem, the algorithm or program used to solve it, the underlying hardware (notably memory and network), and the number of CPU cores being used."
  },
  {
    "question": "What is recommended when planning to use a parallel program on a specific cluster?",
    "answer": "It is recommended to conduct a scalability analysis where the software is tested using a fixed problem while varying the number of CPU cores (e.g., 2, 4, 8, 16, 32, 64 cores), and the resulting run time curve is plotted."
  },
  {
    "question": "What are the two primary reasons why scalability is often worse than naively expected?",
    "answer": "The two major reasons are that a percentage of the program's execution remains serial (cannot be parallelized), and there's an increasing cost of 'parallel overhead' due to communication and synchronization among parallel processes."
  },
  {
    "question": "How does the serial percentage of a program's execution affect its parallel efficiency?",
    "answer": "The serial percentage represents an ultimate limit for the parallel efficiency; even with an infinite number of cores, the program's duration cannot go below the time spent in these non-parallelizable operations."
  },
  {
    "question": "What is the best outcome one can hope for regarding the 'serial percentage' in parallel programs?",
    "answer": "The best we can hope for is that this 'serial percentage' shrinks as we increase the size of the problem the software is working on."
  },
  {
    "question": "How does 'parallel overhead' impact the total duration of a program?",
    "answer": "Parallel overhead, caused by communication and synchronization, increases with the number of processes working together, typically as a power of the number of cores (<math>T_c \\propto n^\\alpha</math> where <math>\\alpha > 1</math>), ultimately dominating the total duration as the number of cores increases indefinitely."
  },
  {
    "question": "What is the formula for the total duration of a parallel program (T) as described in the text?",
    "answer": "The total duration of the program is given by <math>T = T_s + T_c = A + B/n + C n^\\alpha</math>, where A, B, and C are positive real numbers, n is the number of cores, and <math>\\alpha > 1</math>."
  },
  {
    "question": "What happens to a parallel program's run time if too many CPU cores are added?",
    "answer": "After a certain number of cores, where a minimum run time is reached, the program's duration will start to increase as more processes are added due to rising parallel overhead."
  },
  {
    "question": "Why is it crucial to perform a scalability analysis when using a parallel program?",
    "answer": "It's crucial to carry out a scalability analysis to determine the optimal number of CPU cores for a specific problem nature, size, and cluster, as using too many cores can actually prolong the program's run time."
  },
  {
    "question": "What are the characteristics of a good test problem for scalability analysis?",
    "answer": "A good test problem should be relatively small for quick testing but not unrepresentative of a production run, ideally requiring 30 to 60 minutes to finish on one or two cores."
  },
  {
    "question": "Why is a test problem running under ten minutes usually not suitable for scalability analysis?",
    "answer": "A test problem that runs in under ten minutes is almost certainly too short to be of value for a scalability analysis, as it may not accurately reflect the behavior of production runs."
  },
  {
    "question": "What kind of test problem is desirable for analyzing a program's behavior under weak scaling?",
    "answer": "For weak scaling analysis, it's desirable to have a test problem whose size can be easily increased, ideally in a fairly continuous manner."
  },
  {
    "question": "What are 'embarrassingly parallel' problems?",
    "answer": "'Embarrassingly parallel' problems are a class of problems for which the parallel overhead factor (C) is practically zero, meaning there's minimal to no need for communication or synchronization between processes."
  },
  {
    "question": "Can you provide an example of an embarrassingly parallel problem?",
    "answer": "A good example is running an analysis on 500 different files, where the analysis of each file is independent of others and simply generates a single number to be stored in an array."
  },
  {
    "question": "What kind of scaling can be achieved with embarrassingly parallel problems?",
    "answer": "With embarrassingly parallel problems, perfect scaling can be achieved out to any number of processes, with the only limitation being the number of independent tasks (e.g., files) available."
  }
]