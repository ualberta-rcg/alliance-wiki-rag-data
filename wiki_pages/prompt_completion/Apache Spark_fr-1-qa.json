[
  {
    "question": "Qu'est-ce qu'Apache Spark?",
    "answer": "Apache Spark est une framework de calcul distribu\u00e9e open source, initialement d\u00e9velopp\u00e9e par l'AMPLab de l'Universit\u00e9 Berkeley et maintenant un projet de la fondation Apache."
  },
  {
    "question": "Quelle est la principale diff\u00e9rence entre Apache Spark et l'algorithme MapReduce de Hadoop?",
    "answer": "Contrairement \u00e0 l'algorithme MapReduce de Hadoop qui utilise le stockage sur disque, Spark utilise des primitives conserv\u00e9es en m\u00e9moire."
  },
  {
    "question": "Quel avantage de performance Spark offre-t-il par rapport \u00e0 MapReduce?",
    "answer": "Spark peut atteindre des performances jusqu'\u00e0 100 fois plus rapides pour certaines applications gr\u00e2ce \u00e0 l'utilisation de primitives conserv\u00e9es en m\u00e9moire."
  },
  {
    "question": "Pour quelles applications Spark est-il particuli\u00e8rement appropri\u00e9?",
    "answer": "Spark est particuli\u00e8rement appropri\u00e9 pour l'apprentissage automatique et l'analyse de donn\u00e9es interactive."
  },
  {
    "question": "Comment les modules Spark et Python sont-ils charg\u00e9s dans l'exemple PySpark?",
    "answer": "Les modules sont charg\u00e9s avec 'module load spark/2.3.0' et 'module load python/3.7'."
  },
  {
    "question": "Quel est le but de la variable d'environnement MKL_NUM_THREADS?",
    "answer": "La variable MKL_NUM_THREADS est une configuration recommand\u00e9e pour l'appel de routines Intel MKL \u00e0 partir d'applications multi-thread\u00e9es."
  },
  {
    "question": "Comment la m\u00e9moire Spark par n\u0153ud est-elle calcul\u00e9e dans les scripts d'exemple?",
    "answer": "La m\u00e9moire Spark par n\u0153ud (SLURM_SPARK_MEM) est calcul\u00e9e \u00e0 95% de la m\u00e9moire SLURM_MEM_PER_NODE, arrondie \u00e0 l'entier le plus proche."
  },
  {
    "question": "Comment le ma\u00eetre Spark est-il d\u00e9marr\u00e9 dans les scripts d'exemple?",
    "answer": "Le ma\u00eetre Spark est d\u00e9marr\u00e9 avec la commande 'start-master.sh'."
  },
  {
    "question": "Comment l'URL du ma\u00eetre Spark est-elle r\u00e9cup\u00e9r\u00e9e?",
    "answer": "L'URL du ma\u00eetre est r\u00e9cup\u00e9r\u00e9e en utilisant 'grep -Po '(?=spark://).*' $SPARK_LOG_DIR/spark-${SPARK_IDENT_STRING}-org.apache.spark.deploy.master*.out'."
  },
  {
    "question": "Comment les esclaves Spark sont-ils d\u00e9marr\u00e9s dans les scripts d'exemple?",
    "answer": "Les esclaves Spark sont d\u00e9marr\u00e9s avec la commande 'SPARK_NO_DAEMONIZE=1 srun -n ${NWORKERS} -N ${NWORKERS} --label --output=$SPARK_LOG_DIR/spark-%j-workers.out start-slave.sh -m ${SLURM_SPARK_MEM}M -c ${SLURM_CPUS_PER_TASK} ${MASTER_URL} &'."
  },
  {
    "question": "Comment un job PySpark est-il soumis dans l'exemple?",
    "answer": "Un job PySpark est soumis avec 'srun -n 1 -N 1 spark-submit --master ${MASTER_URL} --executor-memory ${SLURM_SPARK_MEM}M $SPARK_HOME/examples/src/main/python/pi.py'."
  },
  {
    "question": "Comment le ma\u00eetre et les esclaves Spark sont-ils arr\u00eat\u00e9s?",
    "answer": "Le ma\u00eetre et les esclaves Spark sont arr\u00eat\u00e9s en utilisant 'kill $slaves_pid' et 'stop-master.sh'."
  },
  {
    "question": "Comment les applications Java Spark (Jars) sont-elles soumises dans l'exemple?",
    "answer": "Les applications Java Spark sont soumises en utilisant la commande '$SLURM_SPARK_SUBMIT --class org.apache.spark.examples.SparkPi $SPARK_HOME/examples/jars/spark-examples_2.11-2.3.0.jar 1000' et '$SLURM_SPARK_SUBMIT --class org.apache.spark.examples.SparkLR $SPARK_HOME/examples/jars/spark-examples_2.11-2.3.0.jar 1000'."
  },
  {
    "question": "Que peut-on faire avec les journaux d'activit\u00e9s des applications Spark?",
    "answer": "Les journaux d'activit\u00e9s des applications Spark peuvent \u00eatre sauvegard\u00e9s et consult\u00e9s ult\u00e9rieurement \u00e0 l'aide d'une application web fournie avec Spark."
  }
]