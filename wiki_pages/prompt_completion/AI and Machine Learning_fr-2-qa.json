[
  {
    "question": "O\u00f9 dois-je stocker mon ensemble de donn\u00e9es si sa taille est sup\u00e9rieure \u00e0 l'espace de stockage local du n\u0153ud de calcul?",
    "answer": "Si votre ensemble de donn\u00e9es est tr\u00e8s grand, vous pourriez devoir le laisser dans un espace partag\u00e9 comme /project pour un stockage permanent, ou /scratch qui est parfois plus rapide mais n'est pas con\u00e7u pour du stockage permanent."
  },
  {
    "question": "Comment les espaces de stockage partag\u00e9s (/home, /project, /scratch) doivent-ils \u00eatre utilis\u00e9s?",
    "answer": "Les espaces de stockage partag\u00e9s servent \u00e0 lire et \u00e0 stocker des donn\u00e9es \u00e0 faible fr\u00e9quence, par exemple, 1 gros bloc par 10 secondes plut\u00f4t que 10 petits blocs par seconde."
  },
  {
    "question": "Quels sont les probl\u00e8mes qui peuvent survenir avec des ensembles de donn\u00e9es compos\u00e9s de nombreux petits fichiers en apprentissage machine?",
    "answer": "Les probl\u00e8mes peuvent inclure des quotas de syst\u00e8me de fichiers restreignant le nombre de fichiers et un ralentissement consid\u00e9rable de l'application d\u00fb au transfert des fichiers de /project ou /scratch vers un n\u0153ud de calcul."
  },
  {
    "question": "Comment devrais-je g\u00e9rer un grand nombre de petits fichiers dans un syst\u00e8me de fichiers distribu\u00e9?",
    "answer": "Les donn\u00e9es devraient \u00eatre rassembl\u00e9es dans un seul fichier d'archive. Pour plus d'informations, consultez la page [[Handling large collections of files/fr|Travailler avec un grand nombre de fichiers]]."
  },
  {
    "question": "Que recommande-t-on pour les calculs d'apprentissage machine de longue dur\u00e9e?",
    "answer": "Il est recommand\u00e9 d'utiliser des points de contr\u00f4le (checkpoints) pour sauvegarder le travail, par exemple en divisant un entra\u00eenement de trois jours en trois blocs de 24 heures."
  },
  {
    "question": "Quels sont les avantages d'utiliser des points de contr\u00f4le pour les calculs de longue dur\u00e9e?",
    "answer": "L'utilisation de points de contr\u00f4le permet d'\u00e9viter la perte de travail en cas de panne et peut b\u00e9n\u00e9ficier d'une meilleure priorisation des t\u00e2ches, car plus de n\u0153uds sont r\u00e9serv\u00e9s pour les t\u00e2ches courtes."
  },
  {
    "question": "Que dois-je faire si ma biblioth\u00e8que d'apprentissage machine ne prend pas en charge les points de contr\u00f4le?",
    "answer": "Si votre programme ne prend pas en charge les points de contr\u00f4le, vous devriez consulter la [[Points de contr\u00f4le|solution g\u00e9n\u00e9rique]]."
  },
  {
    "question": "O\u00f9 puis-je trouver des exemples de cr\u00e9ation de points de contr\u00f4le pour PyTorch et TensorFlow?",
    "answer": "Des exemples sont disponibles dans [[PyTorch/fr#Cr\u00e9er_des_points_de_contr\u00f4le|Points de contr\u00f4le PyTorch]] et [[TensorFlow/fr#Cr\u00e9er_des_points_de_contr\u00f4le|Points de contr\u00f4le TensorFlow]]."
  },
  {
    "question": "Comment g\u00e9rer l'ex\u00e9cution de plusieurs t\u00e2ches similaires, comme la recherche d'hyperparam\u00e8tres ou l'entra\u00eenement de variantes?",
    "answer": "Vous devriez grouper plusieurs t\u00e2ches pour n'en former qu'une seule avec un outil comme [[META-Farm/fr|META]], [[GLOST/fr|GLOST]] ou [[GNU Parallel/fr|GNU Parallel]]."
  },
  {
    "question": "Quels outils peuvent aider \u00e0 optimiser l'allocation de calcul et \u00e0 suivre les exp\u00e9rimentations?",
    "answer": "[[Weights & Biases (wandb)/fr|Weights & Biases (wandb)]] et [[Comet.ml/fr|Comet.ml]] peuvent aider en facilitant le suivi et l'analyse des processus d'apprentissage et en permettant l'optimisation bay\u00e9sienne des hyperparam\u00e8tres."
  },
  {
    "question": "Weights & Biases (wandb) et Comet.ml sont-ils disponibles sur Graham?",
    "answer": "Non, Comet et Wandb ne sont pas disponibles pr\u00e9sentement sur Graham."
  },
  {
    "question": "O\u00f9 puis-je trouver des informations sur la scalabilit\u00e9 des m\u00e9thodes classiques d'apprentissage machine pour de grands ensembles de donn\u00e9es?",
    "answer": "Vous pouvez consulter la page wiki [[Large_Scale_Machine_Learning_(Big_Data)/fr|Apprentissage machine \u00e0 grande \u00e9chelle (m\u00e9gadonn\u00e9es)]]."
  },
  {
    "question": "Quel probl\u00e8me non d\u00e9terministe peut survenir avec cuDNN dans CUDA Toolkit 10.2 et plus?",
    "answer": "Un comportement non d\u00e9terministe peut survenir dans les r\u00e9seaux de neurones r\u00e9currents (RNN) et les appels \u00e0 l\u2019API d\u2019auto-attention multit\u00eates lorsque la biblioth\u00e8que cuDNN est pr\u00e9sente dans CUDA Toolkit versions 10.2 et plus."
  },
  {
    "question": "Comment puis-je \u00e9viter le comportement non d\u00e9terministe dans les RNN avec CUDA Toolkit 10.2+ et cuDNN?",
    "answer": "Pour \u00e9viter ce probl\u00e8me, vous pouvez configurer la variable d\u2019environnement CUBLAS_WORKSPACE_CONFIG avec une seule taille pour la m\u00e9moire tampon, par exemple `:16:8` ou `:4096:2`."
  }
]