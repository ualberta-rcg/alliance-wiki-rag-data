[
  {
    "question": "What is the CPU configuration for the 2 nodes with 6000G of available memory?",
    "answer": "These 2 nodes are equipped with 4 x Intel Gold 5215 Cascade Lake @ 2.5GHz CPUs."
  },
  {
    "question": "How many nodes on Cedar are configured with NVIDIA P100 Pascal (12G HBM2 memory) GPUs?",
    "answer": "There are 96 nodes that feature 4 x NVIDIA P100 Pascal (12G HBM2 memory) GPUs."
  },
  {
    "question": "What is the CPU type and speed for the 32 nodes that have 250G of available memory and P100 Pascal GPUs?",
    "answer": "These 32 nodes use 2 x Intel E5-2650 v4 Broadwell @ 2.2GHz CPUs."
  },
  {
    "question": "How many NVIDIA V100 Volta (32G HBM2 memory) GPUs are in each of the 192 nodes, and what is the CPU model?",
    "answer": "Each of the 192 nodes contains 4 x NVIDIA V100 Volta (32G HBM2 memory) GPUs and runs on 2 x Intel Silver 4216 Cascade Lake @ 2.1GHz CPUs."
  },
  {
    "question": "What kind of CPUs are used in the 608 nodes with 48 cores and no GPUs?",
    "answer": "The 608 nodes utilize 2 x Intel Platinum 8160F Skylake @ 2.1GHz CPUs."
  },
  {
    "question": "What is the available memory and CPU configuration for the 768 nodes with 48 cores?",
    "answer": "These 768 nodes have 187G (192000M) of available memory and are equipped with 2 x Intel Platinum 8260 Cascade Lake @ 2.4GHz CPUs."
  },
  {
    "question": "Why is the available memory on Cedar nodes typically less than the advertised 'round number' of the hardware?",
    "answer": "The available memory is less than the round number because a portion of it is permanently occupied by the kernel and the operating system."
  },
  {
    "question": "What is the scheduler's policy regarding memory allocation to prevent swapping?",
    "answer": "The scheduler will not allocate jobs whose memory requirements exceed the amount of available memory to avoid wasting time by swapping/paging."
  },
  {
    "question": "What local temporary storage is available on compute nodes (excluding GPU nodes)?",
    "answer": "Compute nodes (except GPU nodes) have two 480GB SSD drives, providing a total raw capacity of 960GB for local temporary storage."
  },
  {
    "question": "How much local temporary storage do GPU nodes typically have?",
    "answer": "GPU nodes have either an 800GB or a 480GB SSD drive for local temporary storage."
  },
  {
    "question": "Which environment variable should be used to access node-local storage for a job?",
    "answer": "Node-local storage should be accessed through the job-specific directory created by the scheduler, which is `$SLURM_TMPDIR`."
  },
  {
    "question": "Are there any node types reserved for jobs requiring exclusive access to whole nodes?",
    "answer": "Yes, a number of 48-core nodes are reserved for jobs that require whole nodes."
  },
  {
    "question": "What happens if a job requests fewer than 48 cores per node?",
    "answer": "Jobs that request less than 48 cores per node can end up sharing nodes with other jobs."
  },
  {
    "question": "Is it recommended to specify a particular node type (e.g., Broadwell, Skylake) for jobs on Cedar?",
    "answer": "It is recommended not to select a specific node type for jobs, as performance differences are generally small compared to job waiting times."
  },
  {
    "question": "How can a user constrain their job to run on a Cascade Lake node?",
    "answer": "To constrain a job to a Cascade Lake node, a user can use `--constraint=cascade`."
  },
  {
    "question": "What constraint should be used if a job requires any AVX512 node?",
    "answer": "For any AVX512 node, the constraint `--constraint=[skylake|cascade]` should be used."
  },
  {
    "question": "What was the significant policy change for job submission on Cedar as of April 17, 2019?",
    "answer": "As of April 17, 2019, jobs can no longer run from the `/home` filesystem."
  },
  {
    "question": "Why was the policy to disallow job submission from the /home filesystem implemented?",
    "answer": "This policy was implemented to reduce the load on the `/home` filesystem and improve its responsiveness for interactive work."
  },
  {
    "question": "What should a user do if they receive a message stating 'Submitting jobs from directories residing in /home is not permitted'?",
    "answer": "Users should transfer their files to either their `/project` or `/scratch` directory and submit the job from there."
  },
  {
    "question": "What is Cedar's theoretical peak double precision performance for CPUs?",
    "answer": "Cedar's theoretical peak double precision performance for CPUs is 6547 teraflops."
  },
  {
    "question": "What is Cedar's theoretical peak double precision performance for GPUs?",
    "answer": "Cedar's theoretical peak double precision performance for GPUs is 7434 teraflops."
  },
  {
    "question": "What is the approximate total theoretical peak double precision performance of Cedar?",
    "answer": "Cedar has almost 14 petaflops of theoretical peak double precision performance in total."
  },
  {
    "question": "How does the network topology's blocking factor work on Cedar?",
    "answer": "Cedar's network topology has a 2:1 blocking factor between islands, but within an island, the Omni-Path fabric interconnect is fully non-blocking."
  },
  {
    "question": "How many Broadwell nodes and cores are typically found within one of Cedar's Broadwell islands?",
    "answer": "Most islands contain 32 Broadwell nodes, each with 32 cores, totaling 1024 cores per island."
  },
  {
    "question": "How many cores are available in an island composed of 32 Skylake or Cascade Lake nodes?",
    "answer": "An island with 32 Skylake or Cascade Lake nodes, each having 48 cores, offers 1536 cores per island."
  },
  {
    "question": "How many islands are dedicated to P100 GPU nodes?",
    "answer": "There are 4 islands that are dedicated to P100 GPU nodes."
  },
  {
    "question": "What is the total number of V100 GPU nodes across all dedicated V100 GPU islands?",
    "answer": "There are 6 islands with 32 V100 GPU nodes each, totaling 192 V100 GPU nodes."
  },
  {
    "question": "How many big memory nodes are distributed across the two dedicated big memory islands?",
    "answer": "There are 64 big memory nodes distributed across two islands."
  },
  {
    "question": "What are the various memory capacities of the 64 big memory nodes?",
    "answer": "The 64 big memory nodes include 40 nodes of 0.5TB, 16 nodes of 1.5TB, 6 nodes of 4TB, and 2 nodes of 6TB."
  }
]