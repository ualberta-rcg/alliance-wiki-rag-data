[
  {
    "question": "What type of processors are used in the CPU nodes of the Fir cluster, and what is their clock speed?",
    "answer": "Each CPU node features 2 \u00d7 AMD EPYC 9655 (Zen 5) processors operating at 2.7 GHz."
  },
  {
    "question": "How many physical cores are available on each CPU node?",
    "answer": "Each CPU node has a total of 192 physical cores."
  },
  {
    "question": "Describe the architecture of the CPU nodes regarding NUMA and chiplets.",
    "answer": "The CPU nodes are built on a chiplet-based NUMA architecture, where each chiplet (CCD) functions as a separate NUMA node. Memory and cache hierarchy are non-uniform, making performance sensitive to data locality."
  },
  {
    "question": "How many sockets does a CPU node have, and how many cores are on each socket?",
    "answer": "Each CPU node has 2 sockets, with each socket containing 96 cores."
  },
  {
    "question": "What is the L3 cache configuration within a CCD on a CPU node?",
    "answer": "Each CCD (chiplet) on a CPU node contains 8 cores and has 32 MiB of shared L3 cache."
  },
  {
    "question": "What are the L1 and L2 cache sizes per core on a CPU node?",
    "answer": "Each core on a CPU node has 1 MiB L2 cache and 32+32 KiB L1 instruction/data cache."
  },
  {
    "question": "How many DDR5 memory channels are available per core on a CPU node?",
    "answer": "Each core on a CPU node has 12 DDR5 memory channels, shared via the I/O die."
  },
  {
    "question": "What is the total number of NUMA nodes and L3 cache size per CPU node?",
    "answer": "Each CPU node has a total of 24 NUMA nodes (12 per socket \u00d7 2) and 768 MiB of total L3 cache."
  },
  {
    "question": "What Slurm option is recommended to align tasks to CCDs on CPU nodes for optimal performance?",
    "answer": "To align tasks to CCDs and keep threads within a single CCD, use `#SBATCH --cpus-per-task=8`."
  },
  {
    "question": "How many tasks per node should be launched on CPU nodes to fully utilize all CCDs without overloading any single NUMA node?",
    "answer": "With 24 NUMA domains per node, it is recommended to launch 24 tasks per node. This can be achieved using `#SBATCH --ntasks-per-node=24`."
  },
  {
    "question": "What Slurm options should be used together to fully utilize a 192-core CPU node cleanly?",
    "answer": "To fully utilize a 192-core CPU node cleanly, use `#SBATCH --cpus-per-task=8` and `#SBATCH --ntasks-per-node=24`."
  },
  {
    "question": "What processor is found in each GPU node and at what clock speed?",
    "answer": "Each GPU node contains 1 \u00d7 AMD EPYC 9454 (Zen 4) processor running at 2.75 GHz."
  },
  {
    "question": "How many physical cores are in a GPU node?",
    "answer": "Each GPU node has 48 physical cores."
  },
  {
    "question": "How is the socket configured on GPU nodes in terms of CCDs?",
    "answer": "The single socket on GPU nodes is configured with 6 CCDs (Core Complex Dies)."
  },
  {
    "question": "What is the L3 cache configuration for each CCD on a GPU node?",
    "answer": "Each CCD on a GPU node contains 8 cores and 32 MiB of shared L3 cache."
  },
  {
    "question": "What are the L1 and L2 cache sizes per core on a GPU node?",
    "answer": "Each core on a GPU node has 1 MiB L2 cache, 32 KiB L1 instruction cache, and 32 KiB L1 data cache."
  },
  {
    "question": "How many DDR5 memory channels are available per core on a GPU node?",
    "answer": "Each core on a GPU node has 12 DDR5 memory channels."
  },
  {
    "question": "What is NPS=4 mode on GPU nodes and how does it affect NUMA node configuration?",
    "answer": "NPS=4 mode on GPU nodes means the socket is split into 4 NUMA nodes. Each of these NUMA nodes has 12 cores (1.5 CCDs per node) and 3 memory channels."
  },
  {
    "question": "How many NVIDIA H100 80GB accelerators are present in each GPU node?",
    "answer": "Each GPU node contains 2 NVidia H100 80GB accelerators."
  },
  {
    "question": "What Slurm option is recommended to bind threads to CCDs on GPU nodes for optimal performance?",
    "answer": "To bind threads to CCDs and keep them within one CCD, use `#SBATCH --cpus-per-task=8`."
  },
  {
    "question": "What Slurm options are recommended to match tasks to NUMA nodes on GPU nodes for best performance and optimal CPU-GPU data locality?",
    "answer": "For best performance and optimal CPU-GPU data locality, launch 4 tasks per node (or a multiple thereof) using `#SBATCH --ntasks-per-node=4` and `#SBATCH --cpus-per-task=12`."
  },
  {
    "question": "What Slurm option should be used to request a single full H100-80gb GPU?",
    "answer": "To request one H100-80gb GPU, use the Slurm option `--gpus=h100:1`."
  },
  {
    "question": "How can you request 2, 3, or 4 full H100-80gb GPUs on a single node?",
    "answer": "To request multiple H100-80gb GPUs per node, use `--gpus-per-node=h100:2`, `--gpus-per-node=h100:3`, or `--gpus-per-node=h100:4` respectively."
  },
  {
    "question": "What Slurm option allows requesting multiple full H100 GPUs spread across the cluster?",
    "answer": "To request multiple full H100 GPUs spread anywhere, use `--gpus=h100:n`, replacing 'n' with the desired number of GPUs."
  },
  {
    "question": "Which GPU nodes are configured with MIG technology?",
    "answer": "Approximately half of the GPU nodes are configured with MIG technology."
  },
  {
    "question": "What are the available GPU instance sizes when using MIG technology, and their corresponding memory?",
    "answer": "The available GPU instance sizes are: 1g.10gb (1/8th computing power with 10GB GPU memory), 2g.20gb (2/8th computing power with 20GB GPU memory), and 3g.40gb (3/8th computing power with 40GB GPU memory)."
  },
  {
    "question": "What Slurm option is used to request a single 1g.10gb GPU instance?",
    "answer": "To request one 1g.10gb GPU instance, use `--gpus=nvidia_h100_80gb_hbm3_1g.10gb:1`."
  },
  {
    "question": "What Slurm option is used to request a single 2g.20gb GPU instance?",
    "answer": "To request one 2g.20gb GPU instance, use `--gpus=nvidia_h100_80gb_hbm3_2g.20gb:1`."
  },
  {
    "question": "What Slurm option is used to request a single 3g.40gb GPU instance?",
    "answer": "To request one 3g.40gb GPU instance, use `--gpus=nvidia_h100_80gb_hbm3_3g.40gb:1`."
  }
]