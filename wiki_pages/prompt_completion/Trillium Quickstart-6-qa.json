[
  {
    "question": "What are the allowed GPU request options on Trillium's GPU subcluster?",
    "answer": "You are only allowed to request exactly 1 GPU or a multiple of 4 GPUs. You cannot request --gpus-per-node=2 or 3."
  },
  {
    "question": "Does Trillium's GPU subcluster support NVIDIA's MIG technology?",
    "answer": "No, Trillium does not support NVIDIA's MIG technology to allocate a subdivision of a GPU."
  },
  {
    "question": "How can processes within the same job share a GPU on Trillium?",
    "answer": "Inside a job, you can use NVIDIA's Multi-Process Service (MPS) to share a GPU among processes running on the same job."
  },
  {
    "question": "What is the recommended `--gpus-per-node` option for single-GPU jobs?",
    "answer": "For single-GPU jobs, use `--gpus-per-node=1`."
  },
  {
    "question": "What is the recommended `--gpus-per-node` option for whole-node GPU jobs?",
    "answer": "For whole-node GPU jobs, use `--gpus-per-node=4`."
  },
  {
    "question": "What is the maximum number of running GPU compute jobs allowed in the 'compute' partition?",
    "answer": "The limit for running GPU compute jobs in the 'compute' partition is 150."
  },
  {
    "question": "What is the maximum number of submitted GPU compute jobs (including running) allowed in the 'compute' partition?",
    "answer": "The limit for submitted GPU compute jobs (including running) in the 'compute' partition is 500."
  },
  {
    "question": "What is the minimum job size for GPU compute jobs in the 'compute' partition?",
    "answer": "The minimum job size for GPU compute jobs in the 'compute' partition is 1/4 node (24 cores / 1 GPU)."
  },
  {
    "question": "What is the default maximum job size for GPU compute jobs in the 'compute' partition?",
    "answer": "The default maximum job size for GPU compute jobs in the 'compute' partition is 5 nodes (480 cores / 20 GPUs)."
  },
  {
    "question": "What is the maximum job size for GPU compute jobs in the 'compute' partition for users with an allocation?",
    "answer": "With an allocation, the maximum job size for GPU compute jobs in the 'compute' partition is 25 nodes (2400 cores / 100 GPUs)."
  },
  {
    "question": "What is the maximum walltime for GPU compute jobs in the 'compute' partition?",
    "answer": "The maximum walltime for GPU compute jobs in the 'compute' partition is 24 hours."
  },
  {
    "question": "What are the limits on running and submitted jobs for testing GPU jobs in the 'debug' partition?",
    "answer": "For testing GPU jobs in the 'debug' partition, the limit on running jobs is 1, and the limit on submitted jobs (including running) is also 1."
  },
  {
    "question": "What is the maximum job size for testing GPU jobs in the 'debug' partition?",
    "answer": "The maximum job size for testing GPU jobs in the 'debug' partition is 2 nodes (192 cores / 8 GPUs)."
  },
  {
    "question": "What is the maximum walltime for a single-GPU testing job in the 'debug' partition?",
    "answer": "The maximum walltime for a single-GPU testing job in the 'debug' partition is 2 hours (1 GPU)."
  },
  {
    "question": "What is the maximum walltime for an 8-GPU testing job in the 'debug' partition?",
    "answer": "The maximum walltime for an 8-GPU testing job in the 'debug' partition is 30 minutes (8 GPUs)."
  },
  {
    "question": "What factors affect the waiting time for jobs in the queue on Trillium?",
    "answer": "The waiting time depends on your group's allocation amount, how much allocation has been used in the recent past, the number of requested nodes and walltime, and how many other jobs are waiting in the queue."
  },
  {
    "question": "What modules are loaded in the example for a single-GPU Python job?",
    "answer": "The example for a single-GPU Python job loads `StdEnv/2023`, `cuda/12.6`, and `python/3.11.5` modules."
  },
  {
    "question": "What command is used to check GPU allocation within a job script?",
    "answer": "The command `srun nvidia-smi` is used to check GPU allocation within a job script."
  },
  {
    "question": "What is a best practice regarding the `--mem` option for GPU jobs?",
    "answer": "Do not use `--mem` because memory is fixed per GPU (192 GB) or per node (768 GB)."
  },
  {
    "question": "What is a best practice for software versions in GPU job scripts?",
    "answer": "Be explicit with software versions for reproducibility (e.g., `cuda/12.6` rather than just `cuda`)."
  },
  {
    "question": "What should you do before scaling a GPU job to multiple GPUs or nodes?",
    "answer": "Test on a single GPU before scaling to multiple GPUs or nodes."
  },
  {
    "question": "How can you monitor GPU usage to ensure full utilization?",
    "answer": "Monitor usage with `nvidia-smi` to ensure GPUs are fully utilized."
  }
]