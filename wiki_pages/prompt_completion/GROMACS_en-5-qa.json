[
  {
    "question": "What is the SLURM configuration for a whole-node GROMACS job on Rorqual, Fir, Nibi, or Trillium regarding nodes, tasks, and CPUs?",
    "answer": "The configuration requests 1 node, 96 MPI tasks per node, and 2 OpenMP threads per MPI task, totaling 192 CPUs per node."
  },
  {
    "question": "What is the memory and time limit specified for a GROMACS whole-node job on systems like Rorqual, Fir, Nibi, or Trillium?",
    "answer": "The memory limit is 2000M per CPU, and the time limit is 0-01:00 (1 hour)."
  },
  {
    "question": "Which modules must be loaded for a GROMACS whole-node job on Rorqual, Fir, Nibi, or Trillium?",
    "answer": "The required modules are `StdEnv/2023 gcc/12.3 openmpi/4.1.5 gromacs/2024.4`."
  },
  {
    "question": "What GROMACS command is executed in the whole-node job scripts for Rorqual, Fir, Nibi, and Trillium?",
    "answer": "The command is `srun --cpus-per-task=$OMP_NUM_THREADS gmx_mpi mdrun -deffnm md`."
  },
  {
    "question": "How many GPUs are requested per node in the provided GROMACS GPU job script?",
    "answer": "The script requests 1 GPU per node (`#SBATCH --gpus-per-node=1`)."
  },
  {
    "question": "What is the number of OpenMP threads per MPI process for the GROMACS GPU job script?",
    "answer": "The GPU job script specifies 4 OpenMP threads per MPI process (`#SBATCH --cpus-per-task=4`)."
  },
  {
    "question": "What are the memory and time limits for the GROMACS GPU job script?",
    "answer": "The memory limit is 2000M per CPU core, and the time limit is 0:30:00 (30 minutes)."
  },
  {
    "question": "Which modules are necessary for running the GROMACS GPU job script?",
    "answer": "The necessary modules are `StdEnv/2023 gcc/12.3 openmpi/4.1.5 cuda/12.2 gromacs/2024.4`."
  },
  {
    "question": "What GROMACS command is used to run a simulation with GPU acceleration according to the GPU job script?",
    "answer": "The command is `gmx mdrun -ntomp ${SLURM_CPUS_PER_TASK:-1} -deffnm md`."
  },
  {
    "question": "How is the `OMP_NUM_THREADS` environment variable set in the provided whole-node and GPU job scripts?",
    "answer": "The `OMP_NUM_THREADS` environment variable is set by exporting it with the value of `${SLURM_CPUS_PER_TASK:-1}`."
  }
]