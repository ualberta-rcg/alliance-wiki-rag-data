[
  {
    "question": "When is the tamIA cluster available?",
    "answer": "The tamIA cluster is available starting March 31, 2025."
  },
  {
    "question": "What is the login node for tamIA?",
    "answer": "The login node for tamIA is tamia.alliancecan.ca."
  },
  {
    "question": "What is the Globus collection identifier for tamIA?",
    "answer": "The Globus collection is 'TamIA's Globus v5 Server'."
  },
  {
    "question": "Which node should be used for data transfers via rsync, scp, or sftp on tamIA?",
    "answer": "For data transfers via rsync, scp, or sftp, the data transfer node is tamia.alliancecan.ca."
  },
  {
    "question": "What is the URL for the tamIA portal?",
    "answer": "The URL for the tamIA portal is https://portail.tamia.ecpia.ca/."
  },
  {
    "question": "What is tamIA?",
    "answer": "tamIA is a cluster dedicated to artificial intelligence for the Canadian scientific community."
  },
  {
    "question": "Where is the tamIA cluster located?",
    "answer": "The tamIA cluster is located at Universit\u00e9 Laval."
  },
  {
    "question": "Who co-manages the tamIA cluster?",
    "answer": "tamIA is co-managed with Mila and Calcul Qu\u00e9bec."
  },
  {
    "question": "What is the tamIA cluster named after?",
    "answer": "The cluster is named after the eastern chipmunk."
  },
  {
    "question": "What is PAICE?",
    "answer": "PAICE is the Pan-Canadian AI Compute Environment."
  },
  {
    "question": "Is tamIA part of PAICE?",
    "answer": "Yes, tamIA is part of PAICE, the Pan-Canadian AI Compute Environment."
  },
  {
    "question": "Can tamIA's compute nodes access the internet?",
    "answer": "By policy, tamIA's compute nodes cannot access the internet. Exceptions require contacting technical support with an explanation."
  },
  {
    "question": "Is `crontab` available on tamIA?",
    "answer": "No, `crontab` is not offered on tamIA."
  },
  {
    "question": "What is the minimum job duration on tamIA?",
    "answer": "Each job should be at least one hour long, but test jobs are allowed a minimum of five minutes."
  },
  {
    "question": "What is the maximum number of jobs a user can have running and pending simultaneously on tamIA?",
    "answer": "Users cannot have more than 1000 jobs (running and pending) at a time."
  },
  {
    "question": "What is the maximum duration for a job on tamIA?",
    "answer": "The maximum duration of a job is one day (24 hours)."
  },
  {
    "question": "How must GPUs be utilized by jobs on tamIA?",
    "answer": "Each job must use all 4 GPUs on all nodes it is allocated, as jobs are allocated whole nodes."
  },
  {
    "question": "How do researchers gain access to the tamIA cluster?",
    "answer": "Researchers must complete an access request in the CCDB to gain access to the tamIA cluster."
  },
  {
    "question": "How long does it typically take to get access to tamIA after an access request is completed?",
    "answer": "Access to the cluster may take up to one hour after completing the access request."
  },
  {
    "question": "Who are considered eligible principal investigators for tamIA access?",
    "answer": "Eligible principal investigators are members of an AIP-type RAP (prefix `aip-`)."
  },
  {
    "question": "What is the procedure for a principal investigator to sponsor other researchers for tamIA access?",
    "answer": "The procedure is: go to the CCDB home page, navigate to the 'Resource Allocation Projects' table, find and click the RAPI of the `aip-` project, then click 'Manage RAP memberships', go to 'Add Members', and enter the CCRI of the user to be added."
  },
  {
    "question": "From where is the tamIA cluster reachable?",
    "answer": "The cluster is only reachable from Canada."
  },
  {
    "question": "What is the purpose of the HOME file system on tamIA?",
    "answer": "The HOME file system is for home directories, each having a small fixed quota. For larger storage needs, the `project` space should be used."
  },
  {
    "question": "Does the HOME file system on tamIA have backups?",
    "answer": "There is currently no backup of the home directories on the HOME file system, but it is estimated to be available by Summer 2025."
  },
  {
    "question": "What is the SCRATCH file system used for on tamIA?",
    "answer": "The SCRATCH file system is a large space for storing temporary files during computations."
  },
  {
    "question": "Are files on the SCRATCH file system backed up on tamIA?",
    "answer": "No, there is no backup system in place for the SCRATCH file system."
  },
  {
    "question": "Is there a purging policy for files on the SCRATCH file system?",
    "answer": "Yes, there is an automated purge of older files in the SCRATCH space."
  },
  {
    "question": "What is the purpose of the PROJECT file system on tamIA?",
    "answer": "The PROJECT file system is designed for sharing data among members of a research group and for storing large amounts of data."
  },
  {
    "question": "What type of quota does the PROJECT file system have?",
    "answer": "The PROJECT file system has a large and adjustable per-group quota."
  },
  {
    "question": "What high-performance interconnect technology does tamIA use?",
    "answer": "tamIA uses an InfiniBand NVIDIA NDR network to link all nodes."
  },
  {
    "question": "How are H100 GPUs connected to the InfiniBand network on tamIA?",
    "answer": "Each H100 GPU is connected to a single NDR200 port through an NVIDIA ConnectX-7 HCA."
  },
  {
    "question": "How many NDR200 ports does each GPU server on tamIA have connected to the InfiniBand fabric?",
    "answer": "Each GPU server has 4 NDR200 ports connected to the InfiniBand fabric."
  },
  {
    "question": "What is the network topology of tamIA's InfiniBand network?",
    "answer": "The InfiniBand network is non-blocking for compute servers and is composed of two levels of switches in a fat-tree topology."
  },
  {
    "question": "How are storage and management nodes connected to the tamIA network core?",
    "answer": "Storage and management nodes are connected via four 400Gb/s connections to the network core."
  },
  {
    "question": "What are the characteristics of tamIA's GPU-enabled compute nodes?",
    "answer": "tamIA has 42 GPU-enabled nodes, each with 48 cores, 512GB of available memory, 2 Intel Xeon Gold 6442Y 2.6 GHz (24C) CPUs, 1 x 7.68TB SSD, and 4 NVIDIA HGX H100 SXM 80GB HBM3 700W GPUs connected via NVLink."
  },
  {
    "question": "What are the characteristics of tamIA's CPU-only compute nodes?",
    "answer": "tamIA has 4 CPU-only nodes, each with 64 cores, 512GB of available memory, 2 Intel Xeon Gold 6438M 2.2 GHz (32C/64T) CPUs, and 1 x 7.68TB SSD, with no GPUs."
  },
  {
    "question": "What is the standard software environment on tamIA?",
    "answer": "The standard software environment on tamIA is `StdEnv/2023`."
  },
  {
    "question": "How can users monitor their jobs on tamIA?",
    "answer": "Users can monitor their jobs using CPUs and GPUs in real time, or examine past jobs, from the tamIA portal."
  },
  {
    "question": "What specific resources can be monitored for jobs on tamIA?",
    "answer": "Users can monitor their usage of compute nodes, memory, and GPU."
  },
  {
    "question": "Why is it important to monitor job resource usage on tamIA?",
    "answer": "Monitoring job resource usage helps optimize resource usage, shorten wait times in the queue, and is important for correcting resource requests when compute resources are underutilized."
  }
]