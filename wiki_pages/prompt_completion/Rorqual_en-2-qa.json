[
  {
    "question": "What are the storage specifications for the 8 nodes with 3013G of memory?",
    "answer": "The 8 nodes with 3013G (or 3086250M) of available memory have 1 x SATA SSD, 480G (6Gbit/s) storage."
  },
  {
    "question": "What CPU and GPU hardware do the 81 nodes feature?",
    "answer": "The 81 nodes feature 2 x Intel Xeon Gold 6448Y @ 2.10 GHz CPUs and 4 x NVidia H100 SXM5 (80GB) GPUs."
  },
  {
    "question": "How much memory and cores do the 81 nodes possess?",
    "answer": "The 81 nodes have 64 cores and 498G (or 510000M) of available memory."
  },
  {
    "question": "What is the storage type and size for the 81 nodes?",
    "answer": "The 81 nodes are equipped with 1 x NVMe SSD, 3.84TB storage."
  },
  {
    "question": "What is the method to request a larger `$SLURM_TMPDIR` space for a job?",
    "answer": "A job can be submitted with `--tmp=xG` to get a larger `$SLURM_TMPDIR` space, where `x` is a value between 370 and 3360."
  },
  {
    "question": "How many sockets do CPU nodes have and how many system memory channels are associated with each socket?",
    "answer": "Each CPU node has 2 sockets, and each socket has 12 system memory channels."
  },
  {
    "question": "How many NUMA nodes are present per socket on a CPU node?",
    "answer": "There are 4 NUMA nodes per socket on a CPU node."
  },
  {
    "question": "How many system memory channels are connected to each NUMA node on a CPU node?",
    "answer": "Each NUMA node on a CPU node is connected to 3 system memory channels."
  },
  {
    "question": "What is the L3 cache memory size for each chiplet within a NUMA node on a CPU node?",
    "answer": "Each of the 3 chiplets per NUMA node has its own 32 MiB L3 cache memory."
  },
  {
    "question": "What are the L1 and L2 cache specifications for the cores within a chiplet on a CPU node?",
    "answer": "Each of the 8 cores per chiplet has its own 1 MiB L2 cache memory and 32+32 KiB L1 cache memory."
  },
  {
    "question": "What core configuration is optimal for multithreaded parallel programs on CPU nodes?",
    "answer": "Groups of 8 closely spaced cores sharing a single L3 cache are ideal for multithreaded parallel programs."
  },
  {
    "question": "How many cores are found within a NUMA node on a CPU node?",
    "answer": "NUMA nodes consist of 3x8 = 24 cores."
  },
  {
    "question": "What is the total number of cores available per CPU node?",
    "answer": "A CPU node has a total of 192 cores."
  },
  {
    "question": "What steps should be taken to fully utilize the CPU node topology?",
    "answer": "To fully benefit from the CPU node topology, full nodes must be reserved (e.g., with `--ntasks-per-node=24 --cpus-per-task=8`) and the place of processes and threads must be explicitly controlled."
  },
  {
    "question": "How many system memory channels and L3 cache memory does each socket in a GPU node have?",
    "answer": "Each socket in a GPU node has 8 system memory channels and 60 MiB L3 cache memory."
  },
  {
    "question": "How many equidistant cores are there per socket in a GPU node, and what are their L1 and L2 cache sizes?",
    "answer": "Each socket in a GPU node has 32 equidistant cores, each with its own 2 MiB L2 cache memory and 32+48 KiB L1 cache memory."
  },
  {
    "question": "How many NVidia H100 accelerators are equipped per socket in a GPU node?",
    "answer": "Each socket in a GPU node is equipped with 2 NVidia H100 accelerators."
  },
  {
    "question": "By what technology are the 4 node accelerators in GPU nodes interconnected?",
    "answer": "The 4 node accelerators in GPU nodes are interconnected by SXM5."
  },
  {
    "question": "What are the short names for the H100-80gb GPU instance?",
    "answer": "The short names for the H100-80gb GPU instance are `h100` (without unit) and `h100_80gb` (by memory)."
  },
  {
    "question": "What Slurm options are used to request a single H100-80gb GPU?",
    "answer": "To request one H100-80gb GPU, use `--gpus=h100:1` or `--gpus=h100_80gb:1`."
  },
  {
    "question": "How can a user request 2, 3, or 4 H100-80gb GPUs per node using Slurm?",
    "answer": "To request multiple H100-80gb GPUs per node, use `--gpus-per-node=h100:2`, `--gpus-per-node=h100:3`, or `--gpus-per-node=h100:4` respectively."
  },
  {
    "question": "What Slurm option allows requesting multiple full H100 GPUs spread across the cluster?",
    "answer": "To request multiple full H100 GPUs spread anywhere, use the Slurm option `--gpus=h100:n`, replacing `n` with the desired number of GPUs."
  },
  {
    "question": "What proportion of GPU nodes are configured with Multi-Instance GPU (MIG) technology?",
    "answer": "Approximately half of the GPU nodes are configured with Multi-Instance GPU (MIG) technology."
  },
  {
    "question": "What are the three available GPU instance sizes for MIG technology?",
    "answer": "The three available GPU instance sizes are H100-1g.10gb, H100-2g.20gb, and H100-3g.40gb."
  },
  {
    "question": "What are the computing power and memory specifications for an H100-1g.10gb MIG instance?",
    "answer": "An H100-1g.10gb MIG instance offers 1/8th of the computing power with 10GB GPU memory."
  },
  {
    "question": "How much computing power and GPU memory does an H100-2g.20gb MIG instance provide?",
    "answer": "An H100-2g.20gb MIG instance provides 2/8th of the computing power with 20GB GPU memory."
  },
  {
    "question": "What is the computing power and memory for an H100-3g.40gb MIG instance?",
    "answer": "An H100-3g.40gb MIG instance offers 3/8th of the computing power with 40GB GPU memory."
  },
  {
    "question": "Which Slurm option is used to request a single H100-1g.10gb GPU instance?",
    "answer": "To request one H100-1g.10gb GPU instance, use the Slurm option `--gpus=h100_1g.10gb:1`."
  },
  {
    "question": "How do you request a single H100-2g.20gb GPU instance for a compute job?",
    "answer": "To request a single H100-2g.20gb GPU instance, use the Slurm option `--gpus=h100_2g.20gb:1`."
  },
  {
    "question": "What is the Slurm option for requesting one H100-3g.40gb GPU instance?",
    "answer": "The Slurm option for requesting one H100-3g.40gb GPU instance is `--gpus=h100_3g.40gb:1`."
  }
]