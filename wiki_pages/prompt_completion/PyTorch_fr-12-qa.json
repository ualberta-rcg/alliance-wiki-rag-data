[
  {
    "question": "What is the purpose of 'ZeRO avec CPU' in the context of DeepSpeed?",
    "answer": "ZeRO avec CPU uses DeepSpeed's Zero Redundancy Optimizer (ZeRO) stage 3 to offload optimizer states and model parameters to the CPU, making the compute node's memory available for tensors when not required by the GPU."
  },
  {
    "question": "What are the practical advantages of using ZeRO Stage 3 with CPU offloading?",
    "answer": "Practically, it's like adding 32GB of additional memory to the GPU. This reduces the load on GPU memory, allowing users to increase batch sizes or model sizes."
  },
  {
    "question": "Which optimizer is recommended for ZeRO Stage 3 with CPU offloading?",
    "answer": "The `DeepSpeedCPUAdam` optimizer is recommended instead of native PyTorch optimizers for ZeRO Stage 3 with CPU offloading."
  },
  {
    "question": "What is the performance comparison of ZeRO Stage 3 with CPU offloading to pure data parallelism?",
    "answer": "The performance of ZeRO Stage 3 with CPU offloading using `DeepSpeedCPUAdam` is comparable to pure data parallelism."
  },
  {
    "question": "What module must be loaded when utilizing DeepSpeed optimizers with CPU offloading?",
    "answer": "A `cuda/<version>` module must be loaded, where 'version' corresponds to the CUDA version used to compile the PyTorch package you are using."
  },
  {
    "question": "What SLURM resources are requested in the `deepspeed-stage3-offload-cpu.sh` script?",
    "answer": "The script requests 1 node, 2 GPUs (`--gres=gpu:2`), 2 tasks per node (`--tasks-per-node=2`), 32GB of memory (`--mem=32G`), and a time limit of 20 minutes (`--time=0-00:20`)."
  },
  {
    "question": "How is the PyTorch Lightning Trainer configured for ZeRO Stage 3 with CPU offloading in the `deepspeed-stage3-offload-cpu.py` script?",
    "answer": "The Trainer is initialized with `accelerator=\"gpu\"`, `devices=2`, `num_nodes=1`, and `strategy=DeepSpeedStrategy(stage=3, offload_optimizer=True, offload_parameters=True)`."
  }
]