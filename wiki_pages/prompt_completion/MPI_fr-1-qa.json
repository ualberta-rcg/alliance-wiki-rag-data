[
  {
    "question": "What analogy is used to illustrate parallel programming?",
    "answer": "The document uses the analogy: 'To pull a bigger wagon it is easier to add more oxen than to find (or build) a bigger ox.' (Gropp, Lusk & Skjellum, Using MPI)."
  },
  {
    "question": "What is the primary motivation for using parallel programming?",
    "answer": "Parallel programming is used to construct solutions more quickly by distributing work among multiple people or processors, allowing tasks to be done simultaneously. It addresses the limits of a single processor's execution speed and high memory demands."
  },
  {
    "question": "What is considered the most crucial aspect in designing parallel programs?",
    "answer": "Communication is the most crucial aspect in the design and development of parallel programs, as its requirements introduce complexity."
  },
  {
    "question": "What are the two main models of parallel programming?",
    "answer": "The two main models in parallel programming are shared memory parallelism (SMP) and distributed memory parallelism."
  },
  {
    "question": "How does shared memory parallelism (SMP) work?",
    "answer": "In shared memory parallelism (SMP), all processors access and see the same memory image, meaning memory can be addressed globally. Processors communicate implicitly by reading from and writing to this shared memory."
  },
  {
    "question": "What is the main challenge associated with shared memory parallelism?",
    "answer": "The main challenge in shared memory parallelism is ensuring data consistency, specifically that data is modified by only one process at a time."
  },
  {
    "question": "How does distributed memory parallelism operate?",
    "answer": "Distributed memory parallelism is structured like a cluster of computers connected by a dedicated network. Each process has its own memory and can run on distinct machines, communicating explicitly by sending and receiving messages through functions."
  },
  {
    "question": "What is the primary difficulty in distributed memory parallelism?",
    "answer": "The primary difficulty in distributed memory parallelism is minimizing communication, as networks transmit data much slower than a single computer's memory access (microseconds compared to nanoseconds)."
  },
  {
    "question": "What does the acronym MPI stand for?",
    "answer": "MPI stands for 'Message Passing Interface'."
  },
  {
    "question": "What is MPI?",
    "answer": "MPI is a standard that defines subroutines, functions, objects, and other elements for developing parallel programs in a distributed memory environment."
  },
  {
    "question": "Can you name some common implementations of MPI?",
    "answer": "Common implementations of MPI include Open MPI, Intel MPI, MPICH, and MVAPICH."
  },
  {
    "question": "Which programming languages are officially supported by the MPI standard?",
    "answer": "The MPI standard describes interfaces for Fortran, C, and C++."
  },
  {
    "question": "Are C++ interfaces still supported in MPI 3.0?",
    "answer": "MPI 3.0 no longer directly supports C++ interfaces, but users can use C interfaces from C++ or Boost MPI."
  },
  {
    "question": "What are the benefits of using MPI for parallel programming?",
    "answer": "Benefits of MPI include program portability across different computers, efficient parallelism, good scalability, simplified debugging due to distinct memory ranges (avoiding access conflicts), and the ability to use standard serial debugging tools for segmentation errors."
  },
  {
    "question": "Why might MPI programs appear more complex than others?",
    "answer": "MPI programs might appear more complex because they require explicit management of communication and synchronization, unlike programs where communication is handled implicitly."
  },
  {
    "question": "What recommendation is given to improve the computation speed of an MPI program?",
    "answer": "It is recommended to restrict communications between processes to favor the computation speed of an MPI program."
  }
]