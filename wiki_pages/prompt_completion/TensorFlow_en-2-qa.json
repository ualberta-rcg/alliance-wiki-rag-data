[
  {
    "question": "What information needs to be replaced in the SSH command to access TensorBoard remotely?",
    "answer": "You need to replace `computenode` with the actual node hostname, `userid` with your Alliance username, and `cluster` with the cluster hostname (e.g., `beluga`, `cedar`, `graham`)."
  },
  {
    "question": "What happens if port 6006 is already in use when setting up TensorBoard?",
    "answer": "If port 6006 is already in use, TensorBoard will use another port, such as 6007 or 6008."
  },
  {
    "question": "How do you access TensorBoard in a web browser after establishing the SSH connection?",
    "answer": "Once the connection is created, navigate to `http://localhost:6006` in your web browser."
  },
  {
    "question": "What are the two most common methods for managing variables when training TensorFlow 1.x models on multiple GPUs?",
    "answer": "The two most common methods are \"Parameter Server\" and \"Replicated\"."
  },
  {
    "question": "Where can users find example code for implementing multi-GPU variable management methods in TensorFlow 1.x?",
    "answer": "Users can reference the TensorFlow Benchmarks code on GitHub at `https://github.com/tensorflow/benchmarks`."
  },
  {
    "question": "How does the Parameter Server method manage variables in distributed TensorFlow 1.x training?",
    "answer": "Variables are stored on a parameter server that holds the master copy. In distributed training, parameter servers are separate processes on different devices. Each step, each tower gets a copy of variables from the parameter server and sends its gradients back."
  },
  {
    "question": "How can you configure the Parameter Server method to store parameters on a CPU?",
    "answer": "Use the command `python tf_cnn_benchmarks.py --variable_update=parameter_server --local_parameter_device=cpu`."
  },
  {
    "question": "How can you configure the Parameter Server method to store parameters on a GPU?",
    "answer": "Use the command `python tf_cnn_benchmarks.py --variable_update=parameter_server --local_parameter_device=gpu`."
  },
  {
    "question": "How does the Replicated method manage variables in distributed TensorFlow 1.x training?",
    "answer": "With the Replicated method, each GPU has its own copy of the variables. An `all_reduce` algorithm or regular cross-device aggregation is used to replicate combined gradients to all towers."
  },
  {
    "question": "How do you use the default all_reduce method for variable replication in TensorFlow 1.x?",
    "answer": "Use the command `python tf_cnn_benchmarks.py --variable_update=replicated`."
  },
  {
    "question": "How do you specify the `xring` all_reduce method for variable replication in TensorFlow 1.x?",
    "answer": "Use the command `python tf_cnn_benchmarks.py --variable_update=replicated --all_reduce_spec=xring`."
  },
  {
    "question": "How do you specify the `pscpu` all_reduce method for variable replication in TensorFlow 1.x?",
    "answer": "Use the command `python tf_cnn_benchmarks.py --variable_update=replicated --all_reduce_spec=pscpu`."
  },
  {
    "question": "How do you specify the `NCCL` all_reduce method for variable replication in TensorFlow 1.x?",
    "answer": "Use the command `python tf_cnn_benchmarks.py --variable_update=replicated --all_reduce_spec=nccl`."
  },
  {
    "question": "What is recommended for users to optimize performance with different variable managing methods in TensorFlow 1.x?",
    "answer": "Users are highly recommended to test their own models with all methods on different types of GPU nodes, as performance varies."
  },
  {
    "question": "Which TensorFlow version and supporting libraries were used for the multi-GPU benchmarks?",
    "answer": "TensorFlow v1.5 (built with CUDA 9 and cuDNN 7) was used."
  },
  {
    "question": "What is the batch size per GPU used for the ResNet-50 and VGG-16 benchmarks?",
    "answer": "The batch size is 32 per GPU for both ResNet-50 and VGG-16 benchmarks."
  },
  {
    "question": "What metric is used to report the results of the ResNet-50 and VGG-16 benchmarks?",
    "answer": "The results are reported in \"images per second\"."
  },
  {
    "question": "What was the performance of ResNet-50 on a Graham GPU node with 1 GPU?",
    "answer": "171.23 images per second."
  },
  {
    "question": "What was the best ResNet-50 benchmark result on a Graham GPU node with 2 GPUs, and which method achieved it?",
    "answer": "The best result was 324.04 images per second, achieved with the 'ps, gpu' method."
  },
  {
    "question": "What was the best ResNet-50 benchmark result on a Cedar GPU Base node with 4 GPUs, and which method achieved it?",
    "answer": "The best result was 662.65 images per second, achieved with the 'ps,cpu' method."
  },
  {
    "question": "What was the best ResNet-50 benchmark result on a Cedar GPU Large node with 4 GPUs, and which method achieved it?",
    "answer": "The best result was 754.35 images per second, achieved with the 'replicated' method."
  },
  {
    "question": "What was the performance of VGG-16 on a Graham GPU node with 1 GPU?",
    "answer": "115.89 images per second."
  },
  {
    "question": "What was the best VGG-16 benchmark result on a Graham GPU node with 2 GPUs, and which method achieved it?",
    "answer": "The best result was 219.72 images per second, achieved with the 'replicated, nccl' method."
  },
  {
    "question": "What was the best VGG-16 benchmark result on a Cedar GPU Base node with 4 GPUs, and which method achieved it?",
    "answer": "The best result was 388.53 images per second, achieved with the 'replicated, nccl' method."
  },
  {
    "question": "What was the best VGG-16 benchmark result on a Cedar GPU Large node with 4 GPUs, and which method achieved it?",
    "answer": "The best result was 490.52 images per second, achieved with the 'replicated, nccl' method."
  }
]