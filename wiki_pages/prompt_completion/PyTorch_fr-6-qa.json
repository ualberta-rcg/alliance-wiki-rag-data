[
  {
    "question": "What is the primary purpose of the `pytorch-ddp-test.py` script?",
    "answer": "The `pytorch-ddp-test.py` script is designed to demonstrate distributed data parallel training for CIFAR10 classification models using PyTorch's `DistributedDataParallel` class."
  },
  {
    "question": "How does the `pytorch-ddp-test.py` script determine which GPU device to use for each process?",
    "answer": "The script uses the `SLURM_LOCALID` environment variable to determine the `local_rank`, which is then used to set the current CUDA device via `torch.cuda.set_device(current_device)`."
  },
  {
    "question": "Which PyTorch class is responsible for enabling distributed data parallelism in the `pytorch-ddp-test.py` script?",
    "answer": "The `torch.nn.parallel.DistributedDataParallel` class is used to wrap the neural network model, enabling distributed data parallelism."
  },
  {
    "question": "What parameters are used to initialize the process group in the `pytorch-ddp-test.py` script?",
    "answer": "The process group is initialized using command-line arguments: `backend` (e.g., 'nccl'), `init_method` (e.g., 'tcp://127.0.0.1:3456'), `world_size`, and `rank`."
  },
  {
    "question": "What dataset is utilized for training within the `pytorch-ddp-test.py` example?",
    "answer": "The CIFAR10 dataset is used for training, and the script expects it to be located in a `./data` directory."
  },
  {
    "question": "What is the function of `torch.utils.data.distributed.DistributedSampler` in the script?",
    "answer": "`DistributedSampler` ensures that each process receives a unique subset of the training data in each epoch, which is crucial for correct distributed data parallel training. It also helps in shuffling data across epochs."
  },
  {
    "question": "Describe the network architecture defined in the `Net` class of the `pytorch-ddp-test.py` script.",
    "answer": "The `Net` class defines a simple convolutional neural network with two convolutional layers (`nn.Conv2d`), each followed by a max-pooling layer (`nn.MaxPool2d`) and ReLU activation (`F.relu`). The output of the convolutional layers is flattened and passed through three fully connected layers (`nn.Linear`)."
  },
  {
    "question": "What optimizer and loss function are configured in the `pytorch-ddp-test.py` script?",
    "answer": "The script uses `optim.SGD` (Stochastic Gradient Descent) as the optimizer, configured with a learning rate, momentum (0.9), and weight decay (1e-4). `nn.CrossEntropyLoss` is used as the loss function."
  },
  {
    "question": "How are the input data and targets moved to the GPU during training in the `train` function?",
    "answer": "Inside the training loop, `inputs` and `targets` are explicitly moved to the GPU using `inputs = inputs.cuda()` and `targets = targets.cuda()`."
  },
  {
    "question": "What is PyTorch Lightning and how does it relate to the `DistributedDataParallel` class in PyTorch?",
    "answer": "PyTorch Lightning is a Python package that simplifies common, code-heavy tasks in PyTorch, including multi-GPU training. It provides an abstraction that makes it easier to implement distributed training, often encapsulating the explicit use of classes like `DistributedDataParallel`."
  },
  {
    "question": "What command-line arguments can be passed to the `pytorch-ddp-test.py` script to configure its behavior?",
    "answer": "The script accepts arguments such as `--lr` (learning rate), `--batch_size`, `--max_epochs`, `--num_workers`, `--init_method` (for distributed process group initialization), `--dist-backend` (the backend for distributed communication), `--world_size`, and a `--distributed` flag."
  }
]