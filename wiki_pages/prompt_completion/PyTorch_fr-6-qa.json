[
  {
    "question": "What is the purpose of the `pytorch-ddp-test.py` script?",
    "answer": "It is an example Python script designed for distributed data parallel training of CIFAR10 classification models."
  },
  {
    "question": "Which PyTorch class is used for distributed data parallel training in `pytorch-ddp-test.py`?",
    "answer": "The script uses `torch.nn.parallel.DistributedDataParallel` for distributed training."
  },
  {
    "question": "How does `pytorch-ddp-test.py` handle command-line arguments?",
    "answer": "It uses `argparse.ArgumentParser` to define and parse command-line arguments such as `lr`, `batch_size`, `max_epochs`, `num_workers`, `init_method`, `dist-backend`, `world_size`, and `distributed`."
  },
  {
    "question": "How is the distributed process group initialized in `pytorch-ddp-test.py`?",
    "answer": "The process group is initialized using `dist.init_process_group` with `backend`, `init_method`, `world_size`, and `rank` parameters."
  },
  {
    "question": "How does the script determine the current CUDA device for each process?",
    "answer": "It retrieves the `local_rank` from the `SLURM_LOCALID` environment variable and sets `torch.cuda.set_device(local_rank)`."
  },
  {
    "question": "What dataset is used for training in the `pytorch-ddp-test.py` script?",
    "answer": "The CIFAR10 dataset is used for training."
  },
  {
    "question": "How is data loaded in a distributed manner in `pytorch-ddp-test.py`?",
    "answer": "It uses `torch.utils.data.distributed.DistributedSampler` in conjunction with `DataLoader` to load data for distributed training."
  },
  {
    "question": "What optimizer and loss function are used in the `pytorch-ddp-test.py` script?",
    "answer": "The script uses `torch.optim.SGD` as the optimizer and `torch.nn.CrossEntropyLoss` as the loss function."
  },
  {
    "question": "What is PyTorch Lightning?",
    "answer": "PyTorch Lightning is a Python package that provides interfaces to PyTorch, simplifying many common, code-heavy tasks, including training on multiple GPUs."
  },
  {
    "question": "Why is `DistributedDataParallel` recommended by PyTorch developers for multi-GPU training?",
    "answer": "It is recommended for multi-GPU data parallelism across single or multiple nodes."
  },
  {
    "question": "How is the neural network model moved to the GPU in `pytorch-ddp-test.py`?",
    "answer": "After defining and compiling the `Net` model, it is moved to the GPU by calling `net.cuda()`."
  },
  {
    "question": "How is the loss function moved to the GPU in `pytorch-ddp-test.py`?",
    "answer": "The loss function is moved to the GPU by calling `.cuda()` on the `nn.CrossEntropyLoss()` instance: `criterion = nn.CrossEntropyLoss().cuda()`."
  },
  {
    "question": "What is the default learning rate (`lr`) used in the `pytorch-ddp-test.py` script?",
    "answer": "The default learning rate (`lr`) is `0.1`."
  },
  {
    "question": "What is the default batch size in the `pytorch-ddp-test.py` script?",
    "answer": "The default batch size is `768`."
  },
  {
    "question": "How many epochs does the `pytorch-ddp-test.py` script run by default?",
    "answer": "The script runs for `4` epochs by default, as specified by `args.max_epochs`."
  }
]