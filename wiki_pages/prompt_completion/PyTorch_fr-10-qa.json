[
  {
    "question": "How do you create model checkpoints with custom training loops in PyTorch?",
    "answer": "For examples on creating model checkpoints with custom training loops, consult the PyTorch documentation on saving and loading a general checkpoint."
  },
  {
    "question": "Is additional code required for checkpoints when using PyTorch Lightning in distributed training?",
    "answer": "No, with PyTorch Lightning, no additional code is required for checkpoints in distributed training, other than inserting the callbacks parameter."
  },
  {
    "question": "Which process should create checkpoints when using DistributedDataParallel or Horovod in distributed training?",
    "answer": "When using DistributedDataParallel or Horovod, checkpoints should be created by only one process (rank) of your program, typically rank 0, as all processes will have the same state."
  },
  {
    "question": "How can you save a checkpoint from the first process (rank 0) during distributed training?",
    "answer": "You can save a checkpoint from the first process (rank 0) using `if global_rank == 0: torch.save(ddp_model.state_dict(), \"./checkpoint_path\")`."
  },
  {
    "question": "What potential issues can arise when loading checkpoints in distributed training, and how can they be avoided?",
    "answer": "Potential issues include processes attempting to load an unsaved checkpoint. To avoid this, add `torch.distributed.barrier()` to ensure the checkpoint is fully written to disk before other processes try to load it."
  },
  {
    "question": "How should `torch.load` be configured to correctly load tensors on specific GPUs during distributed training?",
    "answer": "To load tensors on the correct GPU for each process, you must pass the `map_location` parameter to `torch.load`, for example, `map_location = f\"cuda:{local_rank}\"`."
  },
  {
    "question": "What does the CUDA error 'no kernel image is available for execution on the device' indicate?",
    "answer": "This CUDA error means that the current Torch installation does not support the compute architecture or GPU being used."
  },
  {
    "question": "How can you resolve the 'CUDA error: no kernel image is available for execution on the device' error?",
    "answer": "To resolve this error, you can install a newer version of `torch` or request a GPU that is compatible with your current Torch version."
  },
  {
    "question": "What is LibTorch used for?",
    "answer": "LibTorch is used to implement C++ extensions to PyTorch and to create pure C++ machine learning applications."
  },
  {
    "question": "What does the LibTorch distribution include?",
    "answer": "The LibTorch distribution includes the necessary headers, libraries, and CMake configuration files for working with PyTorch, as described in its documentation."
  },
  {
    "question": "What are the initial steps to configure the environment for using LibTorch?",
    "answer": "To configure the environment for LibTorch, you need to load the required modules and then install PyTorch within a Python virtual environment."
  },
  {
    "question": "Which modules should be loaded for LibTorch with StdEnv/2023?",
    "answer": "For LibTorch with StdEnv/2023, you should load `StdEnv/2023`, `gcc`, `cuda/12.2`, `cmake`, `protobuf`, `cudnn`, `python/3.11`, `abseil`, `cusparselt`, and `opencv/4.8.1`."
  },
  {
    "question": "How can you determine the specific versions of modules like abseil, cusparselt, and opencv used to compile a PyTorch wheel for LibTorch?",
    "answer": "You can determine the specific versions by running `ldd $VIRTUAL_ENV/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so | sed -n 's&^.*/\\(\\(opencv\\|abseil\\|cusparselt\\)/[^/]*\\).*&\\1&p' | sort -u`."
  },
  {
    "question": "What modules are required for LibTorch when using StdEnv/2020?",
    "answer": "For LibTorch with StdEnv/2020, you need to load `gcc`, `cuda/11.4`, `cmake`, `protobuf`, `cudnn`, and `python/3.10`."
  },
  {
    "question": "What are the two files needed to compile a minimal LibTorch example?",
    "answer": "To compile a minimal LibTorch example, you need `example.cpp` and `CMakeLists.txt`."
  },
  {
    "question": "What command is used to configure a LibTorch project for compilation with StdEnv/2023?",
    "answer": "With StdEnv/2023, activate the virtual environment and use the command `cmake -B build -S . -DCMAKE_PREFIX_PATH=$VIRTUAL_ENV/lib/python3.11/site-packages -DCMAKE_EXE_LINKER_FLAGS=\"-Wl,-rpath=$VIRTUAL_ENV/lib/python3.11/site-packages/torch/lib,-L$EBROOTCUDA/extras/CUPTI/lib64\" -DCMAKE_SKIP_RPATH=ON -DTORCH_CUDA_ARCH_LIST=\"6.0;7.0;7.5;8.0;9.0\"`."
  },
  {
    "question": "How do you build a LibTorch project after configuring it with CMake?",
    "answer": "After configuring a LibTorch project with CMake, you build it using `cmake --build build`."
  },
  {
    "question": "How do you execute the compiled LibTorch example program?",
    "answer": "You execute the compiled LibTorch example program using `build/example`."
  },
  {
    "question": "What is required to test a LibTorch application with CUDA?",
    "answer": "To test a LibTorch application with CUDA, you must request an interactive job with a GPU."
  }
]