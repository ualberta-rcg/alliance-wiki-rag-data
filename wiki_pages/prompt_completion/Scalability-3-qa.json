[
  {
    "question": "What is scalability in the context of parallel programming?",
    "answer": "Scalability refers to the capacity of a program to efficiently use added computing resources, such as CPU cores."
  },
  {
    "question": "What factors influence the performance gain observed when using parallel programs?",
    "answer": "The gain in performance depends on the nature of the problem, the algorithm or program used, the underlying hardware (memory and network), and the number of CPU cores being used."
  },
  {
    "question": "What is recommended when planning to use a parallel program on a particular cluster?",
    "answer": "It is recommended to conduct a scalability analysis where the software is tested with a fixed problem while varying the number of CPU cores."
  },
  {
    "question": "How is a scalability analysis typically conducted?",
    "answer": "The software is tested using a fixed problem while varying the number of CPU cores (e.g., 2, 4, 8, 16, 32, 64 cores), the run time is obtained for each number of cores, and the resulting curve is plotted."
  },
  {
    "question": "What is the first major reason why scalability is often worse than expected?",
    "answer": "Not every operation in the code can be done in parallel, so a percentage of the program's execution remains serial, representing an ultimate limit for parallel efficiency."
  },
  {
    "question": "Can a program's duration drop below the time spent on its serial operations, even with an infinite number of cores?",
    "answer": "No, even with an infinite number of cores, the program's duration could not sink below the time spent on operations that cannot be parallelized."
  },
  {
    "question": "What is the hope for the 'serial percentage' as the problem size increases?",
    "answer": "The hope is that this 'serial percentage' shrinks as the size of the problem the software is working on increases."
  },
  {
    "question": "What is the second major reason for scalability being worse than hoped for?",
    "answer": "The parallelization of the program normally requires communication and synchronization among parallel processes, and the cost of this 'parallel overhead' increases with the number of processes."
  },
  {
    "question": "How does parallel overhead typically increase with the number of cores?",
    "answer": "Parallel overhead typically increases as a power of the number of cores, specifically as T_c \u221d n^\u03b1 where \u03b1 > 1."
  },
  {
    "question": "What happens to the total program duration as the number of cores approaches infinity?",
    "answer": "The total duration of the program will ultimately be dominated by the parallel overhead factor as the number of cores approaches infinity."
  },
  {
    "question": "What crucial point should be noted about the run time curve from a scalability analysis?",
    "answer": "While the run time falls for smaller numbers of cores, a minimum is reached at a certain number of cores, after which the program duration starts to increase as more processes are added."
  },
  {
    "question": "Why is it crucial to carry out a scalability analysis?",
    "answer": "It's crucial to carry out a scalability analysis to determine the optimal choice of CPU cores for the nature and size of problem you're working on and the cluster you're using."
  },
  {
    "question": "How should a test problem for scalability analysis be chosen?",
    "answer": "It should be relatively small for quick testing, but not so small as to be unrepresentative of a production run, ideally taking 30 to 60 minutes on one or two cores."
  },
  {
    "question": "What characteristics are desirable for a test problem used in weak scaling analysis?",
    "answer": "For weak scaling, it's desirable to have a test problem whose size can be easily increased, ideally in a fairly continuous manner."
  },
  {
    "question": "What are problems called for which the parallel overhead is practically zero?",
    "answer": "Such problems are called 'embarrassingly parallel'."
  },
  {
    "question": "Can you give an example of an 'embarrassingly parallel' problem?",
    "answer": "A good example is running an analysis on 500 different files, where the analysis of each file is independent and generates a single number."
  },
  {
    "question": "Why do embarrassingly parallel problems achieve perfect scaling?",
    "answer": "There is no need to synchronize operations or communicate among processes, allowing for perfect scaling up to any number of processes."
  },
  {
    "question": "What are the two different forms of scaling discussed?",
    "answer": "The two forms of scaling are strong scaling and weak scaling."
  },
  {
    "question": "When the term 'scaling' is used without qualification, what does it normally refer to?",
    "answer": "When used without qualification, 'scaling' normally refers to strong scaling."
  },
  {
    "question": "When does strong scaling apply?",
    "answer": "Strong scaling applies when you wish to perform the same size of simulations as before, but more quickly."
  },
  {
    "question": "When does weak scaling apply?",
    "answer": "Weak scaling applies when you wish to simulate larger or more detailed models, and are willing to wait just as long as before, but for better results."
  },
  {
    "question": "What remains fixed in strong scaling analysis?",
    "answer": "The problem to be used for the scalability analysis is fixed while the number of CPU cores increases."
  },
  {
    "question": "What is the ideal expectation for scaling in strong scaling?",
    "answer": "Ideally, the scaling is expected to be linear, meaning the decrease in run time is the reciprocal of the increase in cores compared to a reference."
  },
  {
    "question": "How is efficiency calculated for strong scaling, according to the example table?",
    "answer": "Efficiency is calculated by dividing the reference run time (e.g., at two cores) by the run time at 'n' cores, then dividing the result by 'n/2', and finally multiplying by a hundred to get a percentage."
  },
  {
    "question": "What does an efficiency of 100% mean in strong scaling?",
    "answer": "An efficiency of 100% means that doubling the number of cores halves the run time."
  },
  {
    "question": "What is 'superlinear scaling' and when does it occur?",
    "answer": "Superlinear scaling is when efficiency is greater than 100%, and it rarely occurs, usually due to a CPU cache functioning more effectively as each CPU core has less to do."
  },
  {
    "question": "Based on the strong scaling example, what was the efficiency at 128 cores?",
    "answer": "The efficiency at 128 cores was 18%."
  },
  {
    "question": "What efficiency is generally considered good for a parallel program?",
    "answer": "An efficiency of 75% or more is considered good."
  },
  {
    "question": "Based on the strong scaling example, what is the advised number of CPU cores for optimal resource use?",
    "answer": "The user is advised to submit jobs using 16 CPU cores, as the improvement beyond that, while still decreasing run time, would not be a good use of resources."
  },
  {
    "question": "How many data points are recommended for a scalability analysis?",
    "answer": "At least five or six values are recommended for a scalability analysis."
  },
  {
    "question": "What happens to the problem size in weak scaling?",
    "answer": "In weak scaling, the problem size is increased in proportion to the increase in the number of CPU cores."
  },
  {
    "question": "What is the ideal outcome in weak scaling with linear scaling?",
    "answer": "In an ideal situation of linear weak scaling, the program's run time will always remain the same."
  },
  {
    "question": "How is 'problem size' defined in weak scaling?",
    "answer": "The definition of 'problem size' depends on the nature of the problem, such as the number of atoms in a molecular simulation or the number of cells in a fluid dynamics simulation."
  },
  {
    "question": "How is efficiency calculated for weak scaling, according to the example table?",
    "answer": "Efficiency is calculated by dividing the reference run time by the run time at 'n' cores, then multiplying by a hundred to obtain a percentage."
  },
  {
    "question": "What is the goal for efficiency in weak scaling?",
    "answer": "The goal is to achieve an efficiency of at least 75%."
  },
  {
    "question": "For what types of applications is weak scaling especially pertinent?",
    "answer": "Weak scaling tends to be especially pertinent for applications that are memory-bound."
  },
  {
    "question": "When is weak scaling usually good for an application?",
    "answer": "Weak scaling is usually good if the parallel program has been designed to privilege communications between nearest neighbours."
  },
  {
    "question": "What kind of application may exhibit poor performance in a weak scalability analysis?",
    "answer": "An application which performs a lot of nonlocal communication, such as a fast Fourier transform, may exhibit poor performance in a weak scalability analysis."
  }
]