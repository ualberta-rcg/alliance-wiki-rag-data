[
  {
    "question": "How do you save a checkpoint during distributed training with DistributedDataParallel or Horovod?",
    "answer": "Checkpointing should be done by only one process (e.g., `global_rank == 0`) of your program, for example, by using `if global_rank == 0: torch.save(ddp_model.state_dict(), \"./checkpoint_path\")`."
  },
  {
    "question": "What is a potential issue when loading a checkpoint in a distributed training environment?",
    "answer": "If a process tries to load a checkpoint before another process has finished saving it, you might encounter errors or incorrect results."
  },
  {
    "question": "How can you prevent issues when loading checkpoints during distributed training?",
    "answer": "You can add `torch.distributed.barrier()` to your code to ensure the checkpoint-creating process finishes writing to disk before others attempt to load it. Also, pass `map_location` to `torch.load` to load tensors on the correct GPU for each rank."
  },
  {
    "question": "How do you load a checkpoint created by a single process in a distributed PyTorch application, ensuring tensors are placed on the correct GPU?",
    "answer": "After a `torch.distributed.barrier()`, set `map_location = f\"cuda:{local_rank}\"` and then call `ddp_model.load_state_dict(torch.load(\"./checkpoint_path\", map_location=map_location))`."
  },
  {
    "question": "What does the CUDA error 'no kernel image is available for execution on the device' indicate?",
    "answer": "This exception means that your current PyTorch installation does not support the compute architecture or the specific GPU you are trying to use."
  },
  {
    "question": "How can you fix a 'CUDA error: no kernel image is available for execution on the device'?",
    "answer": "You need to either update your PyTorch (`torch`) installation to a more recent version or request a different GPU that is compatible with your current PyTorch version."
  },
  {
    "question": "What is LibTorch?",
    "answer": "LibTorch is a C++ library that allows you to implement C++ extensions to PyTorch and create pure C++ machine learning applications. It includes all necessary headers, libraries, and CMake configuration files for PyTorch dependencies."
  },
  {
    "question": "What are the general steps to set up the environment for using LibTorch?",
    "answer": "First, load the required modules for LibTorch, and then install PyTorch within a Python virtual environment."
  },
  {
    "question": "Which modules are required to set up LibTorch in the StdEnv/2023 environment?",
    "answer": "You need to load `StdEnv/2023`, `gcc`, `cuda/12.2`, `cmake`, `protobuf`, `cudnn`, `python/3.11`, `abseil`, `cusparselt`, and `opencv/4.8.1`."
  },
  {
    "question": "How do you set up a Python virtual environment and install PyTorch for LibTorch in StdEnv/2023?",
    "answer": "Run `virtualenv --no-download --clear ~/ENV && source ~/ENV/bin/activate` followed by `pip install --no-index torch numpy`."
  },
  {
    "question": "How can you determine the correct versions for `abseil`, `cusparselt`, and `opencv` modules when setting up LibTorch?",
    "answer": "You can use the command `ldd $VIRTUAL_ENV/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so | sed -n 's&^.*/\\(\\(opencv\\|abseil\\|cusparselt\\)/[^/]*\\).*&\\1&p' | sort -u` to find out which versions were used to compile the Python wheel for torch."
  },
  {
    "question": "Which files are needed for a minimal LibTorch example?",
    "answer": "A minimal LibTorch example requires two files: `example.cpp` and `CMakeLists.txt`."
  },
  {
    "question": "How do you compile a LibTorch example after setting up the StdEnv/2023 environment?",
    "answer": "With the Python virtual environment activated, configure the project using `cmake -B build -S . -DCMAKE_PREFIX_PATH=$VIRTUAL_ENV/lib/python3.11/site-packages -DCMAKE_EXE_LINKER_FLAGS=-Wl,-rpath=$VIRTUAL_ENV/lib/python3.11/site-packages/torch/lib,-L$EBROOTCUDA/extras/CUPTI/lib64 -DCMAKE_SKIP_RPATH=ON -DTORCH_CUDA_ARCH_LIST=\"6.0;7.0;7.5;8.0;9.0\"` and then compile with `cmake --build build`."
  },
  {
    "question": "How do you run a compiled LibTorch example?",
    "answer": "Execute the program using `build/example`."
  },
  {
    "question": "How can you test a LibTorch application with CUDA support?",
    "answer": "To test an application with CUDA, you need to request an interactive job with a GPU."
  },
  {
    "question": "Where can I find more documentation on LibTorch?",
    "answer": "More information can be found at https://pytorch.org/cppdocs/."
  }
]