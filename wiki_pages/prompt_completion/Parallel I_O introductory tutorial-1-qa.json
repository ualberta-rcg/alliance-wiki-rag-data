[
  {
    "question": "What is the main focus of this tutorial?",
    "answer": "This tutorial will discuss issues in handling large amounts of data in HPC, parallel I/O strategies for large-scale input/output with parallel jobs, and the use of MPI-IO, NetCDF, and HDF5 libraries."
  },
  {
    "question": "What types of machines require large parallel runs for today's problems?",
    "answer": "Many of today\u2019s problems require large parallel runs on large distributed-memory machines, also known as clusters."
  },
  {
    "question": "What are the three important I/O activities in large parallel calculations?",
    "answer": "The three important I/O activities are: reading initial datasets, writing application state for checkpointing, and storing results for follow-up runs or post-processing."
  },
  {
    "question": "What is checkpointing in the context of parallel calculations?",
    "answer": "Checkpointing is the process where the application state is written to a file to allow restarting the application in case of some kind of failure."
  },
  {
    "question": "What is Amdahl's law?",
    "answer": "Amdahl's law states that the speedup of a parallel program is limited by the time needed for the sequential fraction of the program."
  },
  {
    "question": "How does sequential I/O affect the performance of a parallel program according to Amdahl's law?",
    "answer": "If the I/O part of a parallel application works sequentially, Amdahl's law suggests that the code's performance will not be as great as desired because the sequential fraction limits speedup."
  },
  {
    "question": "Is efficient I/O challenging, even with high-performance storage systems?",
    "answer": "Yes, efficient I/O without stressing out the storage system\u2014even a high-performance storage system\u2014is challenging."
  },
  {
    "question": "What are the components that make up the total execution time?",
    "answer": "Total Execution Time is comprised of Computation Time, Communication Time, and I/O time."
  },
  {
    "question": "How can one achieve the best performance in terms of total execution time?",
    "answer": "To achieve the best performance, one must optimize all components of the total execution time equation: Computation Time, Communication Time, and I/O time."
  },
  {
    "question": "How do individual load and store operations compare to individual arithmetic operations in terms of time consumption?",
    "answer": "Individual load and store operations are more time-consuming than individual arithmetic operations."
  },
  {
    "question": "What often dominates total execution time in some cases?",
    "answer": "In some cases, total execution time is dominated by I/O time."
  },
  {
    "question": "How do I/O-related systems typically perform in HPC compared to other parts?",
    "answer": "In HPC systems, I/O-related systems are often slow as compared to other parts."
  },
  {
    "question": "Why is special attention needed for data storage in HPC systems?",
    "answer": "Special attention is needed for data storage in HPC systems because supercomputers can produce data at a rate much faster than storage systems can store it, due to a discrepancy in their relative speed increases over time."
  },
  {
    "question": "What does 'IOPs' mean and what does it measure?",
    "answer": "'IOPs' means I/O operations per second, measuring operations like read/write, and is essentially an inverse of latency."
  },
  {
    "question": "How is 'I/O Bandwidth' defined?",
    "answer": "'I/O Bandwidth' is defined as the 'quantity you read/write'."
  },
  {
    "question": "What types of operations are included when calculating IOPs?",
    "answer": "IOPs include Input/Output operations per second such as read, write, open, close, and seek."
  },
  {
    "question": "What is a limitation of top-of-the-line SSDs for large supercomputing systems?",
    "answer": "Top-of-the-line SSDs on a PCI Express can push up to 1GB IOPs, but they are very expensive, making them unsuitable for several hundreds of terabyte supercomputing systems."
  },
  {
    "question": "What are parallel filesystems optimized for?",
    "answer": "Parallel filesystems are optimized for efficient I/O by multiple users on multiple machines/nodes."
  },
  {
    "question": "Do parallel filesystems achieve 'supercomputing' performance in I/O?",
    "answer": "No, parallel filesystems do not result in 'supercomputing' performance in I/O, even though they are optimized for efficient I/O by multiple users on multiple machines/nodes."
  },
  {
    "question": "Why do parallel filesystems not achieve 'supercomputing' performance?",
    "answer": "Parallel filesystems do not achieve 'supercomputing' performance due to factors like disk-access time and communication over the network, which has limited bandwidth and serves many users."
  },
  {
    "question": "What is the typical I/O software and hardware stack?",
    "answer": "The typical I/O software and hardware stack is: I/O Hardware --> Parallel filesystem --> I/O Middleware --> High-end I/O library --> Application."
  },
  {
    "question": "What layer is at the bottom of the parallel I/O abstraction stack?",
    "answer": "At the bottom of the parallel I/O abstraction stack is I/O hardware, which consists of physical arrays or hard-disks attached to the cluster."
  },
  {
    "question": "What runs on top of the I/O hardware in a cluster?",
    "answer": "A parallel file system runs on top of the I/O hardware in a cluster."
  }
]