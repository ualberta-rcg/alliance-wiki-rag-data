[
  {
    "question": "When should you use the `salloc` command on Trillium?",
    "answer": "You should use `salloc` to request an interactive session from the regular queue if your test job requires more time than allowed by `debugjob`."
  },
  {
    "question": "What is the basic `salloc` command for CPU test jobs?",
    "answer": "The basic command for CPU test jobs is `$ salloc --export=NONE --nodes=N --time=M:00:00 [--ngpus-per-node=G] [--x11]`."
  },
  {
    "question": "What do the parameters N, M, G, and --x11 signify in the `salloc` command?",
    "answer": "`N` is the number of nodes, `M` is the number of hours the job should run, `G` is the number of GPUs per node (when applicable), and `--x11` is required for graphical applications."
  },
  {
    "question": "What is an important consideration when submitting jobs with `salloc` compared to `debugjob`?",
    "answer": "Jobs submitted with `salloc` may take longer to start than with `debugjob` and count towards your allocation."
  },
  {
    "question": "What job scheduler does Trillium use?",
    "answer": "Trillium uses SLURM as its job scheduler."
  },
  {
    "question": "How do you submit a job to the SLURM scheduler on Trillium?",
    "answer": "You use the `sbatch` command on a login node, for example: `$ sbatch jobscript.sh`."
  },
  {
    "question": "From which nodes must CPU compute jobs be submitted?",
    "answer": "CPU compute jobs need to be submitted from the CPU login nodes."
  },
  {
    "question": "From which node must GPU compute jobs be submitted?",
    "answer": "GPU compute jobs must be submitted from the GPU login node."
  },
  {
    "question": "What information should a job script contain for SLURM?",
    "answer": "The job script should contain lines starting with `#SBATCH` that specify the resources the script will need."
  },
  {
    "question": "What factors influence the priority of a job in the SLURM queue?",
    "answer": "The priority depends on requested resources, time spent in the queue, recent past usage, and the SLURM account under which the job was submitted."
  },
  {
    "question": "What are SLURM accounts on Trillium associated with?",
    "answer": "SLURM accounts correspond to Resource Allocation Projects (RAPs)."
  },
  {
    "question": "What are the typical naming conventions for SLURM accounts based on RAPs?",
    "answer": "Default RAPs (RAS) have accounts starting with `def-`, while RAC allocations have accounts typically starting with `rrg-` or `rpp-`."
  },
  {
    "question": "Can a RAC allocation for a different system, such as Nibi, be used on Trillium?",
    "answer": "No, RACs are bound to a specific system, so a RAC for Nibi cannot be used on Trillium."
  },
  {
    "question": "Where must job output be written when running jobs on Trillium's compute nodes?",
    "answer": "Job output must be written to the scratch file system."
  },
  {
    "question": "Why are home and project directories not suitable for writing job output on compute nodes?",
    "answer": "Home and project directories are only available for reading on the compute nodes, enforcing that job output is written to scratch."
  },
  {
    "question": "From which directory is it recommended to submit jobs on Trillium?",
    "answer": "In most cases, you should submit your jobs from your `$SCRATCH` directory."
  },
  {
    "question": "How can users explicitly specify the SLURM account for a job?",
    "answer": "Users can specify the account with the `--account=ACCOUNT_NAME` option in their job script or submission command."
  },
  {
    "question": "Is it permissible to submit new jobs from Trillium's compute nodes?",
    "answer": "No, jobs cannot be submitted from compute nodes (nor datamover nodes)."
  },
  {
    "question": "Why is submitting jobs from compute nodes restricted on Trillium?",
    "answer": "This restriction prevents accidentally spawning many jobs, overloading the scheduler, and overloading the backup process."
  },
  {
    "question": "Can users request a specific number of CPU cores on Trillium's CPU subcluster?",
    "answer": "No, it is not possible to request a certain number of cores; all jobs must use full nodes on the CPU subcluster."
  },
  {
    "question": "What is the minimum number of CPU cores allocated for a job on the CPU subcluster?",
    "answer": "The minimum size of a CPU job has 192 cores at its disposal, corresponding to a full node."
  },
  {
    "question": "What action should be taken if a CPU job underutilizes its allocated cores?",
    "answer": "If a CPU job underutilizes the cores, the support team may reach out to assist in optimizing the workflow, or users can contact them for assistance."
  },
  {
    "question": "How many GPUs are present on each GPU node in the Trillium GPU subcluster?",
    "answer": "Each GPU node contains 4 GPUs."
  },
  {
    "question": "What are the allowed GPU request sizes on Trillium's GPU subcluster?",
    "answer": "On Trillium, you are only allowed to request exactly 1 GPU or a multiple of 4 GPUs."
  },
  {
    "question": "Does Trillium support NVIDIA's MIG technology for fractional GPU allocation?",
    "answer": "No, Trillium does not support MIG as on other clusters."
  },
  {
    "question": "What resources are allocated for a single-GPU job on the GPU subcluster?",
    "answer": "A single-GPU job amounts to a quarter node, with 24 cores and about 188 GiB of RAM."
  },
  {
    "question": "What technology can be used to share a GPU among processes within a job on Trillium?",
    "answer": "You can use NVIDIA's Multi-Process Service (MPS) or Hyper-Q within your jobs to share a GPU among processes."
  }
]