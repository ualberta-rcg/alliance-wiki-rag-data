[
  {
    "question": "Comment soumettre une t\u00e2che MPI sur Trillium ?",
    "answer": "Soumettez le script `mpi_job.sh` depuis votre r\u00e9pertoire `$SCRATCH` sur un n\u0153ud de connexion \u00e0 l'aide de la commande `sbatch mpi_job.sh`."
  },
  {
    "question": "Que signifie la premi\u00e8re ligne `#!/bin/bash` dans un script de t\u00e2che ?",
    "answer": "La premi\u00e8re ligne `#!/bin/bash` indique qu'il s'agit d'un script bash."
  },
  {
    "question": "\u00c0 quoi servent les lignes commen\u00e7ant par `#SBATCH` dans un script de t\u00e2che ?",
    "answer": "Les lignes commen\u00e7ant par `#SBATCH` sont des directives pour l'ordonnanceur."
  },
  {
    "question": "Quel nom la t\u00e2che MPI re\u00e7oit-elle de `sbatch` ?",
    "answer": "La t\u00e2che MPI re\u00e7oit le nom `mpi_job`."
  },
  {
    "question": "Quelles ressources l'exemple de t\u00e2che MPI demande-t-il ?",
    "answer": "L'exemple de t\u00e2che MPI demande deux n\u0153uds pour ex\u00e9cuter chacun 192 t\u00e2ches pendant une heure."
  },
  {
    "question": "Que fait la commande `source /scinet/vast/etc/vastpreload-openmpi.bash` dans un script MPI ?",
    "answer": "Cette commande pr\u00e9charge une biblioth\u00e8que qui optimise MPI-IO pour le syst\u00e8me de fichiers VAST."
  },
  {
    "question": "Quelle commande doit \u00eatre utilis\u00e9e pour l'optimisation MPI-IO sur VAST si on utilise Intel MPI au lieu d'Open MPI ?",
    "answer": "Si vous utilisez Intel MPI au lieu d'Open MPI, utilisez `source /scinet/vast/etc/vastpreload-intelmpi.bash`."
  },
  {
    "question": "Comment soumettre une t\u00e2che OpenMP sur Trillium ?",
    "answer": "Soumettez le script `openmp_job.sh` depuis votre r\u00e9pertoire `$SCRATCH` sur un n\u0153ud de connexion CPU \u00e0 l'aide de la commande `sbatch openmp_job.sh`."
  },
  {
    "question": "Quel nom la t\u00e2che OpenMP re\u00e7oit-elle de `sbatch` ?",
    "answer": "La t\u00e2che OpenMP re\u00e7oit le nom `openmp_job`."
  },
  {
    "question": "Quelles ressources l'exemple de t\u00e2che OpenMP demande-t-il ?",
    "answer": "L'exemple de t\u00e2che OpenMP demande un n\u0153ud avec 192 CPU pour ex\u00e9cuter une t\u00e2che pendant une heure."
  },
  {
    "question": "Comment la variable d'environnement `OMP_NUM_THREADS` est-elle configur\u00e9e dans l'exemple de t\u00e2che OpenMP ?",
    "answer": "La variable `OMP_NUM_THREADS` est configur\u00e9e avec la valeur de `$SLURM_CPUS_PER_TASK`."
  },
  {
    "question": "Comment soumettre une t\u00e2che hybride MPI/OpenMP sur Trillium ?",
    "answer": "Soumettez le script `hybrid_job.sh` depuis votre r\u00e9pertoire `$SCRATCH` sur un n\u0153ud de connexion \u00e0 l'aide de la commande `sbatch hybrid_job.sh`."
  },
  {
    "question": "Quel nom la t\u00e2che hybride MPI/OpenMP re\u00e7oit-elle de `sbatch` ?",
    "answer": "La t\u00e2che hybride MPI/OpenMP re\u00e7oit le nom `hybrid_job`."
  },
  {
    "question": "Quelles ressources l'exemple de t\u00e2che hybride MPI/OpenMP demande-t-il ?",
    "answer": "L'exemple de t\u00e2che hybride demande deux n\u0153uds pour ex\u00e9cuter chacun 48 t\u00e2ches avec chacune 4 fils pendant une heure."
  },
  {
    "question": "Quelles sont les options OpenMP export\u00e9es dans l'exemple de t\u00e2che hybride ?",
    "answer": "Les options OpenMP export\u00e9es sont `OMP_NUM_THREADS`, `OMP_PLACES` et `OMP_PROC_BIND`."
  },
  {
    "question": "Quel est le but de l'option `--map-by` dans la commande `mpirun` d'une t\u00e2che hybride ?",
    "answer": "L'option `--map-by` est utilis\u00e9e pour distribuer \u00e9galement les processus et les fils sur l'ensemble des c\u0153urs."
  },
  {
    "question": "Que doit-on changer dans l'option `--map-by` si on utilise entre 8 et 24 fils par processus dans une t\u00e2che hybride ?",
    "answer": "Si vous utilisez plus de 8 fils par processus (t\u00e2che ou rang MPI) mais au plus 24, changez `l3cache` pour `numa`."
  },
  {
    "question": "Que doit-on changer dans l'option `--map-by` si on utilise plus de 24 fils par processus dans une t\u00e2che hybride ?",
    "answer": "Si vous utilisez plus de 24 fils par processus, changez pour `socket`."
  },
  {
    "question": "Comment demander un seul GPU pour une t\u00e2che sur la sous-grappe de GPU de Trillium ?",
    "answer": "Pour une t\u00e2che avec un seul GPU, utilisez `--gpus-per-node=1`."
  },
  {
    "question": "Comment demander un n\u0153ud GPU entier pour une t\u00e2che sur la sous-grappe de GPU de Trillium ?",
    "answer": "Pour une t\u00e2che avec un n\u0153ud GPU entier, utilisez `--gpus-per-node=4`."
  },
  {
    "question": "Est-il possible de demander 2 ou 3 GPUs par n\u0153ud sur Trillium ?",
    "answer": "Non, vous ne pouvez pas demander `--gpus-per-node=2` ou `3`."
  },
  {
    "question": "La technologie NVIDIA MIG est-elle prise en charge sur Trillium pour allouer une fraction de GPU ?",
    "answer": "Non, Trillium ne prend pas en charge MIG comme sur les autres grappes."
  },
  {
    "question": "Quel service peut \u00eatre utilis\u00e9 pour partager un GPU entre les processus ex\u00e9cut\u00e9s dans une t\u00e2che sur Trillium ?",
    "answer": "Vous pouvez utiliser MPS (Multi-Process Service) de NVIDIA pour partager un GPU entre les processus ex\u00e9cut\u00e9s dans la t\u00e2che."
  },
  {
    "question": "Quelles sont les limites de taille et de dur\u00e9e pour les t\u00e2ches de calcul GPU sur Trillium (partition `calcul`) ?",
    "answer": "Les t\u00e2ches de calcul GPU ont une limite de 150 t\u00e2ches en cours, 500 t\u00e2ches soumises/en cours, une taille minimale de 1/4 n\u0153ud (24 c\u0153urs / 1 GPU), une taille maximale par d\u00e9faut de 5 n\u0153uds (480 c\u0153urs / 20 GPU) ou 25 n\u0153uds (2400 c\u0153urs / 100 GPU) avec une allocation, un temps d'ex\u00e9cution minimum de 15 minutes et un temps d'ex\u00e9cution maximum de 24 heures."
  },
  {
    "question": "Quelles sont les limites de taille et de dur\u00e9e pour les t\u00e2ches de test GPU sur Trillium (partition `d\u00e9bogage`) ?",
    "answer": "Les t\u00e2ches de test GPU ont une limite de 1 t\u00e2che en cours et 1 t\u00e2che soumise/en cours, une taille minimale de 1/4 n\u0153ud (24 c\u0153urs / 1 GPU), une taille maximale de 2 n\u0153uds (192 c\u0153urs / 8 GPU), pas de temps d'ex\u00e9cution minimum, et un temps d'ex\u00e9cution maximum de 2 heures (1 GPU) ou 30 minutes (8 GPU)."
  },
  {
    "question": "Quels facteurs influencent le temps d'attente d'une t\u00e2che dans la file d'attente sur Trillium ?",
    "answer": "Le temps d'attente d\u00e9pend de nombreux facteurs tels que la taille des ressources pour votre groupe, la quantit\u00e9 de l'allocation utilis\u00e9e r\u00e9cemment, le nombre de n\u0153uds et le temps d'ex\u00e9cution demand\u00e9s, ainsi que le nombre total des t\u00e2ches en attente."
  }
]