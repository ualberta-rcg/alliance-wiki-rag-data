[
  {
    "question": "Which modules should be loaded for Ansys Fluent jobs on compute nodes using `StdEnv/2023`?",
    "answer": "Users should load `module load StdEnv/2023` and `module load ansys/2023R2` or newer versions."
  },
  {
    "question": "How do you specify the journal file name in the `script-flu-bynode-intel.sh` Slurm script?",
    "answer": "The journal file name is specified using the `MYJOURNALFILE` variable, for example, `MYJOURNALFILE=sample.jou`."
  },
  {
    "question": "What are the common Fluent versions that can be specified in the Slurm scripts?",
    "answer": "Common Fluent versions include `2d`, `2ddp`, `3d`, or `3ddp`, specified via the `MYVERSION` variable."
  },
  {
    "question": "How is the machinefile generated for an Intel MPI Fluent job?",
    "answer": "The machinefile is generated using `slurm_hl2hl.py --format ANSYS-FLUENT > /tmp/machinefile-$SLURM_JOB_ID`."
  },
  {
    "question": "How are the total number of cores (`NCORES`) calculated in an Intel MPI Fluent job using the 'Multinode (by node)' script?",
    "answer": "The total number of cores is calculated as `SLURM_NNODES * SLURM_NTASKS_PER_NODE * SLURM_CPUS_PER_TASK`."
  },
  {
    "question": "What is the Fluent command for running a single-node Intel MPI job?",
    "answer": "The command is `fluent -g $MYVERSION -t $NCORES -affinity=0 -mpi=intel -pshmem -i $MYJOURNALFILE`."
  },
  {
    "question": "What is the Fluent command for running a multi-node Intel MPI job?",
    "answer": "The command is `fluent -g $MYVERSION -t $NCORES -affinity=0 -mpi=intel -pib -cnf=/tmp/machinefile-$SLURM_JOB_ID -i $MYJOURNALFILE`."
  },
  {
    "question": "How does the 'Multinode (by core)' script specify the total number of cores?",
    "answer": "It uses the `#SBATCH --ntasks` directive, for example, `#SBATCH --ntasks=16`."
  },
  {
    "question": "How is memory per CPU specified in the 'Multinode (by core)' Fluent script?",
    "answer": "Memory per CPU is specified using `#SBATCH --mem-per-cpu`, for example, `#SBATCH --mem-per-cpu=4G`."
  },
  {
    "question": "What Open MPI-specific environment variables are set for Fluent jobs on Narval cluster?",
    "answer": "The `OPENMPI_ROOT` and `OMPI_MCA_hwloc_base_binding_policy` environment variables are exported, for example, `export OPENMPI_ROOT=$EBROOTOPENMPI` and `export OMPI_MCA_hwloc_base_binding_policy=core`."
  },
  {
    "question": "How is the machinefile specifically created for Open MPI Fluent jobs on Narval?",
    "answer": "It's created by first running `slurm_hl2hl.py --format ANSYS-FLUENT > /tmp/mf-$SLURM_JOB_ID` and then processing that temporary file to generate `/tmp/machinefile-$SLURM_JOB_ID`."
  },
  {
    "question": "What is the Fluent command for running a single-node Open MPI job on Narval?",
    "answer": "The command is `fluent -g $MYVERSION -t $NCORES -affinity=0 -mpi=openmpi -pshmem -i $MYJOURNALFILE`."
  },
  {
    "question": "What is the Fluent command for running a multi-node Open MPI job on Narval?",
    "answer": "The command is `fluent -g $MYVERSION -t $NCORES -affinity=0 -mpi=openmpi -pib -cnf=/tmp/machinefile-$SLURM_JOB_ID -i $MYJOURNALFILE`."
  },
  {
    "question": "What Intel MPI modules are loaded on Narval cluster if `EBVERSIONGENTOO` is 2020?",
    "answer": "If `CC_CLUSTER` is narval and `EBVERSIONGENTOO` is 2020, `intel/2021 intelmpi` modules are loaded."
  }
]