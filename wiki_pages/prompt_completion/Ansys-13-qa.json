[
  {
    "question": "What are the SBATCH directives required for a single-node Ansys CFX job script?",
    "answer": "The necessary SBATCH directives for a single-node Ansys CFX job script are `--account`, `--time`, `--nodes=1`, `--ntasks-per-node`, `--mem`, and `--cpus-per-task=1`."
  },
  {
    "question": "Which modules should be loaded for Ansys CFX single-node jobs when using the StdEnv/2023 environment?",
    "answer": "For Ansys CFX single-node jobs with StdEnv/2023, you should load `module load StdEnv/2023` and `module load ansys/2023R2` (or newer module versions)."
  },
  {
    "question": "How do you execute the `cfx5solve` command for a single-node job on the Narval cluster versus other clusters?",
    "answer": "On the Narval cluster, you run `cfx5solve -def YOURFILE.def -start-method \"Open MPI Local Parallel\" -part $SLURM_CPUS_ON_NODE`. For other clusters, the command is `cfx5solve -def YOURFILE.def -start-method \"Intel MPI Local Parallel\" -part $SLURM_CPUS_ON_NODE`."
  },
  {
    "question": "What are the maximum core limits per node for single-node Ansys CFX jobs on Graham, Cedar, Beluga, and Narval clusters?",
    "answer": "The maximum cores per node are: Graham 44, Cedar 32 or 48, Beluga 40, and Narval 64."
  },
  {
    "question": "What are the SBATCH directives required for a multi-node Ansys CFX job script?",
    "answer": "The necessary SBATCH directives for a multi-node Ansys CFX job script are `--account`, `--time`, `--nodes` (2 or more), `--ntasks-per-node`, `--mem=0`, and `--cpus-per-task=1`."
  },
  {
    "question": "Which modules should be loaded for Ansys CFX multi-node jobs when using the StdEnv/2023 environment?",
    "answer": "For Ansys CFX multi-node jobs with StdEnv/2023, you should load `module load StdEnv/2023` and `module load ansys/2023R2` (or newer module versions)."
  },
  {
    "question": "How is the `cfx5solve` command executed for a multi-node job on the Narval cluster compared to other clusters?",
    "answer": "On the Narval cluster, you run `cfx5solve -def YOURFILE.def -start-method \"Open MPI Distributed Parallel\" -par-dist $NNODES`. For other clusters, you first set `export I_MPI_HYDRA_BOOTSTRAP=ssh` and `unset I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS`, then run `cfx5solve -def YOURFILE.def -start-method \"Intel MPI Distributed Parallel\" -par-dist $NNODES`."
  },
  {
    "question": "What tool is used to determine the number of nodes (`NNODES`) for Ansys CFX multi-node jobs?",
    "answer": "The `slurm_hl2hl.py --format ANSYS-CFX` utility is used to determine the number of nodes (`NNODES`)."
  },
  {
    "question": "What is the initialization procedure for an Ansys Workbench project file before its first submission to a cluster queue?",
    "answer": "Before first submission, connect to the cluster via TigerVNC, navigate to the project directory, start Workbench with the appropriate Ansys module, open the project, right-click on 'Setup' and select 'Clear All Generated Data', then exit Workbench by choosing 'File -> Exit' and clicking 'No' when prompted to save changes."
  },
  {
    "question": "How can I prevent an Ansys Workbench job from saving the solution automatically after successful completion, especially for scaling tests?",
    "answer": "To prevent automatic saving, remove the line `;Save(Overwrite=True)` from the last line of your Workbench Slurm script."
  },
  {
    "question": "What alternative methods can be used to manage Workbench project files during multiple test runs for scaling, to avoid repeatedly clearing solutions manually?",
    "answer": "As an alternative, you can maintain a copy of the original `YOURPROJECT.wbpj` file and its `YOURPROJECT_files` subdirectory, and restore them after each solution is written."
  }
]