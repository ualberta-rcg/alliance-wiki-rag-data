[
  {
    "question": "What is recommended for long running computations?",
    "answer": "If your computations are long, you should use checkpointing."
  },
  {
    "question": "How should a 3-day training time be managed?",
    "answer": "You should split it in 3 chunks of 24 hours using checkpointing."
  },
  {
    "question": "What are the benefits of using checkpointing for long computations?",
    "answer": "It will prevent you from losing all the work in case of an outage, and give you an edge in terms of priority (more nodes are available for short jobs)."
  },
  {
    "question": "Do most machine learning libraries support checkpointing natively?",
    "answer": "Yes, most machine learning libraries natively support checkpointing."
  },
  {
    "question": "What is provided if a program does not natively support checkpointing?",
    "answer": "A general checkpointing solution is provided."
  },
  {
    "question": "Where can one find examples of checkpointing with PyTorch and TensorFlow?",
    "answer": "Examples are available in 'Checkpointing with PyTorch' and 'Checkpointing with TensorFlow' sections."
  },
  {
    "question": "When should similar jobs be grouped into one?",
    "answer": "You should consider grouping many jobs into one if you are performing hyperparameter search, training many variants of the same method, or running many optimization processes of similar duration."
  },
  {
    "question": "What tools are available to help group many similar jobs?",
    "answer": "META, GLOST, and GNU Parallel are available to help you with this."
  },
  {
    "question": "Which tools can assist with experiment tracking and hyperparameter optimization?",
    "answer": "Weights & Biases (wandb) and Comet.ml can help you get the most out of your compute allocation."
  },
  {
    "question": "How do Weights & Biases and Comet.ml help with compute allocation?",
    "answer": "They allow easier tracking and analysis of training runs and provide Bayesian hyperparameter search."
  },
  {
    "question": "Are Comet and Wandb available on Graham?",
    "answer": "No, Comet and Wandb are not currently available on Graham."
  },
  {
    "question": "How do modern deep learning packages handle large-scale training?",
    "answer": "Modern deep learning packages like Pytorch and TensorFlow include utilities to handle large-scale training natively and tutorials on how to do it abound."
  },
  {
    "question": "Is scaling classic machine learning methods widely discussed?",
    "answer": "No, scaling classic machine learning (i.e., not deep learning) methods is not as widely discussed."
  },
  {
    "question": "Where can one find ideas and options for training classic ML models on very large datasets?",
    "answer": "A guide on Large Scale Machine Learning (Big Data) contains ideas and practical options, along with tutorials."
  },
  {
    "question": "What causes non-deterministic behavior in RNN and multi-head attention API calls?",
    "answer": "RNN and multi-head attention API calls may exhibit non-deterministic behaviour when the cuDNN library is built with CUDA Toolkit 10.2 or higher."
  },
  {
    "question": "How can non-deterministic behavior in cuDNN RNN and multi-head attention APIs be eliminated?",
    "answer": "The user can eliminate the non-deterministic behaviour by setting a single buffer size in the CUBLAS_WORKSPACE_CONFIG environmental variable."
  },
  {
    "question": "What are examples of settings for the CUBLAS_WORKSPACE_CONFIG environmental variable?",
    "answer": "Examples include :16:8 or :4096:2."
  }
]