[
  {
    "question": "What modules are necessary to load for a distributed mpi4py CPU job?",
    "answer": "You need to load `StdEnv/2023`, `gcc`, `mpi4py/4.0.0`, and `python/3.12`."
  },
  {
    "question": "How do you create a virtual environment on each allocated node for a distributed mpi4py CPU job?",
    "answer": "Run `srun --ntasks $SLURM_NNODES --tasks-per-node=1 bash << EOF\nvirtualenv --no-download $SLURM_TMPDIR/env\nsource $SLURM_TMPDIR/env/bin/activate\nEOF`."
  },
  {
    "question": "Which Python packages are installed within the virtual environment for a distributed mpi4py CPU job?",
    "answer": "The `pip` package is upgraded, and `numpy==2.1.1` is installed."
  },
  {
    "question": "How is the virtual environment activated on the main node for a distributed mpi4py CPU job?",
    "answer": "Activate it using `source $SLURM_TMPDIR/env/bin/activate;`."
  },
  {
    "question": "What command is used to execute the Python script `mpi4py-np-bc.py` in a distributed mpi4py CPU job?",
    "answer": "The script is executed with `srun python mpi4py-np-bc.py;`."
  },
  {
    "question": "What SBATCH parameters are specified in the `submit-mpi4py-whole-nodes.sh` script for running an mpi4py job on whole nodes?",
    "answer": "The parameters are `--account=def-someprof`, `--time=01:00:00`, `--nodes=2`, `--ntasks-per-node=40`, and `--mem-per-cpu=1G`."
  },
  {
    "question": "Where can more information about running mpi4py jobs on whole nodes be found?",
    "answer": "More information is available at `https://docs.alliancecan.ca/wiki/Advanced_MPI_scheduling#Whole_nodes`."
  },
  {
    "question": "What modules must be loaded for an mpi4py job that runs on whole nodes?",
    "answer": "Load `StdEnv/2023`, `gcc`, `openmpi`, `mpi4py/4.0.0`, and `python/3.12`."
  },
  {
    "question": "What is recommended before submitting an mpi4py job script to the scheduler?",
    "answer": "It is important to test that your submission script will start without errors, by doing a quick test in an interactive job."
  },
  {
    "question": "How do you submit a distributed mpi4py job to the scheduler?",
    "answer": "Submit the job using `sbatch submit-mpi4py-distributed.sh`."
  },
  {
    "question": "How do you download the `use_cupy.py` demo example for an mpi4py GPU job?",
    "answer": "From a login node, run `wget https://raw.githubusercontent.com/mpi4py/mpi4py/refs/heads/master/demo/cuda-aware-mpi/use_cupy.py`."
  },
  {
    "question": "Where can other mpi4py demo examples be found?",
    "answer": "Other examples can be found in the `mpi4py/mpi4py/tree/master/demo` folder on GitHub."
  },
  {
    "question": "What SBATCH parameters are required for an mpi4py GPU job?",
    "answer": "The necessary parameters are `--account=def-someprof`, `--time=08:00:00`, `--ntasks=2`, `--mem-per-cpu=2G`, and `--gpus=1`."
  },
  {
    "question": "Which modules need to be loaded for an mpi4py GPU job?",
    "answer": "You need to load `StdEnv/2023`, `gcc`, `cuda/12`, `mpi4py/4.0.0`, and `python/3.11`."
  },
  {
    "question": "Which Python packages are installed for an mpi4py GPU job after creating the virtual environment?",
    "answer": "`pip` is upgraded, and `cupy` and `numba` are installed."
  },
  {
    "question": "How do you submit an mpi4py GPU job?",
    "answer": "Submit the job using `sbatch submit-mpi4py-gpu.sh`."
  },
  {
    "question": "What error message indicates that the 'mpi4py' module is not accessible?",
    "answer": "The error message `ModuleNotFoundError: No module named 'mpi4py'` indicates that the module is not accessible."
  },
  {
    "question": "What are potential solutions for a `ModuleNotFoundError: No module named 'mpi4py'`?",
    "answer": "Possible solutions include checking compatible Python versions using `module spider mpi4py/X.Y.Z` and verifying `python -c 'import mpi4py'` works, or loading the module before activating your virtual environment as described in the 'mpi4py as a package dependency' section."
  }
]