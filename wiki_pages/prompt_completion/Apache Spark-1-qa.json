[
  {
    "question": "What is Apache Spark?",
    "answer": "Apache Spark is an open-source distributed computing framework."
  },
  {
    "question": "Where was Apache Spark initially developed?",
    "answer": "Apache Spark was initially developed by the AMPLab at the University of Berkeley."
  },
  {
    "question": "Which organization now maintains Apache Spark?",
    "answer": "Apache Spark is now a project of the Apache Foundation."
  },
  {
    "question": "What is a key difference between Spark and Hadoop's MapReduce in terms of data storage?",
    "answer": "Unlike Hadoop's MapReduce which uses disk storage, Spark uses primitives conserved in memory."
  },
  {
    "question": "How much faster can Spark be compared to MapReduce for certain applications?",
    "answer": "Spark can achieve performances up to 100 times faster for certain applications compared to MapReduce."
  },
  {
    "question": "What makes Spark suitable for machine learning and interactive data analysis?",
    "answer": "The ability to load data into memory allows for frequent querying, making Spark particularly appropriate for machine learning and interactive data analysis."
  },
  {
    "question": "What is the purpose of the `pyspark_submit.sh` script provided?",
    "answer": "The `pyspark_submit.sh` script is an example script for submitting PySpark jobs, demonstrating how to set up master and worker nodes and run a Python application."
  },
  {
    "question": "What Spark and Python versions are loaded in the `pyspark_submit.sh` script?",
    "answer": "Spark version 2.3.0 and Python version 3.7 are loaded in the `pyspark_submit.sh` script."
  },
  {
    "question": "What is the recommended setting for `MKL_NUM_THREADS` when calling Intel MKL routines from multi-threaded applications in Spark?",
    "answer": "The recommended setting is `export MKL_NUM_THREADS=1`."
  },
  {
    "question": "How is `SLURM_SPARK_MEM` calculated in the PySpark submission script?",
    "answer": "`SLURM_SPARK_MEM` is calculated as 95% of `SLURM_MEM_PER_NODE`."
  },
  {
    "question": "What command starts the Spark master process in the PySpark script?",
    "answer": "The `start-master.sh` command starts the Spark master process."
  },
  {
    "question": "How does the `pyspark_submit.sh` script obtain the Spark master URL?",
    "answer": "The master URL is obtained by parsing the Spark master log file using `grep -Po '(?=spark://).*' $SPARK_LOG_DIR/spark-${SPARK_IDENT_STRING}-org.apache.spark.deploy.master*.out`."
  },
  {
    "question": "What command runs an example Python application (pi.py) in the PySpark script?",
    "answer": "The command `srun -n 1 -N 1 spark-submit --master ${MASTER_URL} --executor-memory ${SLURM_SPARK_MEM}M $SPARK_HOME/examples/src/main/python/pi.py` runs the example Python application."
  },
  {
    "question": "How are Spark master and slave processes shut down in the PySpark script?",
    "answer": "The `kill $slaves_pid` command stops the slave processes, and `stop-master.sh` stops the master process."
  },
  {
    "question": "What is the purpose of the `pyspark_java_submit.sh` script?",
    "answer": "The `pyspark_java_submit.sh` script is an example script for submitting Spark applications packaged as Java Jars."
  },
  {
    "question": "What two example Java applications are submitted in the `pyspark_java_submit.sh` script?",
    "answer": "The `org.apache.spark.examples.SparkPi` and `org.apache.spark.examples.SparkLR` applications from `spark-examples_2.11-2.3.0.jar` are submitted."
  },
  {
    "question": "How can Spark application activity logs be viewed after execution?",
    "answer": "Spark application activity logs can be saved and viewed later using a web application provided with Spark."
  },
  {
    "question": "What directory should be created to store Spark application event logs?",
    "answer": "A directory `~/.spark/<spark version>/eventlog` should be created to contain the application logs."
  },
  {
    "question": "What directory is needed for Spark configuration parameters?",
    "answer": "A directory `~/.spark/<spark version>/conf` should be created for Spark configuration parameters if it doesn't already exist."
  },
  {
    "question": "What is the name of the configuration file for Spark's default settings?",
    "answer": "The configuration file is named `spark-defaults.conf`."
  },
  {
    "question": "How do you enable event logging in `spark-defaults.conf`?",
    "answer": "Add the line `spark.eventLog.enabled true` to `spark-defaults.conf` to enable event logging."
  },
  {
    "question": "What property specifies the directory for Spark event logs in `spark-defaults.conf`?",
    "answer": "The `spark.eventLog.dir` property specifies the directory for Spark event logs, e.g., `/home/<userid>/.spark/<spark version>/eventlog`."
  },
  {
    "question": "What property specifies the history server log directory in `spark-defaults.conf`?",
    "answer": "The `spark.history.fs.logDirectory` property specifies the history server log directory, e.g., `/home/<userid>/.spark/<spark version>/eventlog`."
  },
  {
    "question": "What is the first step for visualizing Spark logs remotely?",
    "answer": "You need to create an SSH tunnel between your computer and the compute cluster."
  },
  {
    "question": "What command is used to load the Spark module for log visualization?",
    "answer": "The command `module load spark/2.3.0` loads the Spark module."
  },
  {
    "question": "How do you launch the web application for visualizing Spark logs?",
    "answer": "Launch the web application using the command `SPARK_NO_DAEMONIZE=1 start-history-server.sh`."
  },
  {
    "question": "What is the default port used by the Spark History Server web application?",
    "answer": "The Spark History Server web application typically binds to port 18080."
  },
  {
    "question": "How do you access the Spark History Server web interface once it's launched?",
    "answer": "Copy the URL displayed in the terminal and paste it into your web browser."
  },
  {
    "question": "How do you stop the Spark log visualization application?",
    "answer": "To stop the Spark log visualization application, enter the Ctrl-C key combination in the terminal where the application was launched."
  }
]