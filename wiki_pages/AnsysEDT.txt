<languages />
[[Category:Software]]

<translate>
<!--T:2-->
[https://www.ansys.com/products/electronics AnsysEDT] bundles electromagnetics simulation solutions such as Ansys HFSS, Ansys Maxwell, Ansys Q3D Extractor, Ansys SIwave, and Ansys Icepak using electrical CAD (ECAD) and mechanical CAD (MCAD) workflows.  AnsysEDT also integrates with the complete Ansys portfolio of thermal, fluid, and mechanical solvers for comprehensive multiphysics analysis.

= Licensing = <!--T:4-->

<!--T:2844-->
The Alliance is a hosting provider for AnsysEDT. This means we have the software installed on our clusters, but do not provide a generic license accessible to everyone. However, many institutions, faculties, and departments already have license servers that can be used if the legal aspects can be worked out.  Network changes would need to be made to enable the license server to be reached from the cluster compute nodes.  The Ansys software would then be able to check out licenses after loading the ansysedt module.  For help contact [[technical support]].

<!--T:10-->
== Configuring your license file ==
Specify your ansysedt license server by creating a file named <code>$HOME/.licenses/ansys.lic</code> consisting of two lines.  See [[Ansys#Configuring_your_license_file|Configuring your license file]] on the ansys wiki page for further details.

= Cluster batch job submission = <!--T:23-->

<!--T:1091-->
AnsysEDT can be run interactively in batch (non-gui) mode by first starting an salloc session with options <code>salloc --time=3:00:00 --tasks=8 --mem=16G --account=def-account</code> and then copy-pasting the full <code>ansysedt</code> command found in the last line of <i>script-local-cmd.sh</i>, being sure to manually specify $YOUR_AEDT_FILE.

=== Slurm scripts === <!--T:1092-->

<!--T:1093-->
Jobs may be submitted to a cluster queue with the <code>sbatch script-name.sh</code> command using either of the following single node scripts.  As of January 2023, the scripts had only been tested on Graham and therefore may be updated in the future as required to support other clusters.  Before using them, specify the simulation time, memory, number of cores and replace YOUR_AEDT_FILE with your input file name.   A full listing of command line options can be obtained by starting AnsysEDT in [[ANSYS#Graphical_use|graphical mode]] with commands <code>ansysedt -help</code> or <code>ansysedt -Batchoptionhelp</code> to obtain scrollable graphical popups.  

<!--T:1094-->
<tabs>
<tab name="Single node (command line)">
{{File
|name=script-local-cmd.sh
|lang="bash"
|contents=
#!/bin/bash

<!--T:1095-->
#SBATCH --account=account      # Specify your account (def or rrg)
#SBATCH --time=00-01:00        # Specify time (DD-HH:MM)
#SBATCH --mem=16G              # Specify memory (set to 0 to use all compute node memory)
#SBATCH --ntasks=8             # Specify cores (beluga 40, cedar 32 or 48, graham 32 or 44, narval 64)
#SBATCH --nodes=1              # Request one node (Do Not Change)

<!--T:1096-->
#module load StdEnv/2020
#module load ansysedt/2021R2

<!--T:1098-->
module load StdEnv/2023
module load ansysedt/2023R2    # or newer

<!--T:1097-->
# Uncomment next line to run a test example:
cp -f $EBROOTANSYSEDT/AnsysEM21.2/Linux64/Examples/HFSS/Antennas/TransientGeoRadar.aedt .

<!--T:2811-->
# Specify input file such as:
YOUR_AEDT_FILE="TransientGeoRadar.aedt"

<!--T:2812-->
# Remove previous output:
rm -rf $YOUR_AEDT_FILE.* ${YOUR_AEDT_FILE}results

<!--T:2813-->
# ---- do not change anything below this line ---- #

<!--T:2840-->
echo -e "\nANSYSLI_SERVERS= $ANSYSLI_SERVERS"
echo "ANSYSLMD_LICENSE_FILE= $ANSYSLMD_LICENSE_FILE"
echo -e "SLURM_TMPDIR= $SLURM_TMPDIR on $SLURMD_NODENAME\n"

<!--T:2841-->
export KMP_AFFINITY=disabled
ansysedt -monitor -UseElectronicsPPE -ng -distributed -machinelist list=localhost:1:$SLURM_NTASKS \
-batchoptions "TempDirectory=$SLURM_TMPDIR HPCLicenseType=pool HFSS/EnableGPU=0" -batchsolve "$YOUR_AEDT_FILE"
}}
</tab>
<tab name="Single node (options file)">
{{File
|name=script-local-opt.sh
|lang="bash"
|contents=
#!/bin/bash

<!--T:2816-->
#SBATCH --account=account      # Specify your account (def or rrg)
#SBATCH --time=00-01:00        # Specify time (DD-HH:MM)
#SBATCH --mem=16G              # Specify memory (set to 0 to allocate all compute node memory)
#SBATCH --ntasks=8             # Specify cores (beluga 40, cedar 32 or 48, graham 32 or 44, narval 64)
#SBATCH --nodes=1              # Request one node (Do Not Change)

<!--T:2817-->
#module load StdEnv/2020
#module load ansysedt/2021R2

<!--T:1099-->
module load StdEnv/2023
module load ansysedt/2023R2    # or newer

<!--T:2818-->
# Uncomment next line to run a test example:
cp -f $EBROOTANSYSEDT/AnsysEM21.2/Linux64/Examples/HFSS/Antennas/TransientGeoRadar.aedt .

<!--T:2819-->
# Specify input filename such as:
YOUR_AEDT_FILE="TransientGeoRadar.aedt"

<!--T:2820-->
# Remove previous output:
rm -rf $YOUR_AEDT_FILE.* ${YOUR_AEDT_FILE}results

<!--T:2821-->
# Specify options filename:
OPTIONS_TXT="Options.txt"

<!--T:2822-->
# Write sample options file
rm -f $OPTIONS_TXT
cat > $OPTIONS_TXT <<EOF
\$begin 'Config'
'TempDirectory'='$SLURM_TMPDIR'
'HPCLicenseType'='pool'
'HFSS/EnableGPU'=0
\$end 'Config'
EOF

<!--T:2823-->
# ---- do not change anything below this line ---- #

<!--T:2842-->
echo -e "\nANSYSLI_SERVERS= $ANSYSLI_SERVERS"
echo "ANSYSLMD_LICENSE_FILE= $ANSYSLMD_LICENSE_FILE"
echo -e "SLURM_TMPDIR= $SLURM_TMPDIR on $SLURMD_NODENAME\n"

<!--T:2843-->
export KMP_AFFINITY=disabled

<!--T:2845-->
ansysedt -monitor -UseElectronicsPPE -ng -distributed -machinelist list=localhost:1:$SLURM_NTASKS \
-batchoptions $OPTIONS_TXT -batchsolve "$YOUR_AEDT_FILE"
}}
</tab>
</tabs>

= Graphical use = <!--T:94-->

<!--T:941-->
Ansys programs may be run interactively in GUI mode on cluster compute nodes or Graham VDI Nodes.

== Compute nodes == <!--T:943--> 

<!--T:201-->
AnsysEDT  can be run interactively on a single compute node for up to 24 hours.  This approach is ideal for testing large simulations, since all cores and memory can be requested with salloc as described in [[VNC#Compute_Nodes|TigerVNC]].  Once connected with vncviewer, any of the following program versions can be started after loading the required modules as shown below.

<!--T:1672-->
::: Start an interactive session using the following form of the salloc command (to specify cores and available memory):
::: <code>salloc --time=3:00:00 --nodes=1 --cores=8 --mem=16G --account=def-group</code>
::: <code>xfwm4 --replace &</code> (then hit enter twice)
::: <code>module load StdEnv/2020 ansysedt/2021R2</code>, or
::: <code>module load StdEnv/2020 ansysedt/2023R2</code>, or
::: <code>module load StdEnv/2023 ansysedt/2023R2</code>, or
::: <code>module load StdEnv/2023 ansysedt/2024R2</code>  <--- !!! this module version is currently only on graham !!!
::: <code>ansysedt</code>
::: o Click <code>Tools -> Options -> HPC and Analysis Options -> Edit</code> then :
:::: 1) untick Use Automatic Settings box (required one time only)
:::: 2) under Machines tab do not change Cores (auto-detected from slurm)
::: o To run interactive analysis click:  <code>Project -> Analyze All</code>

== VDI nodes == <!--T:947-->

<!--T:125-->
Ansys programs can be run for up to 7 days on grahams VDI nodes (gra-vdi.alliancecan.ca) using 8 cores (16 cores max) and 128GB memory.  The VDI System provides GPU OpenGL acceleration and is therefore ideal for tasks that benefit from high performance graphics.  One might use VDI to create or modify simulation input files, post-process data or visualize simulation results.  To log in, connect with [[VNC#VDI_Nodes|TigerVNC]] then open a new terminal window and start one of the program versions shown below.  The vertical bar <code>|</code> notation is used to separate the various commands.   The maximum job size for any parallel job run on gra-vdi should be limited to 16&nbsp;cores to avoid overloading the servers and impacting other users.  To run two simultaneous GUI jobs (16&nbsp;cores max each) on gra-vdi, establish two independent vnc sessions.  For the first session, connect to gra-vdi3.sharcnet.ca with vnc. For the second session connect to gra-vdi4.sharecnet.ca also with vnc.  Then within each session, start Ansys in GUI mode and run your simulation.  Note that simultaneous simulations should in general be run in different directories to avoid file conflict issues.  Unlike compute nodes vnc connections (which impose slurm limits through salloc) there is no time limit constraint on gra-vdi when running simulations.

<!--T:1677-->
::: Open a terminal window and load a module:
::: <code>module load SnEnv</code>
::: <code>module load ansysedt/2023R2</code> (or older versions), or,
::: <code>module load SnEnv</code>
::: <code>module load ansys/2024R2[.04]</code>, or,
::: <code>module load CcEnv StdEnv/2023</code>
::: <code>module load ansys/2025R1[.02]</code> (or newer versions, testing)
::: Type <code>ansysedt</code> in the terminal and wait for the gui to start
::: The following only needs to be done once:
:::: click <code>Tools -> Options -> HPC and Analysis Options -> Options</code>
:::: change <code>HPC License</code> pulldown to <b>Pool</b> (allows > 4 cores to be used)
:::: click <code>OK</code>
::: ----------   EXAMPLES  ----------
::: To copy the 2023R2 Antennas examples directory into your account:
:::: login to a cluster such as graham
:::: <code>module load ansysedt/2023R2</code>
:::: <code>mkdir -p ~/Ansoft/$EBVERSIONANSYSEDT; cd ~/Ansoft/$EBVERSIONANSYSEDT; rm -rf Antennas</code>
:::: <code>cp -a $EBROOTANSYSEDT/v232/Linux64/Examples/HFSS/Antennas ~/Ansoft/$EBVERSIONANSYSEDT</code>
::: To run an example:
:::: open a simulation .aedt file then click <code>HFSS -> Validation Check</code>
:::: (if errors are reported by the validation check, close then reopen the simulation and repeat as required)
::::  to run simulation click <code>Project -> Analyze All</code>
:::: to quit without saving the converged solution click <code>File -> Close -> No </code>
::: If the program crashes and won't restart try running the following commands:
:::: <code>pkill -9 -u $USER -f "ansys*|mono|mwrpcss|apip-standalone-service"</code>
:::: <code>rm -rf ~/.mw</code> (ansysedt will re-run first-time configuration on startup)

= Site-Specific = <!--T:86-->

== SHARCNET license == <!--T:118-->

<!--T:90-->
The usage terms of the SHARCNET ANSYS License (which includes AnsysEDT) along with other various details maybe found in the SHARCNET license section of the Ansys wiki and will not be repeated here. 

==== License file ==== <!--T:92-->

<!--T:920-->
The SHARCNET Ansys license can be used for the AnsysEDT modules on any Alliance cluster by any researcher for free, by configuring your <code>ansys.lic</code> file as follows:
<source lang="bash">
[username@cluster:~] cat ~/.licenses/ansys.lic
setenv("ANSYSLMD_LICENSE_FILE", "1055@license3.sharcnet.ca")
setenv("ANSYSLI_SERVERS", "2325@license3.sharcnet.ca")
</source>

== Local modules == <!--T:93-->

<!--T:124-->
Use of local modules installed on gra-vdi or Graham may be of interest when there is a version available, which is not (yet) installed into the global environment on all systems either because it's very new or there are technical issues.  When starting programs from local modules on gra-vdi, you can select the CMC license server or accept the default SHARCNET license server as they are furnished with a startup wrapper including reading your <code>~/.licenses/ansysedt.lic</code> file. Suitable usage of AnsysEDT on gra-vdi includes: running a single test job interactively with up to 8&nbsp;cores and/or 128G&nbsp;RAM, create or modify simulation input files, post-process or visualize data.  A local ansysedt module can also be loaded on the Graham cluster; however, the procedure is slightly different as will be shown below.

=== Use on gra-vdi === <!--T:95-->

<!--T:955-->
When using gra-vdi, researchers have the choice of loading Ansys modules from our global environment (after loading CcEnv) or loading Ansys modules installed locally on the machine itself (after loading SnEnv).   To load a local ansysedt module on gra-vdi and then run the program in graphical mode, follow these steps :
# Connect to gra-vdi.computecanada.ca with [[VNC#VDI_Nodes|TigerVNC]].
# Open a new terminal window and load a module:
#; <code>module load SnEnv</code>
#; <code>module load ansysedt/2024R2.1</code> (or older)
# Start the Ansys Electromagnetics Desktop program by typing the following command: <code>ansysedt</code>
# Press <code>y</code> and <code>Enter</code> to accept the conditions. 
# Press <code>Enter</code> to accept the <code>n</code> option and use the SHARCNET license server by default (note that  <i>~/.licenses/ansysedt.lic</i> will be used if present, otherwise ANSYSLI_SERVERS and ANSYSLMD_LICENSE_FILE will be used if set in your environment for example to some other remote license server).  If you change <code>n</code> to <code>y</code> and hit enter,  the CMC license server will be used.

=== Use on Graham === <!--T:96-->

<!--T:2846-->
To load a local ansysedt module on graham and then run the program in graphical mode, follow these steps : (coming soon).

</translate>