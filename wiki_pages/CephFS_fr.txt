<languages />


Le système de fichiers CephFS peut être partagé par plusieurs hôtes d'instances OpenStack. Pour profiter de ce service, faites une demande à [mailto:nuage@tech.alliancecan.ca nuage@tech.alliancecan.ca].

La procédure est plutôt technique et nécessite des compétences Linux de base pour créer et modifier des fichiers, définir des permissions et créer des points de montage. Si vous avez besoin d’assistance, écrivez à [mailto:nuage@tech.alliancecan.ca nuage@tech.alliancecan.ca].

<span id="Procedure"></span>
=Procédure=
<b>REMARQUE : Plusieurs chaînes de caractères de l’interface OpenStack ne sont pas traduites en français.</b>

<span id="Request_access_to_shares"></span>
==Demander l’accès aux points de partage (<i>shares</i>)==

Si vous ne disposez pas déjà d’un quota pour ce service, écrivez à  [mailto:nauge@tech.alliancecan.ca nuage@tech.alliancecan.ca] et indiquez&nbsp;:
* le nom du projet OpenStack
* la capacité du quota requis (en Go)
* le nombre de points de partage requis

<span id="OpenStack_configuration:_Create_a_CephFS_share"></span>
== Configuration OpenStack : Créer un point de partage CephFS ==

<div class="mw-translate-fuzzy">
; Créez un point de partage.
: Dans <i>Project --> Share --> Shares</i>, cliquez sur <i>+Create Share</i>.
: <i>Share Name</i> = entrez un nom significatif pour votre projet (par exemple <i>project-name-shareName</i>)
: <i>Share Protocol</i> = CephFS
: <i>Taille</i> = taille requise pour le point de partage
: <i>Share Type</i> = cephfs
: <i>Zone de disponibilité</i> = nova
: Ne sélectionnez pas <i>Make visible for all</i>, autrement le point de partage sera accessible par tous les utilisateurs dans tous les projets.
: Cliquez sur le bouton <i>Create</i>.
[[File:Cephfs config.png|450px|thumb|left|Configurer CephFS avec l'interface Horizon]]
<br clear=all>
</div>

; Créez une règle pour générer une clé. 
: Dans <i>Project --> Share --> Shares --> colonne Actions</i>, sélectionnez <i>Manage Rules</i> du menu déroulant. 
: Cliquez sur le bouton <i>+Add Rule</i> à droite de la page.
: <i>Access Type</i> = cephx
: <i>Access Level</i> = sélectionnez <i>read-write</i> ou <i>read-only</i> (vous pouvez créer plusieurs règles à plusieurs niveaux) 
: <i>Access To</i> = entrez un nom significatif pour la clé; ce nom est important parce qu'il sera utilisé dans la configuration du client CepFS (ici le nom est i>MyCephFS-RW</i>).
[[File:Cephfs created.png|450px|thumb|left||Configuration correcte de CephFS]]
<br clear=all>

<div class="mw-translate-fuzzy">
; Prenez note des détails dont vous aurez besoin.
: Dans <i>Project --> Share --> Shares</i>, cliquez sur le nom du point de partage.
: Dans <i>Share Overview</i>, notez <i>Path</i>.
: Sous <i>Access Rules</i>, notez <i>Access Key</i> (les clés d’accès sont composées de 40 caractères et se terminent par le symbole =. Si vous ne voyez pas de clé d’accès, vous n’avez probablement pas spécifié que la règle d’accès était de type  <i>cephx</i>).
.
</div>

<span id="Attach_the_CephFS_network_to_your_VM"></span>
== Attacher le réseau CephFS à votre instance ==

=== Sur Arbutus ===
Le réseau CephFS est déjà disponible pour vos instances et donc vous n'avez rien à faire. Allez à [[CephFS/fr#VConfiguration_d'une instance_:_installer_et_configurer_un_client_CephFS |Configuration d'une instance&nbsp;: installer et configurer un client CephFS]] ci-dessous.

=== Sur SD4H/Juno ===
Vous devez attacher le réseau CephFS à l'instance.

;Sur le web
Pour chaque instances, sélectionnez <i>Instance --> Action --> Attach interface --> CephFS-Network</i>. Ne cochez pas la case <i>Fixed IP Address</i>.
[[File:Select CephFS Network.png|750px|thumb|left|]]
<br clear=all>
;Avec le [[OpenStack_command_line_clients|client OpenStack]]
Faites afficher la liste des serveurs et sélectionnez l'identifiant de celui que vous voulez attacher à CephFS 
<source lang='bash'>
$ openstack  server list 
+--------------------------------------+--------------+--------+-------------------------------------------+--------------------------+----------+
| ID                                   | Name         | Status | Networks                                  | Image                    | Flavor   |
+--------------------------------------+--------------+--------+-------------------------------------------+--------------------------+----------+
| 1b2a3c21-c1b4-42b8-9016-d96fc8406e04 | prune-dtn1   | ACTIVE | test_network=172.16.1.86, 198.168.189.3   | N/A (booted from volume) | ha4-15gb |
| 0c6df8ea-9d6a-43a9-8f8b-85eb64ca882b | prune-mgmt1  | ACTIVE | test_network=172.16.1.64                  | N/A (booted from volume) | ha4-15gb |
| 2b7ebdfa-ee58-4919-bd12-647a382ec9f6 | prune-login1 | ACTIVE | test_network=172.16.1.111, 198.168.189.82 | N/A (booted from volume) | ha4-15gb |
+--------------------------------------+--------------+--------+----------------------------------------------+--------------------------+----------+
</source>

Sélectionnez l'identifiant de l'instance que vous voulez attacher, choisissez la première et lancez 
<source lang='bash'>
$ openstack  server add network 1b2a3c21-c1b4-42b8-9016-d96fc8406e04 CephFS-Network
$ openstack  server list 
+--------------------------------------+--------------+--------+---------------------------------------------------------------------+--------------------------+----------+
| ID                                   | Name         | Status | Networks                                                            | Image                    | Flavor   |
+--------------------------------------+--------------+--------+---------------------------------------------------------------------+--------------------------+----------+
| 1b2a3c21-c1b4-42b8-9016-d96fc8406e04 | prune-dtn1   | ACTIVE | CephFS-Network=10.65.20.71; test_network=172.16.1.86, 198.168.189.3 | N/A (booted from volume) | ha4-15gb |
| 0c6df8ea-9d6a-43a9-8f8b-85eb64ca882b | prune-mgmt1  | ACTIVE | test_network=172.16.1.64                                            | N/A (booted from volume) | ha4-15gb |
| 2b7ebdfa-ee58-4919-bd12-647a382ec9f6 | prune-login1 | ACTIVE | test_network=172.16.1.111, 198.168.189.82                           | N/A (booted from volume) | ha4-15gb |
+--------------------------------------+--------------+--------+------------------------------------------------------------------------+--------------------------+----------+
</source>

Nous remarquons que le réseau CephFS est attaché à la première instance.

<span id="VM_configuration:_install_and_configure_CephFS_client"></span>
== Configuration d'une instance : installer et configurer un client CephFS ==

=== Paquets requis pour la famille Red Hat (RHEL, CentOS, Fedora, Rocky, Alma ) ===
Vérifiez quelles versions sont disponibles sur [https://download.ceph.com/ https://download.ceph.com/] et trouvez les répertoires <code>rpm-*</code> récents.
Depuis juillet 2024, <code>quincy</code> est la plus récente version stable. 
Les distributions compatibles sont listées dans [https://download.ceph.com/rpm-quincy/ https://download.ceph.com/rpm-quincy/].
Nous montrons ici des exemples de configurations pour <code>Enterprise Linux 8</code> et <code>Enterprise Linux 9</code>.

;  Installation des dépôts de paquets donnant accès aux paquets d'un client Ceph

<tabs>
<tab name="Enterprise Linux 8 - el8">
{{File
  |name=/etc/yum.repos.d/ceph.repo
  |lang="ini"
  |contents=
[Ceph]
name=Ceph packages for $basearch
baseurl=http://download.ceph.com/rpm-quincy/el8/$basearch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc

[Ceph-noarch]
name=Ceph noarch packages
baseurl=http://download.ceph.com/rpm-quincy/el8/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc

[ceph-source]
name=Ceph source packages
baseurl=http://download.ceph.com/rpm-quincy/el8/SRPMS
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
}}
</tab>
<tab name="Enterprise Linux 9 - el9">
{{File
  |name=/etc/yum.repos.d/ceph.repo
  |lang="ini"
  |contents=
[Ceph]
name=Ceph packages for $basearch
baseurl=http://download.ceph.com/rpm-quincy/el9/$basearch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc

[Ceph-noarch]
name=Ceph noarch packages
baseurl=http://download.ceph.com/rpm-quincy/el9/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc

[ceph-source]
name=Ceph source packages
baseurl=http://download.ceph.com/rpm-quincy/el9/SRPMS
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
}}
</tab>
</tabs>

Le répertoire epel doit être présent.
 sudo dnf install epel-release

Vous pouvez maintenant installer ceph lib, cephfs client aet autres dépendances.
 sudo dnf install -y libcephfs2 python3-cephfs ceph-common python3-ceph-argparse

<div class="mw-translate-fuzzy">
=== Paquets requis pour la famille Debian (Debian, Ubuntu, Mint, etc.) ===
Pour avoir le dépôt de paquets, trouvez le <code>{codename}</code> pour votre distribution avec <code>lsb_release -sc</code>
<source lang='bash'>
sudo apt-add-repository 'deb https://download.ceph.com/debian-quincy/ {codename} main'
</source>
</div>

Vous pouvez maintenant installer ceph lib, cephfs client et les autres dépendances.
<source lang='bash'>
 sudo apt-get install -y libcephfs2 python3-cephfs ceph-common python3-ceph-argparse
</source>

<span id="Configure_ceph_client"></span>
=== Configurer le client ceph ===

Quand le client est installé, créez le fichier <code>ceph.conf</code>.
La valeur de <code>mon host</code> est différente selon le nuage. 
<tabs>
<tab name="Arbutus">
{{File
  |name=/etc/ceph/ceph.conf
  |lang="ini"
  |contents=
[global]
admin socket = /var/run/ceph/$cluster-$name-$pid.asok
client reconnect stale = true
debug client = 0/2
fuse big writes = true
mon host = 10.30.201.3:6789,10.30.202.3:6789,10.30.203.3:6789
[client]
quota = true
}}
</tab>
<tab name="SD4H/Juno">
{{File
  |name=/etc/ceph/ceph.conf
  |lang="ini"
  |contents=
[global]
admin socket = /var/run/ceph/$cluster-$name-$pid.asok
client reconnect stale = true
debug client = 0/2
fuse big writes = true
mon host = 10.65.0.10:6789,10.65.0.12:6789,10.65.0.11:6789
[client]
quota = true
}}
</tab>
</tabs>

Les informations sur le moniteur trouvent dans le champ <i>Path</i> des détails du partage qui sera utilisé pour monter le volume. Si la valeur de la page Web est différente de ce qui est montré ici, cela signifie que la page wiki n'est pas à jour. 

Entrez le nom du client et le secret dans le fichier <code>ceph.keyring</code>. 

<div class="mw-translate-fuzzy">
{{File
  |name=/etc/ceph/ceph.keyring
  |lang="ini"
  |contents=
[client.MyCephFS-RW]
    key = <access Key>
}}
</div>

Encore une fois, la clé d'accès et le nom du client (ici <i>MyCephFS-RW</i>) se trouvent sous les règles d'accès sur la page Web de votre projet.
Cliquez sur <i>Project --> Share --> Shares</i>, puis cliquez sur le nom du point de partage.

<div class="mw-translate-fuzzy">
; Récupérez les informations de connexion dans la page <i>Share</i> pour votre connexion&nbsp;:
: Ouvrez les détails en cliquant sur le nom du point de partage dans la page <i>Shares</i>.
: Copiez le chemin d'accès complet du point de partage pour monter le système de fichiers.
</div>

;Montez le système de fichiers
:Créez un répertoire pour le point de montage quelque part dans votre hôte (ici <code>/cephfs</code>)</li>
<source lang="bash">
 mkdir /cephfs
</source>
:Vous pouvez utiliser le pilote ceph pour monter votre périphérique CephFS de façon permanente en ajoutant ce qui suit dans le fstab de l'instance.
<tabs>
<tab name="Arbutus">
{{File
  |name=/etc/fstab
  |lang="txt"
  |contents=
:/volumes/_nogroup/f6cb8f06-f0a4-4b88-b261-f8bd6b03582c /cephfs/ ceph name=MyCephFS-RW 0  2
}}
</tab>
<tab name="SD4H/Juno">
{{File
  |name=/etc/fstab
  |lang="txt"
  |contents=
:/volumes/_nogroup/f6cb8f06-f0a4-4b88-b261-f8bd6b03582c /cephfs/ ceph name=MyCephFS-RW,mds_namespace=cephfs_4_2,x-systemd.device-timeout=30,x-systemd.mount-timeout=30,noatime,_netdev,rw 0  2
}}
</tab>
</tabs>

'''Remarque''' : le caractère <code>:</code> non standard devant le chemin d'accès au périphérique n'est pas une erreur de frappe.
Les options de montage sont différentes selon les systèmes.
L'option <code>namespace</code> est requise pour SD4H/Juno, tandis que les autres options sont des ajustements de performance.

Vous pouvez aussi faire le montage directement en ligne de commande.
<tabs>
<tab name="Arbutus">
<code>
sudo mount -t ceph :/volumes/_nogroup/f6cb8f06-f0a4-4b88-b261-f8bd6b03582c /cephfs/ -o name=MyCephFS-RW
</code>
</tab>
<tab name="SD4H/Juno">
<code>
sudo mount -t ceph :/volumes/_nogroup/f6cb8f06-f0a4-4b88-b261-f8bd6b03582c /cephfs/ -o name=MyCephFS-RW,mds_namespace=cephfs_4_2,x-systemd.device-timeout=30,x-systemd.mount-timeout=30,noatime,_netdev,rw
</code>
</tab>
</tabs>

CephFS peut aussi être monté directement dans votre espace de travail via ceph-fuse.

Installez la bibliothèque ceph-fuse.

<source lang="bash">
sudo dnf install ceph-fuse
</source>
Pour que le montage soit disponible dans votre espace utilisateur, retirez le code de commentaire <code>user_allow_other</code> dans le fichier <code>fuse.conf</code>.

{{File
  |name=/etc/fstab
  |lang="txt"
  |contents=
# mount_max = 1000
user_allow_other
}}

Vous pouvez maintenant monter cephFS dans votre espace /home.
<source lang="bash">
mkdir ~/my_cephfs
ceph-fuse my_cephfs/ --id=MyCephFS-RW --conf=~/ceph.conf --keyring=~/ceph.keyring   --client-mountpoint=/volumes/_nogroup/f6cb8f06-f0a4-4b88-b261-f8bd6b03582c
</source>
Notez que le nom du client est ici le <code>--id</code>. Le contenu de <code>ceph.conf</code> et de <code>ceph.keyring</code> est exactement le même que pour le montage du noyau ceph.

<span id="Notes"></span>
=Remarques=

Un point de partage particulier peut disposer de plusieurs clés utilisateur. Cela permet un accès plus précis au système de fichiers, par exemple si vous avez besoin que certains hôtes accèdent au système de fichiers uniquement en lecture seule. Si vous disposez de plusieurs clés pour un point de partage, vous pouvez ajouter les clés supplémentaires à votre hôte et modifier la procédure de montage ci-dessus. Ce service n'est pas disponible pour les hôtes extérieurs à la grappe OpenStack.

[[Category:Cloud]]