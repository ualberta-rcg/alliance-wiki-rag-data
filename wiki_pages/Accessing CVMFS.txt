[[Category:CVMFS]]
<languages />

<translate>
<!--T:1-->
= Introduction =
We provide repositories of software and data via a file system called the [[CVMFS|CERN Virtual Machine File System]] (CVMFS). On our systems, CVMFS is already set up for you, so the repositories are automatically available for your use. For more information on using our software environment, please refer to wiki pages [[Available software]], [[Using modules]], [[Python]], [[R]] and [[Installing software in your home directory]].

<!--T:2-->
The purpose of this page is to describe how you can install and configure CVMFS on <i>your</i> computer or cluster, so that you can access the same repositories (and software environment) on your system that are available on ours.

<!--T:3-->
The software environment described on this page has been [https://ssl.linklings.net/conferences/pearc/pearc19_program/views/includes/files/pap139s3-file1.pdf presented] at Practices and Experience in Advanced Research Computing 2019 (PEARC 2019).

<!--T:4-->
= Before you start =
{{Note|Note to staff: see the [https://wiki.alliancecan.ca/wiki/CVMFS_client_setup internal documentation].|reminder}}

</translate>
{{Panel
  |title=Important
  |panelstyle=callout
  |content=
<translate><!--T:55--> <b>Please [[Accessing_CVMFS#Subscribe_to_announcements|subscribe to announcements]] to remain informed of important changes regarding our software environment and CVMFS, and fill out the [https://docs.google.com/forms/d/1eDJEeaMgooVoc4lTkxcZ9y65iR8hl4qeXMOEU9slEck/viewform registration form]. If use of our software environment contributes to your research, please acknowledge it according to [https://alliancecan.ca/en/services/advanced-research-computing/acknowledging-alliance these guidelines].</b> (We would appreciate that you also cite our [https://ssl.linklings.net/conferences/pearc/pearc19_program/views/includes/files/pap139s3-file1.pdf paper]). </translate>
}}
<translate>
<!--T:5-->
== Subscribe to announcements ==
Occasionally, changes will be made regarding CVMFS or the software or other content provided by our CVMFS repositories, which <b>may affect users</b> or <b>require administrators to take action</b> in order to ensure uninterrupted access to our CVMFS repositories. Subscribe to the cvmfs-announce@gw.alliancecan.ca mailing list in order to receive important but infrequent notifications about these changes, by emailing [mailto:cvmfs-announce+subscribe@gw.alliancecan.ca cvmfs-announce+subscribe@gw.alliancecan.ca] and then replying to the confirmation email you subsequently receive. (Our staff can alternatively subscribe [https://groups.google.com/u/0/a/gw.alliancecan.ca/g/cvmfs-announce/about here].)

<!--T:6-->
== Terms of use and support ==
The CVMFS client software is provided by CERN. Our CVMFS repositories are provided <b>without any warranty</b>. We reserve the right to limit or block your access to the CVMFS repositories and software environment if you violate applicable [https://ccdb.computecanada.ca/agreements/user_aup_2021/user_display terms of use] or at our discretion.

<!--T:7-->
== CVMFS requirements ==
=== For a single system ===
To install CVMFS on an individual system, such as your laptop or desktop, you will need:
* A supported operating system (see [[Accessing_CVMFS#Minimal_requirements|Minimal requirements below]]).
* Support for [https://en.wikipedia.org/wiki/Filesystem_in_Userspace FUSE].
* Approximately 50 GB of available local storage, for the cache. (It will only be filled based on usage, and a larger or smaller cache may be suitable in different situations. For light use on a personal computer, just ~ 5-10 GB may suffice. See [https://cvmfs.readthedocs.io/en/stable/cpt-configure.html#sct-cache cache settings] for more details.)
* Outbound HTTP access to the internet.
** Or at least outbound HTTP access to one or more local proxy servers.

<!--T:8-->
If your system lacks FUSE support or local storage, or has limited network connectivity or other restrictions, you may be able to use some [https://cvmfs.readthedocs.io/en/stable/cpt-hpc.html other option].

<!--T:9-->
=== For multiple systems ===
If multiple CVMFS clients are deployed, for example on a cluster, in a laboratory, campus or other site, each system must meet the above requirements, and the following considerations apply as well:
* We recommend that you deploy forward caching HTTP proxy servers at your site to improve performance and bandwidth usage, especially if you have a large number of clients. Refer to [https://cvmfs.readthedocs.io/en/stable/cpt-squid.html Setting up a Local Squid Proxy].
** Note that if you have only one such proxy server it will be a single point of failure for your site. Generally, you should have at least two local proxies at your site, and potentially additional nearby or regional proxies as backups.
* It is recommended to synchronize the identity of the <code>cvmfs</code> service account across all client nodes (e.g. using LDAP or other means).
** This facilitates use of an [https://cvmfs.readthedocs.io/en/stable/cpt-configure.html#alien-cache alien cache] and should be done <b>before</b> CVMFS is installed. Even if you do not anticipate using an alien cache at this time, it is easier to synchronize the accounts initially than to try to potentially change them later.

<!--T:10-->
== Software environment requirements ==
=== Minimal requirements ===
*Supported operating systems:
** Linux: with a Kernel 2.6.32 or newer for our 2016 and 2018 environments, and 3.2 or newer for the 2020 environment. 
** Windows: with Windows Subsystem for Linux version 2, with a distribution of Linux that matches the requirement above.
** Mac OS: only through a virtual machine.
* CPU: x86 CPU supporting at least one of SSE3, AVX, AVX2 or AVX512 instruction sets.

<!--T:11-->
=== Optimal requirements ===
* Scheduler: Slurm or Torque, for tight integration with OpenMPI applications.
* Network interconnect: Ethernet, InfiniBand or OmniPath, for parallel applications.
* GPU: NVidia GPU with CUDA drivers (7.5 or newer) installed, for CUDA-enabled applications. (See below for caveats about CUDA.)
* As few Linux packages installed as possible (fewer packages reduce the odds of conflicts).

<!--T:12-->
= Installing CVMFS =
If you wish to use [https://docs.ansible.com/ansible/latest/index.html Ansible], a [https://github.com/cvmfs-contrib/ansible-cvmfs-client CVMFS client role] is provided as-is, for basic configuration of a CVMFS client on an RPM-based system. 
Also, some [https://github.com/ComputeCanada/CVMFS/tree/main/cvmfs-cloud-scripts scripts] may be used to facilitate installing CVMFS on cloud instances.
Otherwise, use the following instructions.

<!--T:54-->
== Pre-installation ==
It is recommended that the local CVMFS cache (located at <code>/var/lib/cvmfs</code> by default, configurable via the <code>CVMFS_CACHE_BASE</code> setting) be on a dedicated file system so that the storage usage of CVMFS is not shared with that of other applications. Accordingly, you should provision that file system <b>before</b> installing CVMFS.

<!--T:22-->
== Installation and configuration ==
For installation instructions, refer to [https://cvmfs.readthedocs.io/en/stable/cpt-quickstart.html#getting-the-software Getting the Software].

<!--T:74-->
For standard client configuration, see [https://cvmfs.readthedocs.io/en/stable/cpt-quickstart.html#setting-up-the-software Setting up the Software] and [http://cvmfs.readthedocs.io/en/stable/apx-parameters.html#client-parameters Client parameters].

<!--T:73-->
The <code>soft.computecanada.ca</code> repository is provided by the default configuration, so no additional steps are required to access it (though you may wish to include it in <code>CVMFS_REPOSITORIES</code> in your client configuration).

== Testing == <!--T:27-->

<!--T:28-->
* First ensure that the repositories you want to test are listed in <code>CVMFS_REPOSITORIES</code>.
* Validate the configuration:
{{Command|sudo cvmfs_config chksetup}}
* Make sure to address any warnings or errors that are reported.
* Check that the repositories are OK:
{{Command|cvmfs_config probe}}

<!--T:29-->
If you encounter problems, [https://cvmfs.readthedocs.io/en/stable/cpt-quickstart.html#troubleshooting this debugging guide] may help.

<!--T:33-->
= Enabling our environment in your session =
Once you have mounted the CVMFS repository, enabling our environment in your sessions is as simple as running the bash script <code>/cvmfs/soft.computecanada.ca/config/profile/bash.sh</code>. 
This will load some default modules. If you want to mimic a specific cluster exactly, simply define the environment variable <code>CC_CLUSTER</code> to one of <code>beluga</code>, <code>cedar</code> or <code>graham</code> before using the script, for example: 
{{Command|export CC_CLUSTER{{=}}beluga}}
{{Command|source /cvmfs/soft.computecanada.ca/config/profile/bash.sh}}

<!--T:34-->
The above command <b>will not run anything if your user ID is below 1000</b>. This is a safeguard, because you should not rely on our software environment for privileged operation. If you nevertheless want to enable our environment, you can first define the environment variable <code>FORCE_CC_CVMFS=1</code>, with the command
{{Command|export FORCE_CC_CVMFS{{=}}1}}
or you can create a file <code>$HOME/.force_cc_cvmfs</code> in your home folder if you want it to always be active, with
{{Command|touch $HOME/.force_cc_cvmfs}}

<!--T:35-->
If, on the contrary, you want to avoid enabling our environment, you can define <code>SKIP_CC_CVMFS=1</code> or create the file <code>$HOME/.skip_cc_cvmfs</code> to ensure that the environment is never enabled in a given account.

<!--T:36-->
== Customizing your environment ==
By default, enabling our environment will automatically detect a number of features of your system, and load default modules. You can control the default behaviour by defining specific environment variables prior to enabling the environment. These are described below. 

<!--T:37-->
=== Environment variables ===
==== <code>CC_CLUSTER</code> ====
This variable is used to identify a cluster. It is used to send some information to the system logs, as well as define behaviour relative to licensed software. By default, its value is <code>computecanada</code>. You may want to set the value of this variable if you want to have system logs tailored to the name of your system.

<!--T:38-->
==== <code>RSNT_ARCH</code> ====
This environment variable is used to identify the set of CPU instructions supported by the system. By default, it will be automatically detected based on <code>/proc/cpuinfo</code>. However if you want to force a specific one to be used, you can define it before enabling the environment. The supported instruction sets for our software environment are:
* sse3
* avx
* avx2
* avx512

<!--T:39-->
==== <code>RSNT_INTERCONNECT</code> ====
This environment variable is used to identify the type of interconnect supported by the system. By default, it will be automatically detected based on the presence of <code>/sys/module/opa_vnic</code> (for Intel OmniPath) or <code>/sys/module/ib_core</code> (for InfiniBand). The fall-back value is <code>ethernet</code>. The supported values are
* omnipath
* infiniband
* ethernet

<!--T:40-->
The value of this variable will trigger different options of transport protocol to be used in OpenMPI.

<!--T:61-->
==== <code>RSNT_CUDA_DRIVER_VERSION</code> ====
This environment variable is used to hide or show some versions of our CUDA modules, according to the required version of NVidia drivers, as documented [[https://docs.nvidia.com/deploy/cuda-compatibility/index.html here]]. If not defined, this is detected based on the files founds under <code>/usr/lib64/nvidia</code>. 

<!--T:62-->
For backward compatibility reasons, if no library is found under <code>/usr/lib64/nvidia</code>, we assume that the driver versions are enough for CUDA 10.2. This is because this feature was introduced just as CUDA 11.0 was released.

<!--T:63-->
Defining <code>RSNT_CUDA_DRIVER_VERSION=0.0</code> will hide all versions of CUDA.

<!--T:64-->
==== <code>RSNT_LOCAL_MODULEPATHS</code> ====
{{Warning|title=Warning|content=This environment variable is intended to be used only for hierarchical module trees which are installed via EasyBuild, so that they follow the same hierarchical structure as our modules.

<!--T:77-->
If you want to add a local path to a flat tree, instead add the command *after* sourcing our scripts
{{Command|module use --priority 1 /path/to/your/flat/module/tree}}
if you want your module tree to have higher priority than ours, or 
{{Command|module use --priority -1 /path/to/your/flat/module/tree}}
if you want your module tree to have lower priority than ours.
}}
This environment variable allows to define locations for local module trees, which will be automatically mesh into our central tree. To use it, define
{{Command|export RSNT_LOCAL_MODULEPATHS{{=}}/opt/software/easybuild/modules}}
and then install your [[EasyBuild]] recipe using 
{{Command|eb --installpath /opt/software/easybuild <your recipe>.eb}}

<!--T:65-->
This will use our module naming scheme to install your recipe locally, and it will be picked up by the module hierarchy. For example, if this recipe was using the <code>iompi,2018.3</code> toolchain, the module will become available after loading the <code>intel/2018.3</code> and the <code>openmpi/3.1.2</code> modules.

<!--T:79-->
==== <code>LMOD_SYSTEM_DEFAULT_MODULES</code> ====
This environment variable defines which modules are loaded by default. If it is left undefined, our environment will define it to load the <code>StdEnv</code> module, which will load by default a version of the Intel compiler, and a version of OpenMPI.

<!--T:42-->
==== <code>MODULERCFILE</code> ====
This is an environment variable used by Lmod to define the default version of modules and aliases. You can define your own <code>modulerc</code> file and add it to the environment variable <code>MODULERCFILE</code>. This will take precedence over what is defined in our environment.

<!--T:43-->
=== System paths ===
While our software environment strives to be as independent from the host operating system as possible, there are a number of system paths that are taken into account by our environment to facilitate interaction with tools installed on the host operating system. Below are some of these paths. 

<!--T:44-->
==== <code>/opt/software/modulefiles</code> ====
If this path exists, it will automatically be added to the default <code>MODULEPATH</code>. This allows the use of our software environment while also maintaining locally installed modules. 

<!--T:45-->
==== <code>$HOME/modulefiles</code> ====
If this path exists, it will automatically be added to the default <code>MODULEPATH</code>. This allows the use of our software environment while also allowing installation of modules inside of home directories.

<!--T:46-->
==== <code>/opt/software/slurm/bin</code>, <code>/opt/software/bin</code>, <code>/opt/slurm/bin</code> ====
These paths are all automatically added to the default <code>PATH</code>. This allows your own executable to be added in the search path.

<!--T:57-->
== Installing software locally ==
Since June 2020, we support installing additional modules locally and have it discovered by our central hierarchy. This was discussed and implemented in [https://github.com/ComputeCanada/software-stack/issues/11 this issue]. 

<!--T:58-->
To do so, first identify a path where you want to install local software. For example <code>/opt/software/easybuild</code>. Make sure that folder exists. Then, export the environment variable <code>RSNT_LOCAL_MODULEPATHS</code>: 
{{Command|export RSNT_LOCAL_MODULEPATHS{{=}}/opt/software/easybuild/modules}}

<!--T:59-->
If you want this branch of the software hierarchy to be found by your users, we recommend you define this environment variable in the cluster's common profile. Then, install the software packages you want using [[EasyBuild]]: 
{{Command|eb --installpath /opt/software/easybuild <some easyconfig recipe>}}

<!--T:60-->
This will install the piece of software locally, using the hierarchical layout driven by our module naming scheme. It will also be automatically found when users load our compiler, MPI and Cuda modules.

<!--T:47-->
= Caveats =
== Use of software environment by system administrators ==
If you perform privileged system operations, or operations related to CVMFS, [[Accessing_CVMFS#Enabling_our_environment_in_your_session|ensure]] that your session does <i>not</i> depend on our software environment when performing any such operations. For example, if you attempt to update CVMFS using YUM while your session uses a Python module loaded from CVMFS, YUM may run using that module and lose access to it during the update, and the update may become deadlocked. Similarly, if your environment depends on CVMFS and you reconfigure CVMFS in a way that temporarily interrupts access to CVMFS, your session may interfere with CVMFS operations, or hang. (When these precautions are taken, in most cases CVMFS can be updated and reconfigured without interrupting access to CVMFS for users, because the update or reconfiguration itself will complete successfully without encountering a circular dependency.)

<!--T:49-->
== Software packages that are not available ==
On our systems, a number of commercial software packages are made available to authorized users according to the terms of the license owners, but they are not available externally, and following the instructions on this page will not grant you access to them. This includes for example the Intel and Portland Group compilers. While the modules for the Intel and PGI compilers are available, you will only have access to the redistributable parts of these packages, usually the shared objects. These are sufficient to run software packages compiled with these compilers, but not to compile new software.

<!--T:50-->
== CUDA location ==
For CUDA-enabled software packages, our software environment relies on having driver libraries installed in the path <code>/usr/lib64/nvidia</code>. However on some platforms, recent NVidia drivers will install libraries in <code>/usr/lib64</code> instead. Because it is not possible to add <code>/usr/lib64</code> to the <code>LD_LIBRARY_PATH</code> without also pulling in all system libraries (which may have incompatibilities with our software environment), we recommend that you create symbolic links in <code>/usr/lib64/nvidia</code> pointing to the installed NVidia libraries. The script below will install the drivers and create the symbolic links that are needed (adjust the driver version that you want) 

<!--T:56-->
{{File|name=script_for_redhat.sh|contents=
NVIDIA_DRV_VER="410.48"
nv_pkg=( "nvidia-driver" "nvidia-driver-libs" "nvidia-driver-cuda" "nvidia-driver-cuda-libs" "nvidia-driver-NVML" "nvidia-driver-NvFBCOpenGL" "nvidia-modprobe" )
yum -y install ${nv_pkg[@]/%/-${NVIDIA_DRV_VER{{)}}{{)}}
for file in $(rpm -ql ${nv_pkg[@]}); do
  [ "${file%/*}" = '/usr/lib64' ] && [ ! -d "${file}" ] && \ 
  ln -snf "$file" "${file%/*}/nvidia/${file##*/}"
done
}}

</translate>
{{File|name=script_for_ubuntu.sh|contents=
#! /usr/bin/bash
# Use the 'major series' number for the package name
VER="570"
nv_pkg=( "libnvidia-cfg1-${VER}-server:amd64"
    		"libnvidia-compute-${VER}-server:amd64"
		"libnvidia-decode-${VER}-server:amd64"
		"libnvidia-encode-${VER}-server:amd64"
		"libnvidia-extra-${VER}-server:amd64"
		"libnvidia-fbc1-${VER}-server:amd64"
		"libnvidia-gl-${VER}-server:amd64"
		"xserver-xorg-video-nvidia-${VER}-server" )
# apt --no-install-recommends install ${nv_pkg[*]}
[ -d "/usr/lib64/nvidia/" ] {{!}}{{!}} mkdir "/usr/lib64/nvidia/"
for file in $(dpkg --listfiles "${nv_pkg[@]}"); do
	[ "${file%/*}" = '/usr/lib/x86_64-linux-gnu' ] && \
	[ ! -d "${file}" ] && \
	ln -snf "$file" "/usr/lib64/nvidia/${file##*/}"
done
}}
<translate>

<!--T:75-->
== <code>LD_LIBRARY_PATH</code> ==
Our software environment is designed to use [https://en.wikipedia.org/wiki/Rpath RUNPATH]. Defining <code>LD_LIBRARY_PATH</code> is [https://gms.tf/ld_library_path-considered-harmful.html not recommended] and can lead to the environment not working. 

<!--T:52-->
== Missing libraries ==
Because we do not define <code>LD_LIBRARY_PATH</code>, and because our libraries are not installed in default Linux locations, binary packages, such as Anaconda, will often not find libraries that they would usually expect. Please see our documentation on [[Installing_software_in_your_home_directory#Installing_binary_packages|Installing binary packages]].

<!--T:53-->
== dbus ==
For some applications, <code>dbus</code> needs to be installed. This needs to be installed locally, on the host operating system.
</translate>