<languages />


This page presents more advanced features of the [[META-Farm]] package.

<span id="Resubmitting_failed_cases_automatically"></span>
= Resoumettre automatiquement les cas qui ont échoué =

If your farm is particularly large, that is, if it needs more resources than ''NJOBS_MAX x job_run_time'', where ''NJOBS_MAX'' is the maximum number of jobs one is allowed to submit, you will have to run <code>resubmit.run</code> after the original farm finishes running-- perhaps more than once. You can do it by hand, but with META you can also automate this process. To enable this feature, add the <code>-auto</code> switch to your <code>submit.run</code> or <code>resubmit.run</code> command:

 $ submit.run N -auto

This can be used in either SIMPLE or META mode. If your original <code>submit.run</code> command did not have the <code>-auto</code> switch, you can add it to <code>resubmit.run</code> after the original farm finishes running, to the same effect.

When you add <code>-auto</code>, <code>(re)submit.run</code> submits one more (serial) job, in addition to the farm jobs. The purpose of this job is to run the <code>resubmit.run</code> command automatically right after the current farm finishes running. The job script for this additional job is <code>resubmit_script.sh</code>, which should be present in the farm directory; a sample file is automatically copied there when you run <code>farm_init.run</code>. The only customization you need to do to this file is to correct the account name in the <code>#SBATCH -A</code> line.

If you are using <code>-auto</code>, the value of the <code>NJOBS_MAX</code> parameter defined in the <code>config.h</code> file should be at least one smaller than the largest number of jobs you can submit on the cluster.
E.g. if the largest number of jobs one can submit on the cluster is 999 and you intend to use <code>-auto</code>, set <code>NJOBS_MAX</code> to 998.

When using <code>-auto</code>, if at some point the only cases left to be processed are the ones which failed earlier, auto-resubmission will stop, and farm computations will end. This is to avoid an infinite loop on badly-formed cases which will always fail. If this happens, you will have to address the reasons for these cases failing before attempting to resubmit the farm.  You can see the relevant messages in the file <code>farm.log</code> created in the farm directory.

<span id="Running_a_post-processing_job_automatically"></span>
= Exécuter automatiquement une tâche de post-traitement =

Another advanced feature is the ability to run a post-processing job automatically once all the cases from table.dat have been '''successfully''' processed.
If any cases failed-- ''i.e.'' had a non-zero exit status-- the post-processing job will not run.
To enable this feature, simply create a script for the post-processing job with the name <code>final.sh</code> inside the farm directory
This job can be of any kind-- serial, parallel, or an array job.

This feature uses the same script, <code>resubmit_script.sh</code>, described for [[#Resubmitting failed cases automatically|<code>-auto</code>]] above.
Make sure <code>resubmit_script.sh</code> has the correct account name in the <code>#SBATCH -A</code> line.

The automatic post-processing feature also causes more serial jobs to be submitted, above the number you request.
Adjust the parameter <code>NJOBS_MAX</code> in <code>config.h</code> accordingly (''e.g.'' if the cluster has a job limit of 999, set it to 998).
However, if you use both the auto-resubmit and the auto-post-processing features, they will together only submit ''one'' additional job.
You do not need to subtract 2 from <code>NJOBS_MAX</code>.

System messages from the auto-resubmit feature are logged in <code>farm.log</code>, in the root farm directory.

<div class="mw-translate-fuzzy">
= Mode WHOLE_NODE =
Starting from the version 1.0.3, meta-farm supports packaging individual serial farming jobs into whole node jobs. This made it possible to use the package on clusters Niagara/Trillium. This mode is off by default. To enable it, edit the file config.h inside your farm directory. Specifically, you need to set <code>WHOLE_NODE=1</code>, and set the variable <code>NWHOLE</code> to the number of CPU cores per node (40 for Niagara; 192 for Trillium).
</div> 

In the WHOLE_NODE mode, the positive integer argument for the <code>submit.run</code> command changes its meaning: instead of being the number of meta-jobs, now it is the number of whole nodes to be used in META mode. For example, consider this command:

<code>
$ submit.run 2
</code>

If the WHOLE_NODE mode is enabled, the above command will allocate 2 whole nodes, which will be used to run up to 384 concurrent serial tasks (192 tasks on each node) using META mode (dynamic workload balancing). These tasks are executed as separate threads within whole-node jobs.

The "-1" argument for <code>submit.run</code> preserves its original meaning: run the farm using the SIMPLE mode. The number of actual (whole node) jobs is computed as <code>Number_of_cases / NWHOLE</code>.

Important details:

* The advanced features "Automatic job resubmission" and "Automatic post-processing job" will only work on Trillium if you place the following line at the end of your ~/.bashrc file:
<code>
module load StdEnv
</code>
* The WHOLE_NODE mode can only be used for serial farming. (That is, it cannot be used for multi-threaded, MPI, or GPU farming).
* The WHOLE_NODE mode can also be used on other clusters (not just on Trillium). It may be advantageous in situations when the queue wait time for whole node jobs becomes shorter that the queue wait time for serial jobs.

<span id="Additional_information"></span>
=Information additionnelle= 

== Using the git repository == 

To use META on a cluster where it is not installed as a module
you can clone the package from our git repository:

 $ git clone https://git.computecanada.ca/syam/meta-farm.git
Then modify your $PATH variable to point to the <code>bin</code> subdirectory of the newly created <code>meta-farm</code> directory.
Assuming you executed <code>git clone</code> inside your home directory, do this:
 $ export PATH=~/meta-farm/bin:$PATH

Then proceed as shown in the META [[META: A package for job farming#Quick start|Quick start]] from the <code>farm_init.run</code> step.

==Passing additional sbatch arguments==

If you need to use additional <code>sbatch</code> arguments (like <code>--mem 4G, --gres=gpu:1</code> ''etc.''), 
add them to <code>job_script.sh</code> as separate <code>#SBATCH</code> lines.

Or if you prefer, you can add them at the end of the <code>submit.run</code> or <code>resubmit.run</code> command
and they will be passed to <code>sbatch</code>, ''e.g.:''

<source lang="bash">
   $  submit.run  -1  --mem 4G
</source>

<span id="Multi-threaded_applications"></span>
==Applications multifils== 

For [[Running_jobs#Threaded_or_OpenMP_job|multi-threaded]] applications (such as those that use [[OpenMP]], for example), 
add the following lines to <code>job_script.sh</code>:

<source lang="bash">
   #SBATCH --cpus-per-task=N
   #SBATCH --mem=M
   export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
</source>

...where ''N'' is the number of CPU cores to use, and ''M'' is the total memory to reserve in megabytes.
You may also supply <code>--cpus-per-task=N</code> and <code>--mem=M</code> as arguments to <code>(re)submit.run</code>.

<span id="MPI_applications"></span>
==Applications MPI== 

For applications that use [[MPI]], 
add the following lines to <code>job_script.sh</code>:

<source lang="bash">
   #SBATCH --ntasks=N  
   #SBATCH --mem-per-cpu=M
</source>

...where ''N'' is the number of CPU cores to use, and ''M'' is the memory to reserve for each core, in megabytes.
You may also supply <code>--ntasks=N</code> and <code>--mem-per-cpu=M</code> as arguments to <code>(re)submit.run</code>.
See [[Advanced MPI scheduling]] for information about more-complicated MPI scenarios.

Also add <code>srun</code> before the path to your code inside <code>single_case.sh</code>,''e.g.'':

<source lang="bash">
   srun  $COMM
</source>

Alternatively, you can prepend <code>srun</code> to each line of <code>table.dat</code>:

<source lang="bash">
   srun /path/to/mpi_code arg1 arg2
   srun /path/to/mpi_code arg1 arg2
   ...
   srun /path/to/mpi_code arg1 arg2
</source>

<span id="GPU_applications"></span>
==Applications GPU== 

For applications which use GPUs, modify <code>job_script.sh</code> following the guidance at [[Using GPUs with Slurm]]: 

<source lang="bash">
#SBATCH --gres=gpu[[:type]:number]
</source>

You may also wish to copy the utility <code>~syam/bin/gpu_test</code> to your <code>~/bin</code> directory (only on Nibi), 
and put the following lines in <code>job_script.sh</code> right before the <code>task.run</code> line:

<source lang="bash">
~/bin/gpu_test
retVal=$?
if [ $retVal -ne 0 ]; then
    echo "No GPU found - exiting..."
    exit 1
fi
</source>

This will catch those rare situations when there is a problem with the node which renders the GPU unavailable. 
If that happens to one of your meta-jobs, and you don't detect the GPU failure somehow, 
then the job will try (and fail) to run all your cases from <code>table.dat</code>.

<span id="Environment_variables_and_--export"></span>
== Variables d'environnement et --export ==

All the jobs generated by META package inherit the environment present when you run <code>submit.run</code> or <code>resubmit.run</code>. 
This includes all the loaded modules and environment variables. 
META relies on this behaviour for its work, using some environment variables to pass information between scripts. 
You have to be careful not to break this default behaviour, such as can happen if you use the <code>--export</code> switch. 
If you need to use <code>--export</code> in your farm, make sure <code>ALL</code> is one of the arguments to this command,
''e.g.'' <code>--export=ALL,X=1,Y=2</code>.

If you need to pass values of custom environment variables to all of your farm jobs 
(including auto-resubmitted jobs and the post-processing job if there is one),
do not use <code>--export</code>. Instead, set the variables on the command line as in this example:

<source lang="bash">
   $  VAR1=1 VAR2=5 VAR3=3.1416 submit.run ...
</source>

Here <code>VAR1, VAR2, VAR3</code> are custom environment variables which will be passed to all farm jobs.

<span id="Example:_Numbered_input_files"></span>
== Exemple : fichiers d'entrée numérotés ==

Suppose you have an application called <code>fcode</code>, and each case needs to read a separate file from standard input-– 
say <code>data.X</code>, where ''X'' ranges from 1 to ''N_cases''.  
The input files are all stored in a directory <code>/home/user/IC</code>.
Ensure <code>fcode</code> is on your <code>$PATH</code> 
(''e.g.'', put <code>fcode</code> in <code>~/bin</code>, and ensure <code>/home/$USER/bin</code> is added to <code>$PATH</code> in <code>~/.bashrc</code>),
or use a full path to <code>fcode</code> in <code>table.dat</code>. 
Create <code>table.dat</code> in the farm META directory like this:

   fcode < /home/user/IC/data.1
   fcode < /home/user/IC/data.2
   fcode < /home/user/IC/data.3
   ...

You might wish to use a shell loop to create <code>table.dat</code>, ''e.g.'':

<source lang="bash">
   $  for ((i=1; i<=100; i++)); do echo "fcode < /home/user/IC/data.$i"; done >table.dat
</source>

<span id="Example:_Input_file_must_have_the_same_name"></span>
== Exemple : fichier d'entrée doit avoir le même nom == 

Some applications expect to read input from a file with a prescribed and unchangeable name, like <code>INPUT</code> for example.
To handle this situation each case must run in its own subdirectory, 
and you must create an input file with the prescribed name in each subdirectory. 
Suppose for this example that you have prepared the different input files for each case 
and stored them in <code>/path/to/data.X</code>, where ''X'' ranges from 1 to ''N_cases''.
Your <code>table.dat</code> can contain nothing but the application name, over and over again:

   /path/to/code
   /path/to/code
   ...

Add a line to <code>single_case.sh</code>
which copies the input file into the farm ''sub''directory for each case--
the first line in the example below:

<source lang="bash">
   cp /path/to/data.$ID INPUT
   $COMM
   STATUS=$?
</source>

==Accéder à chaque paramètre d'un cas==
The examples shown so far assume that each line in the cases table is an executable statement, starting with either the name of the executable file (when it is on your <code>$PATH</code>) or the full path to the executable file, and then listing the command line arguments particular to that case, or something like <code> < input.$ID</code> if your code expects to read a standard input file.

In the most general case, you may want to be able to access all the columns in the table individually. That can be done by modifying <code>single_case.sh</code>:

<source lang="bash">
...
# ++++++++++++  This part can be customized:  ++++++++++++++++
#  $ID contains the case id from the original table
#  $COMM is the line corresponding to the case $ID in the original table, without the ID field
mkdir RUN$ID
cd RUN$ID

# Converting $COMM to an array:
COMM=( $COMM )
# Number of columns in COMM:
Ncol=${#COMM[@]}
# Now one can access the columns individually, as ${COMM[i]} , where i=0...$Ncol-1
# A range of columns can be accessed as ${COMM[@]:i:n} , where i is the first column
# to display, and n is the number of columns to display
# Use the ${COMM[@]:i} syntax to display all the columns starting from the i-th column
# (use for codes with a variable number of command line arguments).

# Call the user code here.
...

# Exit status of the code:
STATUS=$?
cd ..
# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
...
</source>

For example, you might need to provide to your code ''both'' a standard input file ''and'' a variable number of command line arguments. 
Your cases table will look like this:

<div class="mw-translate-fuzzy">
/path/to/IC.1 0.1
   /path/to/IC.2 0.2 10
   ...
</div>

The way to implement this in <code>single_case.sh</code> is as follows:

<source lang="bash">
# Call the user code here.
/path/to/code ${COMM[@]:1} < ${COMM[0]}
</source>

==Reducing waste==

Here is one potential problem when one is running multiple cases per job:  What if the number of running meta-jobs times the requested run-time per meta-job (say, 3 days) is not enough to process all your cases? E.g., you managed to start the maximum allowed 1000 meta-jobs, each of which has a 3-day run-time limit. That means that your farm can only process all the cases in a single run if the ''average_case_run_time x N_cases < 1000 x 3d = 3000'' CPU days. Once your meta-jobs start hitting the 3-day run-time limit, they will start dying in the middle of processing one of your cases. This will result in up to 1000 interrupted cases calculations. This is not a big deal in terms of completing the work--- <code>resubmit.run</code> will find all the cases which failed or never ran, and will restart them automatically. But this can become a waste of CPU cycles. On average, you will be wasting ''0.5 x N_jobs x average_case_run_time''. E.g., if your cases have an average run-time of 1 hour, and you have 1000 meta-jobs running, you will waste about 500 CPU-hours or about 20 CPU-days, which is not acceptable.

Fortunately, the scripts we are providing have some built-in intelligence to mitigate this problem. This is implemented in <code>task.run</code> as follows:

* The script measures the run-time of each case, and adds the value as one line in a scratch file <code>times</code> created in directory <code>/home/$USER/tmp/$NODE.$PID/</code>. (See [[#Output files|Output files]].) This is done by all running meta-jobs.
* Once the first 8 cases were computed, one of the meta-jobs will read the contents of the file <code>times</code> and compute the largest 12.5% quantile for the current distribution of case run-times. This will serve as a conservative estimate of the run-time for your individual cases, ''dt_cutoff''.  The current estimate is stored in file <code>dt_cutoff</code> in <code>/home/$USER/tmp/$NODE.$PID/</code>.
* From now on, each meta-job will estimate if it has the time to finish the case it is about to start computing, by ensuring that ''t_finish - t_now > dt_cutoff''.  Here, ''t_finish'' is the time when the job will die because of the job's run-time limit, and ''t_now'' is the current time. If it computes that it doesn't have the time, it will exit early, which will minimize the chance of a case aborting half-way due to the job's run-time limit.
* At every subsequent power of two number of computed cases (8, then 16, then 32 and so on) ''dt_cutoff'' is recomputed using the above algorithm. This will make the ''dt_cutoff'' estimate more and more accurate. Power of two is used to minimize the overheads related to computing ''dt_cutoff''; the algorithm will be equally efficient for both very small (tens) and very large (many thousands) number of cases.
* The above algorithm reduces the amount of CPU cycles wasted due to jobs hitting the run-time limit by a factor of 8, on average.

As a useful side effect, every time you run a farm you get individual run-times for all of your cases stored in <code>/home/$USER/tmp/$NODE.$PID/times</code>. 
You can analyze that file to fine-tune your farm setup, for profiling your code, etc.

<span id="Troubleshooting"></span>
=Dépannage=

Here we explain typical error messages you might get when using this package.

<span id="Problems_affecting_multiple_commands"></span>
==Problèmes avec des commandes multiples== 

==="Non-farm directory, or no farm has been submitted; exiting"===
Either the current directory is not a farm directory, or you never ran <code>submit.run</code> for this farm.

==Problems with submit.run==
===Wrong first argument: XXX (should be a positive integer or -1) ; exiting===
Use the correct first argument: -1 for the SIMPLE mode, or a positive integer N (number of requested meta-jobs) for the META mode.

==="lockfile is not on path; exiting"===
Make sure the utility <code>lockfile</code> is on your $PATH.
This utility is critical for this package.  
It provides serialized access of meta-jobs to the <code>table.dat</code> file, that is, 
it ensures that two different meta-jobs do not read the same line of <code>table.dat</code> at the same time.

==="Non-farm directory (config.h, job_script.sh, single_case.sh, and/or table.dat are missing); exiting"===
Either the current directory is not a farm directory, or some important files are missing. Change to the correct (farm) directory, or create the missing files.

==="-auto option requires resubmit_script.sh file in the root farm directory; exiting"===
You used the <code>-auto</code> option, but you forgot to create the <code>resubmit_script.sh</code> file inside the root farm directory. A sample <code>resubmit_script.sh</code> is created automatically when you use <code>farm_init.run</code>.

==="File table.dat doesn't exist. Exiting"===
You forgot to create the <code>table.dat</code> file in the current directory, or perhaps you are running <code>submit.run</code> not inside one of your farm sub-directories.

==="Job runtime sbatch argument (-t or --time) is missing in job_script.sh. Exiting"===
Make sure you provide a run-time limit for all meta-jobs as an <code>#SBATCH</code> argument inside your <code>job_script.sh</code> file.
The run-time is the only one which cannot be passed as an optional argument to <code>submit.run</code>.

==="Wrong job runtime in job_script.sh - nnn . Exiting"===
You didn't format properly the run-time argument inside your <code>job_script.sh</code> file.

==="Something wrong with sbatch farm submission; jobid=XXX; aborting"===
==="Something wrong with a auto-resubmit job submission; jobid=XXX; aborting"===
With either of the two messages, there was an issue with submitting jobs with <code>sbatch</code>.
The cluster's scheduler might be misbehaving, or simply too busy. Try again a bit later.

==="Couldn't create subdirectories inside the farm directory ; exiting"===
==="Couldn't create the temp directory XXX ; exiting"===
==="Couldn't create a file inside XXX ; exiting"===
With any of these three messages, something is wrong with a file system: 
Either permissions got messed up, or you have exhausted a quota. Fix the issue(s), then try again.

==Problems with resubmit.run==
==="Jobs are still running/queued; cannot resubmit"===
You cannot use <code>resubmit.run</code> until all meta-jobs from this farm have finished running. 
Use <code>list.run</code> or <code>queue.run</code> to check the status of the farm.

==="No failed/unfinished jobs; nothing to resubmit"===
Your farm was 100% processed.  There are no more (failed or never-ran) cases to compute.

==Problems with running jobs==
==="Too many failed (very short) cases - exiting"===
This happens if the first <code>$N_failed_max</code> cases are very short-- less than <code>$dt_failed</code> seconds in duration. 
See the discussion in section [[#job_script.sh|job_script.sh]] above.
Determine what is causing the cases to fail and fix that,
or else adjust the <code>$N_failed_max</code> and <code>$dt_failed</code> values in <code>config.h</code>.

==="lockfile is not on path on node XXX"===
As the error message suggests, somehow the utility <code>lockfile</code> is not on your <code>$PATH</code> on some node.
Use <code>which lockfile</code> to ensure that the utility is somewhere in your <code>$PATH</code>.
If it is in your <code>$PATH</code> on a login node, then something went wrong on that particular compute node, 
for example a file system may have failed to mount.

==="Exiting after processing one case (-1 option)"===
This is not an error message.   It simply tells you that you submitted the farm with <code>submit.run -1</code> (one case per job mode), so each meta-job is exiting after processing a single case.

==="Not enough runtime left; exiting."===
This message tells you that the meta-job would likely not have enough time left to process the next case (based on the analysis of run-times for all the cases processed so far), so it is exiting early.

==="No cases left; exiting."===
This is not an error message.  This is how each meta-job normally finishes, when all cases have been computed.

==="Only failed cases left; cannot auto-resubmit; exiting"===
This can only happen if you used the <code>-auto</code> switch when submitting the farm. 
Find the failed cases with <code>Status.run -f</code>, fix the issue(s) causing the cases to fail, then run <code>resubmit.run</code>.



''Page enfant de'' [[META: A package for job farming]]