<languages />

<translate>
=Major upgrade of our Advanced Research Computing infrastructure= <!--T:1-->

<!--T:2-->
Our Advanced Research Computing infrastructure is undergoing major changes starting in winter 2024-2025, most new systems are now expected to be available to users during summer 2025. These changes will improve High Performance Computing (HPC) and Cloud services for Canadian researchers. This page will be regularly updated to keep you informed of the activities concerning the transition to the new equipment.

<!--T:31-->
The infrastructure renewal will replace the nearly 80% of our current equipment that is approaching end-of-life. The new equipment will offer faster processing speeds, greater storage capacity, and improved reliability.

=New system details= <!--T:9-->

<!--T:32-->
{| class="wikitable"
|-
| '''New System''' || '''Old System to be Replaced''' || '''Documentation'''
|-
| [[Arbutus]] || [[Cloud]] (as a virtual infrastructure there is no change to the cloud interface.) || [[Arbutus|see this page]]
|-
| [[Rorqual/en|Rorqual]] || [[Beluga/en|Béluga]] || [[Rorqual/en|see this page]]
|-
| [[Fir]] || |[[Cedar]] || [[Fir|see this page]]
|-
| [[Trillium]] || [[Niagara]] & [[Mist]] || [[Trillium|see this page]]
|-
| [[Nibi]] || [[Graham]] || [[Nibi|see this page]]
|}

<!--T:6-->
=System capacity, reductions and outages =
During the installation and the transition to the new systems, outages and reductions will be unavoidable due to constraints on space and electrical power. 
We recommend that you consider the possibility of outages when you plan research programs, graduate examinations, etc.

<!--T:402-->
For a list of completed events, please see [[Infrastructure renewal completed events]].

<!--T:39-->
{| class="wikitable"
|-
| '''Start Time''' || '''End Time''' || '''Status''' || '''System''' || '''Type''' || '''Description'''

<!--T:280-->
|-
| Sept 30, 2025 || Ongoing || Scheduled || Niagara || End of Service ||  
On September 30, 2025, the '''Niagara''' compute cluster will be retired.

<!--T:403-->
''Through September, '''Niagara''' will continue to operate at reduced capacity following the staged reductions ('''863 → 647 → 431 → 215''' nodes) until retirement on September 30.''

<!--T:404-->
''Related:'' '''Trillium''' datamovers, gateway server, and '''HPSS (nearline)''' are expected to be available at this time. For migration guidance, see: [https://docs.alliancecan.ca/wiki/Transition_from_Niagara_to_Trillium Transition from Niagara to Trillium].

<!--T:281-->
|-
| Sept 16, 2025 || Ongoing || In Progress || Mist || End of Service ||  
On September 16, 2025, the '''Mist''' compute cluster was retired.

<!--T:406-->
Please transition new work to '''Trillium'''. See: [https://docs.alliancecan.ca/wiki/Trillium_Quickstart Trillium Quickstart].

<!--T:282-->
|-
| Sept 4, 2025 || Sept 29, 2025 (26 days) || In Progress || Niagara || Reduction ||  
Capacity reductions ahead of Niagara decommissioning. Timeline:  
* Sept 4: '''Niagara''' reduced to '''863''' compute nodes.  
* Sept 9: '''Niagara Open OnDemand''' will be decommissioned; brief data centre connectivity interruption around '''9:00 a.m. (EDT)'''; end of day capacity '''647''' nodes.  
* Sept 11: '''Trillium Open OnDemand''' goes live: [https://ondemand.scinet.utoronto.ca ondemand.scinet.utoronto.ca].  
* Sept 16: Full-day maintenance; '''Niagara''' continues at '''431''' compute nodes; '''Mist''' retired.  
* Sept 24: '''Niagara''' reduced to '''215''' compute nodes.

<!--T:401-->
|-
| Sept 12, 2025 || Ongoing || In Progress || Cedar || End of Service ||  
On September 12, 2025, the '''Cedar''' compute cluster was retired.

<!--T:407-->
'''Data Access'''
*Files stored on '''Cedar''' are already available on '''Fir''' because the two clusters share the same file systems. No action is required regarding your stored files.

<!--T:408-->
Starting September 12, please submit your jobs to another cluster on our [[National systems|new national infrastructure]], including '''Fir'''.

<!--T:278-->
|-
| August 25, 2025 || August 28, 2025 '''(Extended to September 5)''' (12 days) || Completed || Fir || Reduction ||  
To support commissioning of new cooling equipment, some compute nodes on '''Fir''' will be '''temporarily unavailable''' from '''Monday, August 25''' through '''Thursday, August 28'''. '''Fir''' will operate '''below full capacity''' during this period, with normal capacity expected to resume by '''Friday, August 29'''.

<!--T:409-->
* '''Fewer jobs will run at the same time''' on '''Fir'''  
* Jobs may '''start more slowly''' and '''wait times may be longer'''

<!--T:38-->
|-
| Sept 1, 2025 || Ongoing || In Progress || Graham || End of Service ||  
On September 1, 2025, the '''Graham''' compute cluster was retired.

<!--T:410-->
'''Data Access'''
*Files stored on '''Graham''' are already available on '''Nibi''' because the two clusters share the same file systems. No action is required regarding your stored files.

<!--T:411-->
Starting September 1, please submit your jobs to another cluster on our [[National systems|new national infrastructure]], including '''Nibi'''].

<!--T:151-->
|-
| July 15, 2025 ||August 11, 2025 '''(Extended to September 8)''' (55 days) || Completed || Béluga, Narval, Juno || Outage ||
From July 15 to August 25, 2025, the tape storage system behind the TSM service, including backups and migrated /nearline data, will be unavailable during its migration to a new data centre.

<!--T:412-->
* '''File backup and restore services will be unavailable.'''
** Please keep a backup copy of important data on another system and double-check delete operations.
** Restores of data created before July 15 will resume once the tape system returns to service.
** Data created or modified between July 15 and August 11 cannot be recovered.
* On Béluga and Narval, files in /nearline that have been migrated to tape will not be accessible.
** To identify these files, see [https://docs.alliancecan.ca/wiki/Using_nearline_storage#Transferring_data_from_/nearline “Transferring data from /nearline.”]
* '''The TSM service itself will be fully unavailable.'''

<!--T:413-->
*'''Note:''' Other storage systems, compute nodes on all clusters, and Juno Cloud instances will remain fully operational. Globus transfers will function normally except when accessing tape-migrated /nearline files.
|}

<!--T:418-->
=Resource Allocation Competition (RAC)=
The [https://www.alliancecan.ca/en/services/advanced-research-computing/accessing-resources/resource-allocation-competition Resource Allocation Competition]  will be impacted by this transition, but the application process remains the same. <br>
2024/25 allocations will remain in effect on retiring clusters while each cluster remains in service.  The 2025/26 allocations will be implemented everywhere once all new clusters are in service.<br>
Because the old clusters will mostly be out of service before all new ones are available, if you hold both a 2024 and a 2025 RAC award you will experience a period when neither award is available to you. You will be able to compute with your default allocation (<code>def-xxxxxx</code>) on each new cluster as soon as it goes into service, but the 2025 RAC allocations will only become available when all new clusters are in service.
</translate>
<translate>
=User training resources= <!--T:35-->
</translate>
{| class="wikitable"
|-
<translate>
<!--T:47-->
| '''Course Title''' || '''Course Provider''' || '''Instructor''' || '''Date''' || '''Description''' || '''Audience''' || '''Format''' || '''Registration'''
</translate>
|-
<translate>
<!--T:52-->
| [https://education.scinet.utoronto.ca/enrol/index.php?id=1389 HPC105: Intro to SciNet and Trillium] || SciNet || SciNet Education Team || Available Anytime (updated August 2025) || Self-guided course on using SciNet systems (Trillium): account set-up, first login, and running computations. For returning users from legacy systems (Niagara, Mist), includes workflow migration guidance to Trillium. || Prospective users of [https://docs.alliancecan.ca/wiki/Trillium Trillium] and New users to SciNet || Self-guided online course (Estimated time: ~4 hours) || [https://education.scinet.utoronto.ca/enrol/index.php?id=1389 Access the course here/Alliance CCDB account is required]
</translate>
|-
<translate>
<!--T:48-->
| [https://youtu.be/nRX8zTIVEXk Migrating to the upgraded national systems] || SHARCNET || Sergey Mashchenko || Wednesday, July 30, 2025, 12:00pm EDT ||Most of the Alliance national systems have been undergoing a major upgrade during this spring and summer. They have been effectively rebuilt from scratch using up-to-date hardware, bringing a significant increase in computing capacity, memory, and storage to the users. The upgraded clusters have new names: Graham became Nibi, Beluga - Rorqual, Cedar - Fir, Niagara - Trillium. (The remaining cluster - Narval - is not having an upgrade this cycle.) Some of the upgraded systems are already online, not at a full capacity yet. The plan is to make all of them available by the end of July. <br><br>This webinar is to address the concerns and questions the users of the existing systems might have. How will the upgrade affect my workflow? Are there significant changes to the way the job scheduling and file systems operate in the new clusters? How to make the best use of the increased computing capacity of the upgraded clusters, in particular in regards of the computing nodes becoming much "fatter" (many more cpu cores per node)? As the most significant change will happen to the way the GPU computing is done, the webinar will cover this in detail. <br><br>There will be time for questions at the end, please bring your questions and concerns.  || Prospective users of the upgraded systems || Webinar; recordings and materials from previous SHARCNET webinars are available at [http://youtube.sharcnet.ca http://youtube.sharcnet.ca].  || Past
|-
| Workflow Hacks for Large Datasets in HPC || Simon Fraser University (SFU) / West DRI || Alex Razoumov || Tuesday, May 20, 2025, 10:00 AM PT || Over the years, we have delivered webinars on tools that can significantly enhance research workflows involving large datasets. In this session, we will highlight some of these valuable tools:<br>• <b>In-situ visualization:</b> enables interactive rendering of large in-memory arrays without the need to store them to disk.<br>• <b>Lossy 3D data compression:</b> reduces the size of 3D datasets by up to 100X with no visible artifacts, making it ideal for storage and archival.<br>• <b>Distributed storage:</b> helps manage vast amounts of data across multiple locations.<br>• <b>DAR (Disk ARchiver):</b> a modern, high-performance alternative to TAR that offers indexing, differential archives, and faster extraction.|| Users working with large datasets || Webinar; <br>Recordings and materials from previous related webinars are freely available at [https://training.westdri.ca https://training.westdri.ca].  || Past
</translate>
|-
<translate>
<!--T:49-->
| [https://training.sharcnet.ca/courses/enrol/index.php?id=210 Mastering GPU Efficiency] || SHARCNET || Sergey Mashchenko || Available Anytime || This online self-paced course provides basic training for [https://training.sharcnet.ca/courses/mod/glossary/showentry.php?eid=112&displayformat=dictionary Alliance] users on using GPUs on our [https://training.sharcnet.ca/courses/mod/glossary/showentry.php?eid=86&displayformat=dictionary national systems]. Modern GPUs (such as NVIDIA A100 and H100) are massively parallel and very expensive devices. Most of GPU jobs are incapable of utilizing these GPUs efficiently, either due to the problem size being too small to saturate the GPU, or due to the intermittent (bursty) GPU utilization pattern. This course will teach you how to measure the GPU utilization of your jobs on our clusters, and show how to use the two NVIDIA technologies - MPS (Multi-Process Service) and MIG (Multi-Instance GPU) - to improve GPU utilization. || Prospective users of the upgraded systems ||1-hour self-paced online course with a certificate of completion|| [https://training.sharcnet.ca/courses/enrol/index.php?id=210 Access the course here/Alliance CCDB account is required]
</translate>
|-
<translate>
<!--T:50-->
|Introduction to the Fir cluster || Simon Fraser University (SFU) / West DRI || Alex Razoumov || September 2025 (rescheduled) || SFU’s newest cluster, Fir, is now expected to be available during summer 2025.  In this webinar, we will give an overview of the cluster and its hardware, walk through the filesystems and their recommended usage, talk about job submission policies and overall best practices for using the cluster. || Prospective users of [[Fir]] cluster || Webinar || Registration details to be updated closer to the new date
</translate>
|-
<translate>
<!--T:51-->
| [https://youtu.be/pxY3G3BhwyA Survival guide for the upcoming GPU upgrades] || SHARCNET || Sergey Mashchenko || Wednesday, November 20, 2024, 12:00 PM to 1:00 PM ET || In the coming months, national systems will be undergoing significant upgrades. In particular, older GPUs (P100, V100) will be replaced with the newest H100 GPUs from NVIDIA. The total GPU computing power of the upgraded systems will grow by a factor of 3.5, but the number of GPUs will decrease significantly (from 3200 to 2100). This will present a significant challenge for users, as the usual practice of using a whole GPU for each process or MPI rank will no longer be feasible in most cases. Fortunately, NVIDIA provides two powerful technologies that can be used to mitigate this situation: MPS (Multi-Process Service) and MIG (Multi-Instance GPU). The presentation will walk the audience through both technologies and discuss the ways they can be used on the clusters. The discussion will include how to determine which approach will work best for specific code, and a live demonstration will be given at the end. ||Prospective users of the upgraded systems. Users intending to use a substantial amount of H100 resources (e.g., more than one GPU at a time, and/or over 24 hours runtime) || 1-hour [https://youtu.be/pxY3G3BhwyA presentation] and [https://helpwiki.sharcnet.ca/wiki/images/1/1d/MIG_MPS.pdf slides] || Past
</translate>
|}
<translate>

= Frequently asked questions = <!--T:15--> 

<!--T:16-->
== Will my data be copied to its new system? ==
Data migration to the new systems is the responsibility of each National Host Site who will inform you of what you need to do.

<!--T:36-->
== Will my files be deleted when a system is undergoing a complete data center shutdown as part of renewal activities? ==
No, your files will not be deleted. During renewal activities, each National Host Site will migrate /project and /home data from the existing storage system to the new storage system once it is installed. These migrations typically occur during outages, but specific details may vary by National Host Site. Each National Host Site will keep users informed of any specific, user-visible effects. 
Additionally, tape systems for backups and /nearline data are not being replaced, so backups and /nearline data will remain unchanged. 
For further technical questions, please email [[technical support]]. This goes directly to our ticketing system, where a support expert can provide a detailed response.

<!--T:17-->
== When will outages occur? ==
Each National Host Site will have its own schedule for outages as the installation of and transition to new equipment proceeds. As usual, specific outages will be described on [https://status.alliancecan.ca our system status web page]. We will provide more general updates on this wiki page and you will periodically receive emails with updates and outage notices.

<!--T:18-->
== Whom can I contact for questions about the transition? ==
Contact our [[technical support]]. They will try their best to answer any questions they can.

<!--T:19-->
== Will my jobs and applications still be able to run on the new system? ==
Generally yes, but the new CPUs and GPUs may require recompilation or reconfiguration of some applications. More details will be provided as the transition unfolds.

<!--T:20-->
== Will the software from the current systems still be available? ==
Yes, our [[Standard software environments|standard software environment]] will be available on the new systems.

<!--T:34-->
== Will commercial, licensed software be migrated to the new systems? ==
Yes, the plan is that the current commercial software licenses will be transitioned from an old system to the new replacement so to the extent possible users should see identical access to those special applications (Gaussian, AMS/ADF, etc.). There is a small risk that the software providers will change their licensing terms for the new system. Such issues will be addressed individually as they come up.

<!--T:21-->
== Will there be staggered outages? ==
We will do our best to limit overlapping outages, but  because we are very constrained by delivery schedules and funding deadlines, there will probably be periods when several of our systems are simultaneously offline. Outages will be announced as early as possible.

<!--T:28-->
== Can I purchase old hardware after equipment upgrades? ==
Most of the equipment is legally the property of the hosting institution.  When the equipment is retired, the host institution manages its disposal following that institution's guidelines. This typically involves "e-cycling"--- recycling the equipment rather than selling it. If you're looking to acquire the old hardware, it's best to contact the host institution directly, as they may have specific policies or options for selling equipment.

</translate>