<languages />

<translate>

<!--T:37-->
"CUDA® is a parallel computing platform and programming model developed by NVIDIA for general computing on graphical processing units (GPUs)."<ref>[https://developer.nvidia.com/cuda-toolkit NVIDIA CUDA Home Page]. CUDA is a registered trademark of NVIDIA.</ref>

<!--T:38-->
It is reasonable to think of CUDA as a set of libraries and associated C, C++, and Fortran compilers that enable you to write code for GPUs. See [[OpenACC Tutorial]] for another set of GPU programming tools.

== Quick start guide == <!--T:39-->

<!--T:40-->
===Compiling===
Here we show a simple example of how to use the CUDA C/C++ language compiler, <code>nvcc</code>, and run code created with it. For a longer tutorial in CUDA programming, see [[CUDA tutorial]].

<!--T:41-->
First, load a CUDA [[Utiliser des modules/en|module]].
<source lang="console">
$ module purge
$ module load cuda
</source>

<!--T:42-->
The following program will add two numbers together on a GPU. Save the file as <code>add.cu</code>. <i>The <code>cu</code> file extension is important!</i>. 

</translate>
{{File  
  |name=add.cu
  |lang="c++"
  |contents=
#include <iostream>

__global__ void add (int *a, int *b, int *c){
  *c = *a + *b;
}

int main(void){
  int a, b, c;
  int *dev_a, *dev_b, *dev_c;
  int size = sizeof(int);
  
  //  allocate device copies of a,b, c
  cudaMalloc ( (void**) &dev_a, size);
  cudaMalloc ( (void**) &dev_b, size);
  cudaMalloc ( (void**) &dev_c, size);
  
  a=2; b=7;
  //  copy inputs to device
  cudaMemcpy (dev_a, &a, size, cudaMemcpyHostToDevice);
  cudaMemcpy (dev_b, &b, size, cudaMemcpyHostToDevice);
  
  // launch add() kernel on GPU, passing parameters
  add <<< 1, 1 >>> (dev_a, dev_b, dev_c);
  
  // copy device result back to host
  cudaMemcpy (&c, dev_c, size, cudaMemcpyDeviceToHost);
  std::cout<<a<<"+"<<b<<"="<<c<<std::endl;
  
  cudaFree ( dev_a ); cudaFree ( dev_b ); cudaFree ( dev_c );
}
}}
<translate>

<!--T:43-->
Compile the program with <code>nvcc</code> to create an executable named <code>add</code>.
<source lang="console">
$ nvcc add.cu -o add
</source>

<!--T:44-->
=== Submitting jobs===
To run the program, create a Slurm job script as shown below. Be sure to replace <code>def-someuser</code> with your specific account (see [[Running_jobs#Accounts_and_projects|Accounts and projects]]). For options relating to scheduling jobs with GPUs see [[Using GPUs with Slurm]]. 
{{File
  |name=gpu_job.sh
  |lang="sh"
  |contents=
#!/bin/bash
#SBATCH --account=def-someuser
#SBATCH --gres=gpu:1              # Number of GPUs (per node)
#SBATCH --mem=400M                # memory (per node)
#SBATCH --time=0-00:10            # time (DD-HH:MM)
./add #name of your program
}}

<!--T:45-->
Submit your GPU job to the scheduler with 
<source lang="console">
$ sbatch gpu_job.sh
Submitted batch job 3127733
</source>For more information about the <code>sbatch</code> command and running and monitoring jobs, see [[Running jobs]].

<!--T:46-->
Once your job has finished, you should see an output file similar to this:
<source lang="console">
$ cat slurm-3127733.out
2+7=9
</source>
If you run this without a GPU present, you might see output like <code>2+7=0</code>. 

<!--T:48-->
=== Linking libraries ===
If you have a program that needs to link some libraries included with CUDA, for example [https://developer.nvidia.com/cublas cuBLAS], compile with the following flags
<source lang="console">
nvcc -lcublas -Xlinker=-rpath,$CUDA_PATH/lib64
</source>

<!--T:47-->
To learn more about how the above program works and how to make the use of GPU parallelism, see [[CUDA tutorial]].

== Troubleshooting == <!--T:49-->

=== Compute capability === <!--T:50-->

<!--T:51-->
NVidia has created this technical term, "which indicates what features are supported by that GPU and specifies some hardware parameters for that GPU."
See [https://docs.nvidia.com/cuda/cuda-programming-guide/01-introduction/cuda-platform.html#cuda-platform-compute-capability-sm-version Compute Capability and Streaming Multiprocessor Versions]
for more details.

<!--T:53-->
The following errors are connected with compute capability:

<!--T:54-->
<pre>
nvcc fatal : Unsupported gpu architecture 'compute_XX'
</pre>

<!--T:55-->
<pre>
no kernel image is available for execution on the device (209)
</pre>

<!--T:56-->
If you encounter either of these errors, you may be able to fix it by adding the correct <i>flag</i> to the <code>nvcc</code> call:

<!--T:57-->
<pre>
-gencode arch=compute_XX,code=[sm_XX,compute_XX]
</pre>

<!--T:58-->
If you are using <code>cmake</code>, provide the following flag:

<!--T:59-->
<pre>
cmake .. -DCMAKE_CUDA_ARCHITECTURES=XX
</pre>

<!--T:60-->
where “XX” is the compute capability of the Nvidia GPU that you expect to run the application on. 
To find the value to replace “XX“, see [https://developer.nvidia.com/cuda/gpus CUDA GPU Compute Capability] and omit the decimal point.

<!--T:61-->
<b>For example,</b> if you will run your code on a Narval A100 node, the NVidia table gives its compute capability as "8.0".
The correct flag to use when compiling with <code>nvcc</code> is then:

<!--T:62-->
<pre>
-gencode arch=compute_80,code=[sm_80,compute_80]
</pre>

<!--T:63-->
The flag to supply to <code>cmake</code> is:

<!--T:64-->
<pre>
cmake .. -DCMAKE_CUDA_ARCHITECTURES=80
</pre>

</translate>