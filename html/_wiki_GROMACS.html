<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<title>GROMACS - Alliance Doc</title>
<script>(function(){var className="client-js";var cookie=document.cookie.match(/(?:^|; )ccwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split('%2C').forEach(function(pref){className=className.replace(new RegExp('(^| )'+pref.replace(/-clientpref-\w+$|[^\w-]+/g,'')+'-clientpref-\\w+( |$)'),'$1'+pref+'$2');});}document.documentElement.className=className;}());RLCONF={"wgBreakFrames":false,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"aDpFJTv@Jr5vJi8BNccnuwAAA80","wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"GROMACS","wgTitle":"GROMACS","wgCurRevisionId":176552,"wgRevisionId":176552,"wgArticleId":1881,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Software","BiomolecularSimulation"
],"wgPageViewLanguage":"en","wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"GROMACS","wgRelevantArticleId":1881,"wgIsProbablyEditable":false,"wgRelevantPageIsProbablyEditable":false,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgULSAcceptLanguageList":[],"wgMFDisplayWikibaseDescriptions":{"search":false,"watchlist":false,"tagline":false},"wgCiteReferencePreviewsActive":true,"wgTranslatePageTranslation":"source","wgULSPosition":"personal","wgULSisCompactLinksEnabled":true,"wgVector2022LanguageInHeader":false,"wgULSisLanguageSelectorEmpty":false};RLSTATE={"site.styles":"ready","user.styles":"ready","user":"ready","user.options":"loading","ext.translate.tag.languages":"ready","ext.cite.styles":"ready","ext.pygments":"ready","skins.vector.styles.legacy":"ready","jquery.tablesorter.styles":"ready","ext.translate.edit.documentation.styles":"ready","ext.translate":"ready","codex-search-styles":"ready","ext.uls.pt":"ready"};RLPAGEMODULES=["ext.tabs",
"ext.cite.ux-enhancements","ext.pygments.view","site","mediawiki.page.ready","jquery.tablesorter","mediawiki.toc","skins.vector.legacy.js","ext.languageSelector","ext.translate.pagetranslation.uls","ext.uls.compactlinks","ext.uls.geoclient","ext.uls.interface","ext.moderation.notify","ext.moderation.notify.desktop"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.impl(function(){return["user.options@12s5i",function($,jQuery,require,module){mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
}];});});</script>
<link rel="stylesheet" href="/mediawiki/load.php?lang=en&amp;modules=codex-search-styles%7Cext.cite.styles%7Cext.pygments%2Ctranslate%7Cext.translate.edit.documentation.styles%7Cext.translate.tag.languages%7Cext.uls.pt%7Cjquery.tablesorter.styles%7Cskins.vector.styles.legacy&amp;only=styles&amp;skin=vector">
<script async="" src="/mediawiki/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content="">
<link rel="stylesheet" href="/mediawiki/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector">
<meta name="generator" content="MediaWiki 1.43.0">
<meta name="robots" content="max-image-preview:standard">
<meta name="format-detection" content="telephone=no">
<meta name="viewport" content="width=1120">
<link rel="icon" href="/mediawiki/resources/assets/Alliance_favicon.png">
<link rel="search" type="application/opensearchdescription+xml" href="/mediawiki/rest.php/v1/search" title="Alliance Doc (en)">
<link rel="EditURI" type="application/rsd+xml" href="https://docs.alliancecan.ca/mediawiki/api.php?action=rsd">
<link rel="alternate" type="application/atom+xml" title="Alliance Doc Atom feed" href="/mediawiki/index.php?title=Special:RecentChanges&amp;feed=atom">
<style type="text/css" id="tabs-dynamic-styles">/*<![CDATA[*/
/* Dynamically generated tabs styles */
.tabs-input-1:checked ~ .tabs-container .tabs-content-1,
.tabs-input-2:checked ~ .tabs-container .tabs-content-2,
.tabs-input-3:checked ~ .tabs-container .tabs-content-3,
.tabs-input-4:checked ~ .tabs-container .tabs-content-4,
.tabs-input-5:checked ~ .tabs-container .tabs-content-5,
.tabs-input-0:checked ~ .tabs-container .tabs-content-1 {display:inline-block;}
.tabs-input-1:checked ~ .tabs-container .tabs-inline.tabs-content-1,
.tabs-input-2:checked ~ .tabs-container .tabs-inline.tabs-content-2,
.tabs-input-3:checked ~ .tabs-container .tabs-inline.tabs-content-3,
.tabs-input-4:checked ~ .tabs-container .tabs-inline.tabs-content-4,
.tabs-input-5:checked ~ .tabs-container .tabs-inline.tabs-content-5,
.tabs-input-0:checked ~ .tabs-container .tabs-inline.tabs-content-1 {display:inline;}
.tabs-input-1:checked ~ .tabs-container .tabs-block.tabs-content-1,
.tabs-input-2:checked ~ .tabs-container .tabs-block.tabs-content-2,
.tabs-input-3:checked ~ .tabs-container .tabs-block.tabs-content-3,
.tabs-input-4:checked ~ .tabs-container .tabs-block.tabs-content-4,
.tabs-input-5:checked ~ .tabs-container .tabs-block.tabs-content-5,
.tabs-input-0:checked ~ .tabs-container .tabs-block.tabs-content-1 {display:block;}
/* The same styles, but with .checked instead of :checked, for browsers that rely on the JavaScript fallback */
.tabs-input-1.checked ~ .tabs-container .tabs-content-1,
.tabs-input-2.checked ~ .tabs-container .tabs-content-2,
.tabs-input-3.checked ~ .tabs-container .tabs-content-3,
.tabs-input-4.checked ~ .tabs-container .tabs-content-4,
.tabs-input-5.checked ~ .tabs-container .tabs-content-5,
.tabs-input-0.checked ~ .tabs-container .tabs-content-1 {display:inline-block;}
.tabs-input-1.checked ~ .tabs-container .tabs-inline.tabs-content-1,
.tabs-input-2.checked ~ .tabs-container .tabs-inline.tabs-content-2,
.tabs-input-3.checked ~ .tabs-container .tabs-inline.tabs-content-3,
.tabs-input-4.checked ~ .tabs-container .tabs-inline.tabs-content-4,
.tabs-input-5.checked ~ .tabs-container .tabs-inline.tabs-content-5,
.tabs-input-0.checked ~ .tabs-container .tabs-inline.tabs-content-1 {display:inline;}
.tabs-input-1.checked ~ .tabs-container .tabs-block.tabs-content-1,
.tabs-input-2.checked ~ .tabs-container .tabs-block.tabs-content-2,
.tabs-input-3.checked ~ .tabs-container .tabs-block.tabs-content-3,
.tabs-input-4.checked ~ .tabs-container .tabs-block.tabs-content-4,
.tabs-input-5.checked ~ .tabs-container .tabs-block.tabs-content-5,
.tabs-input-0.checked ~ .tabs-container .tabs-block.tabs-content-1 {display:block;}
.tabs-dropdown .tabs-content,.tabs-dropdown .tabs-container,.tabs-dropdown li,.tabs-dropdown ul,.tabs-dropdown ol {background-color: white /* Malicious data in tabs-dropdown-bgcolor */}
/*]]>*/</style>
</head>
<body class="skin-vector-legacy mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-GROMACS rootpage-GROMACS skin-vector action-view"><div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice"></div>
	<div class="mw-indicators">
	<div id="mw-indicator-languageselector" class="mw-indicator"><span id="languageselector-box-1" class="languageselector " style=""><form name="languageselector-form-1" id="languageselector-form-1" method="get" action="/mediawiki/index.php" style="display:inline;"><input type="hidden" value="GROMACS" name="title"><select name="setlang" id="languageselector-select-1" style=""><option value="aae">ArbÃ«risht</option><option value="ab">Ğ°Ô¥ÑÑˆÓ™Ğ°</option><option value="abs">bahasa ambon</option><option value="ace">AcÃ¨h</option><option value="acf">KwÃ©yÃ²l Sent Lisi</option><option value="acm">Ø¹Ø±Ø§Ù‚ÙŠ</option><option value="ady">Ğ°Ğ´Ñ‹Ğ³Ğ°Ğ±Ğ·Ñ</option><option value="ady-cyrl">Ğ°Ğ´Ñ‹Ğ³Ğ°Ğ±Ğ·Ñ</option><option value="aeb">ØªÙˆÙ†Ø³ÙŠ / TÃ»nsÃ®</option><option value="aeb-arab">ØªÙˆÙ†Ø³ÙŠ</option><option value="aeb-latn">TÃ»nsÃ®</option><option value="af">Afrikaans</option><option value="aln">GegÃ«</option><option value="alt">Ğ°Ğ»Ñ‚Ğ°Ğ¹ Ñ‚Ğ¸Ğ»</option><option value="am">áŠ áˆ›áˆ­áŠ›</option><option value="ami">Pangcah</option><option value="an">aragonÃ©s</option><option value="ang">Ã†nglisc</option><option value="ann">Obolo</option><option value="anp">à¤…à¤‚à¤—à¤¿à¤•à¤¾</option><option value="apc">Ø´Ø§Ù…ÙŠ</option><option value="ar">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</option><option value="arc">ÜÜªÜ¡ÜÜ</option><option value="arn">mapudungun</option><option value="arq">Ø¬Ø§Ø²Ø§ÙŠØ±ÙŠØ©</option><option value="ary">Ø§Ù„Ø¯Ø§Ø±Ø¬Ø©</option><option value="arz">Ù…ØµØ±Ù‰</option><option value="as">à¦…à¦¸à¦®à§€à¦¯à¦¼à¦¾</option><option value="ase">American sign language</option><option value="ast">asturianu</option><option value="atj">Atikamekw</option><option value="av">Ğ°Ğ²Ğ°Ñ€</option><option value="avk">Kotava</option><option value="awa">à¤…à¤µà¤§à¥€</option><option value="ay">Aymar aru</option><option value="az">azÉ™rbaycanca</option><option value="azb">ØªÛ†Ø±Ú©Ø¬Ù‡</option><option value="ba">Ğ±Ğ°ÑˆÒ¡Ğ¾Ñ€Ñ‚ÑĞ°</option><option value="ban">Basa Bali</option><option value="ban-bali">á¬©á¬²á¬©á¬®á¬¶</option><option value="bar">Boarisch</option><option value="bbc">Batak Toba</option><option value="bbc-latn">Batak Toba</option><option value="bcc">Ø¬Ù‡Ù„Ø³Ø±ÛŒ Ø¨Ù„ÙˆÚ†ÛŒ</option><option value="bci">wawle</option><option value="bcl">Bikol Central</option><option value="bdr">Bajau Sama</option><option value="be">Ğ±ĞµĞ»Ğ°Ñ€ÑƒÑĞºĞ°Ñ</option><option value="be-tarask">Ğ±ĞµĞ»Ğ°Ñ€ÑƒÑĞºĞ°Ñ (Ñ‚Ğ°Ñ€Ğ°ÑˆĞºĞµĞ²Ñ–Ñ†Ğ°)</option><option value="bew">Betawi</option><option value="bg">Ğ±ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸</option><option value="bgc">à¤¹à¤°à¤¿à¤¯à¤¾à¤£à¤µà¥€</option><option value="bgn">Ø±ÙˆÚ† Ú©Ù¾ØªÛŒÙ† Ø¨Ù„ÙˆÚ†ÛŒ</option><option value="bh">à¤­à¥‹à¤œà¤ªà¥à¤°à¥€</option><option value="bho">à¤­à¥‹à¤œà¤ªà¥à¤°à¥€</option><option value="bi">Bislama</option><option value="bjn">Banjar</option><option value="blk">á€•á€¡á€­á€¯á€á€ºá‚á€˜á€¬á‚á€á€¬á‚</option><option value="bm">bamanankan</option><option value="bn">à¦¬à¦¾à¦‚à¦²à¦¾</option><option value="bo">à½–à½¼à½‘à¼‹à½¡à½²à½‚</option><option value="bpy">à¦¬à¦¿à¦·à§à¦£à§à¦ªà§à¦°à¦¿à¦¯à¦¼à¦¾ à¦®à¦£à¦¿à¦ªà§à¦°à§€</option><option value="bqi">Ø¨Ø®ØªÛŒØ§Ø±ÛŒ</option><option value="br">brezhoneg</option><option value="brh">BrÃ¡huÃ­</option><option value="bs">bosanski</option><option value="btm">Batak Mandailing</option><option value="bto">Iriga Bicolano</option><option value="bug">Basa Ugi</option><option value="bxr">Ğ±ÑƒÑ€ÑĞ°Ğ´</option><option value="ca">catalÃ </option><option value="cbk-zam">Chavacano de Zamboanga</option><option value="ccp">ğ‘„Œğ‘„‹ğ‘„´ğ‘„Ÿğ‘„³ğ‘„¦</option><option value="cdo">é–©æ±èª / MÃ¬ng-dÄ•Ì¤ng-ngá¹³Ì„</option><option value="ce">Ğ½Ğ¾Ñ…Ñ‡Ğ¸Ğ¹Ğ½</option><option value="ceb">Cebuano</option><option value="ch">Chamoru</option><option value="chn">chinuk wawa</option><option value="chr">á£á³á©</option><option value="chy">TsetsÃªhestÃ¢hese</option><option value="ckb">Ú©ÙˆØ±Ø¯ÛŒ</option><option value="co">corsu</option><option value="cps">CapiceÃ±o</option><option value="cpx">è†ä»™èª / PÃ³-sing-gá¹³Ì‚</option><option value="cpx-hans">è†ä»™è¯­ï¼ˆç®€ä½“ï¼‰</option><option value="cpx-hant">è†ä»™èªï¼ˆç¹é«”ï¼‰</option><option value="cr">NÄ“hiyawÄ“win / á“€á¦áƒá”­ááá£</option><option value="crh">qÄ±rÄ±mtatarca</option><option value="crh-cyrl">ĞºÑŠÑ‹Ñ€Ñ‹Ğ¼Ñ‚Ğ°Ñ‚Ğ°Ñ€Ğ´Ğ¶Ğ° (ĞšĞ¸Ñ€Ğ¸Ğ»Ğ»)</option><option value="crh-latn">qÄ±rÄ±mtatarca (Latin)</option><option value="crh-ro">tatarÅŸa</option><option value="cs">ÄeÅ¡tina</option><option value="csb">kaszÃ«bsczi</option><option value="cu">ÑĞ»Ğ¾Ğ²Ñ£Ğ½ÑŒÑĞºÑŠ / â°”â°â°‘â°‚â°¡â°â° â°”â°â°Ÿ</option><option value="cv">Ñ‡Ó‘Ğ²Ğ°ÑˆĞ»Ğ°</option><option value="cy">Cymraeg</option><option value="da">dansk</option><option value="dag">dagbanli</option><option value="de">Deutsch</option><option value="de-at">Ã–sterreichisches Deutsch</option><option value="de-ch">Schweizer Hochdeutsch</option><option value="de-formal">Deutsch (Sie-Form)</option><option value="dga">Dagaare</option><option value="din">ThuÉ”Å‹jÃ¤Å‹</option><option value="diq">Zazaki</option><option value="dsb">dolnoserbski</option><option value="dtp">Kadazandusun</option><option value="dty">à¤¡à¥‹à¤Ÿà¥‡à¤²à¥€</option><option value="dua">DuÃ¡lÃ¡</option><option value="dv">Ş‹Ş¨ŞˆŞ¬Ş€Ş¨Ş„Ş¦ŞŞ°</option><option value="dz">à½‡à½¼à½„à¼‹à½</option><option value="ee">eÊ‹egbe</option><option value="efi">Efá»‹k</option><option value="egl">emiliÃ n e rumagnÃ²l</option><option value="el">Î•Î»Î»Î·Î½Î¹ÎºÎ¬</option><option value="eml">emiliÃ n e rumagnÃ²l</option><option value="en" selected="">English</option><option value="en-ca">Canadian English</option><option value="en-gb">British English</option><option value="eo">Esperanto</option><option value="es">espaÃ±ol</option><option value="es-formal">espaÃ±ol (formal)</option><option value="et">eesti</option><option value="eu">euskara</option><option value="ext">estremeÃ±u</option><option value="fa">ÙØ§Ø±Ø³ÛŒ</option><option value="fat">mfantse</option><option value="ff">Fulfulde</option><option value="fi">suomi</option><option value="fit">meÃ¤nkieli</option><option value="fj">Na Vosa Vakaviti</option><option value="fo">fÃ¸royskt</option><option value="fon">fÉ”Ì€ngbÃ¨</option><option value="fr">franÃ§ais</option><option value="frc">franÃ§ais cadien</option><option value="frp">arpetan</option><option value="frr">Nordfriisk</option><option value="fur">furlan</option><option value="fy">Frysk</option><option value="ga">Gaeilge</option><option value="gaa">Ga</option><option value="gag">Gagauz</option><option value="gan">è´›èª</option><option value="gan-hans">èµ£è¯­ï¼ˆç®€ä½“ï¼‰</option><option value="gan-hant">è´›èªï¼ˆç¹é«”ï¼‰</option><option value="gcf">krÃ©yÃ²l Gwadloup</option><option value="gcr">kriyÃ²l gwiyannen</option><option value="gd">GÃ idhlig</option><option value="gl">galego</option><option value="gld">Ğ½Ğ°Ì„Ğ½Ğ¸</option><option value="glk">Ú¯ÛŒÙ„Ú©ÛŒ</option><option value="gn">AvaÃ±e'áº½</option><option value="gom">à¤—à¥‹à¤‚à¤¯à¤šà¥€ à¤•à¥‹à¤‚à¤•à¤£à¥€ / GÃµychi Konknni</option><option value="gom-deva">à¤—à¥‹à¤‚à¤¯à¤šà¥€ à¤•à¥‹à¤‚à¤•à¤£à¥€</option><option value="gom-latn">GÃµychi Konknni</option><option value="gor">Bahasa Hulontalo</option><option value="got">ğŒ²ğŒ¿ğ„ğŒ¹ğƒğŒº</option><option value="gpe">Ghanaian Pidgin</option><option value="grc">á¼ˆÏÏ‡Î±Î¯Î± á¼‘Î»Î»Î·Î½Î¹Îºá½´</option><option value="gsw">Alemannisch</option><option value="gu">àª—à«àªœàª°àª¾àª¤à«€</option><option value="guc">wayuunaiki</option><option value="gur">farefare</option><option value="guw">gungbe</option><option value="gv">Gaelg</option><option value="ha">Hausa</option><option value="hak">å®¢å®¶èª / Hak-kÃ¢-ngÃ®</option><option value="haw">HawaiÊ»i</option><option value="he">×¢×‘×¨×™×ª</option><option value="hi">à¤¹à¤¿à¤¨à¥à¤¦à¥€</option><option value="hif">Fiji Hindi</option><option value="hif-latn">Fiji Hindi</option><option value="hil">Ilonggo</option><option value="hno">ÛÙ†Ø¯Ú©Ùˆ</option><option value="hr">hrvatski</option><option value="hrx">Hunsrik</option><option value="hsb">hornjoserbsce</option><option value="hsn">æ¹˜èª</option><option value="ht">KreyÃ²l ayisyen</option><option value="hu">magyar</option><option value="hu-formal">magyar (formal)</option><option value="hy">Õ°Õ¡ÕµÕ¥Ö€Õ¥Õ¶</option><option value="hyw">Ô±Ö€Õ¥Ö‚Õ´Õ¿Õ¡Õ°Õ¡ÕµÕ¥Ö€Õ§Õ¶</option><option value="ia">interlingua</option><option value="iba">Jaku Iban</option><option value="ibb">ibibio</option><option value="id">Bahasa Indonesia</option><option value="ie">Interlingue</option><option value="ig">Igbo</option><option value="igl">Igala</option><option value="ii">ê†‡ê‰™</option><option value="ik">IÃ±upiatun</option><option value="ike-cans">áƒá“„á’ƒá‘á‘á‘¦</option><option value="ike-latn">inuktitut</option><option value="ilo">Ilokano</option><option value="inh">Ğ³Ó€Ğ°Ğ»Ğ³Ó€Ğ°Ğ¹</option><option value="io">Ido</option><option value="is">Ã­slenska</option><option value="isv-cyrl">Ğ¼ĞµĞ´Ğ¶ÑƒÑĞ»Ğ¾Ğ²Ñ˜Ğ°Ğ½ÑĞºÑ‹</option><option value="isv-latn">medÅ¾uslovjansky</option><option value="it">italiano</option><option value="iu">áƒá“„á’ƒá‘á‘á‘¦ / inuktitut</option><option value="ja">æ—¥æœ¬èª</option><option value="jam">Patois</option><option value="jbo">la .lojban.</option><option value="jut">jysk</option><option value="jv">Jawa</option><option value="ka">áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜</option><option value="kaa">Qaraqalpaqsha</option><option value="kab">Taqbaylit</option><option value="kai">Karai-karai</option><option value="kbd">Ğ°Ğ´Ñ‹Ğ³ÑĞ±Ğ·Ñ</option><option value="kbd-cyrl">Ğ°Ğ´Ñ‹Ğ³ÑĞ±Ğ·Ñ</option><option value="kbp">KabÉ©yÉ›</option><option value="kcg">Tyap</option><option value="kea">kabuverdianu</option><option value="kg">Kongo</option><option value="kge">Kumoring</option><option value="khw">Ú©Ú¾ÙˆØ§Ø±</option><option value="ki">GÄ©kÅ©yÅ©</option><option value="kiu">KÄ±rmancki</option><option value="kjh">Ñ…Ğ°ĞºĞ°Ñ</option><option value="kjp">á€–á á€¯á€¶á€œá€­á€€á€º</option><option value="kk">Ò›Ğ°Ğ·Ğ°Ò›ÑˆĞ°</option><option value="kk-arab">Ù‚Ø§Ø²Ø§Ù‚Ø´Ø§ (ØªÙ´ÙˆØªÛ•)</option><option value="kk-cn">Ù‚Ø§Ø²Ø§Ù‚Ø´Ø§ (Ø¬Û‡Ù†Ú¯Ùˆ)</option><option value="kk-cyrl">Ò›Ğ°Ğ·Ğ°Ò›ÑˆĞ° (ĞºĞ¸Ñ€Ğ¸Ğ»)</option><option value="kk-kz">Ò›Ğ°Ğ·Ğ°Ò›ÑˆĞ° (ÒšĞ°Ğ·Ğ°Ò›ÑÑ‚Ğ°Ğ½)</option><option value="kk-latn">qazaqÅŸa (latÄ±n)</option><option value="kk-tr">qazaqÅŸa (TÃ¼rkÃ¯ya)</option><option value="kl">kalaallisut</option><option value="km">á—á¶áŸá¶ááŸ’á˜áŸ‚áš</option><option value="kn">à²•à²¨à³à²¨à²¡</option><option value="knc">Yerwa Kanuri</option><option value="ko">í•œêµ­ì–´</option><option value="ko-kp">ì¡°ì„ ë§</option><option value="koi">Ğ¿ĞµÑ€ĞµĞ¼ ĞºĞ¾Ğ¼Ğ¸</option><option value="kr">kanuri</option><option value="krc">ĞºÑŠĞ°Ñ€Ğ°Ñ‡Ğ°Ğ¹-Ğ¼Ğ°Ğ»ĞºÑŠĞ°Ñ€</option><option value="kri">Krio</option><option value="krj">Kinaray-a</option><option value="krl">karjal</option><option value="ks">à¤•à¥‰à¤¶à¥à¤° / Ú©Ù²Ø´ÙØ±</option><option value="ks-arab">Ú©Ù²Ø´ÙØ±</option><option value="ks-deva">à¤•à¥‰à¤¶à¥à¤°</option><option value="ksh">Ripoarisch</option><option value="ksw">á€…á€¾á€®á¤</option><option value="ku">kurdÃ®</option><option value="ku-arab">Ú©ÙˆØ±Ø¯ÛŒ (Ø¹Û•Ø±Û•Ø¨ÛŒ)</option><option value="ku-latn">kurdÃ® (latÃ®nÃ®)</option><option value="kum">ĞºÑŠÑƒĞ¼ÑƒĞºÑŠ</option><option value="kus">KÊ‹saal</option><option value="kv">ĞºĞ¾Ğ¼Ğ¸</option><option value="kw">kernowek</option><option value="ky">ĞºÑ‹Ñ€Ğ³Ñ‹Ğ·Ñ‡Ğ°</option><option value="la">Latina</option><option value="lad">Ladino</option><option value="lb">LÃ«tzebuergesch</option><option value="lbe">Ğ»Ğ°ĞºĞºÑƒ</option><option value="lez">Ğ»ĞµĞ·Ğ³Ğ¸</option><option value="lfn">Lingua Franca Nova</option><option value="lg">Luganda</option><option value="li">Limburgs</option><option value="lij">Ligure</option><option value="liv">LÄ«vÃµ kÄ“Ä¼</option><option value="lki">Ù„Û•Ú©ÛŒ</option><option value="lld">Ladin</option><option value="lmo">lombard</option><option value="ln">lingÃ¡la</option><option value="lo">àº¥àº²àº§</option><option value="loz">Silozi</option><option value="lrc">Ù„ÛŠØ±ÛŒ Ø´ÙˆÙ…Ø§Ù„ÛŒ</option><option value="lt">lietuviÅ³</option><option value="ltg">latgaÄ¼u</option><option value="lua">ciluba</option><option value="lus">Mizo Å£awng</option><option value="luz">Ù„Ø¦Ø±ÛŒ Ø¯ÙˆÙ™Ù…ÛŒÙ†ÛŒ</option><option value="lv">latvieÅ¡u</option><option value="lzh">æ–‡è¨€</option><option value="lzz">Lazuri</option><option value="mad">MadhurÃ¢</option><option value="mag">à¤®à¤—à¤¹à¥€</option><option value="mai">à¤®à¥ˆà¤¥à¤¿à¤²à¥€</option><option value="map-bms">Basa Banyumasan</option><option value="mdf">Ğ¼Ğ¾ĞºÑˆĞµĞ½ÑŒ</option><option value="mg">Malagasy</option><option value="mhr">Ğ¾Ğ»Ñ‹Ğº Ğ¼Ğ°Ñ€Ğ¸Ğ¹</option><option value="mi">MÄori</option><option value="min">Minangkabau</option><option value="mk">Ğ¼Ğ°ĞºĞµĞ´Ğ¾Ğ½ÑĞºĞ¸</option><option value="ml">à´®à´²à´¯à´¾à´³à´‚</option><option value="mn">Ğ¼Ğ¾Ğ½Ğ³Ğ¾Ğ»</option><option value="mnc">manju gisun</option><option value="mnc-latn">manju gisun</option><option value="mnc-mong">á ®á  á ¨á µá¡  á¡¤á¡³á °á¡ á ¨</option><option value="mni">ê¯ƒê¯¤ê¯‡ê¯© ê¯‚ê¯£ê¯Ÿ</option><option value="mnw">á€˜á€¬á€á€¬á€™á€”á€º</option><option value="mo">Ğ¼Ğ¾Ğ»Ğ´Ğ¾Ğ²ĞµĞ½ÑÑĞºÑ</option><option value="mos">moore</option><option value="mr">à¤®à¤°à¤¾à¤ à¥€</option><option value="mrh">Mara</option><option value="mrj">ĞºÑ‹Ñ€Ñ‹Ğº Ğ¼Ğ°Ñ€Ñ‹</option><option value="ms">Bahasa Melayu</option><option value="ms-arab">Ø¨Ù‡Ø§Ø³ Ù…Ù„Ø§ÙŠÙˆ</option><option value="mt">Malti</option><option value="mui">Baso Palembang</option><option value="mwl">MirandÃ©s</option><option value="my">á€™á€¼á€”á€ºá€™á€¬á€˜á€¬á€á€¬</option><option value="myv">ÑÑ€Ğ·ÑĞ½ÑŒ</option><option value="mzn">Ù…Ø§Ø²ÙØ±ÙˆÙ†ÛŒ</option><option value="na">Dorerin Naoero</option><option value="nah">NÄhuatl</option><option value="nan">é–©å—èª / BÃ¢n-lÃ¢m-gÃº</option><option value="nan-hant">é–©å—èªï¼ˆå‚³çµ±æ¼¢å­—ï¼‰</option><option value="nan-latn-pehoeji">BÃ¢n-lÃ¢m-gÃº (PeÌh-Åe-jÄ«)</option><option value="nan-latn-tailo">BÃ¢n-lÃ¢m-gÃº (TÃ¢i-lÃ´)</option><option value="nap">Napulitano</option><option value="nb">norsk bokmÃ¥l</option><option value="nds">PlattdÃ¼Ã¼tsch</option><option value="nds-nl">Nedersaksies</option><option value="ne">à¤¨à¥‡à¤ªà¤¾à¤²à¥€</option><option value="new">à¤¨à¥‡à¤ªà¤¾à¤² à¤­à¤¾à¤·à¤¾</option><option value="nia">Li Niha</option><option value="nit">à°•à±Šà°²à°¾à°®à°¿</option><option value="niu">NiuÄ“</option><option value="nl">Nederlands</option><option value="nl-informal">Nederlands (informeel)</option><option value="nmz">nawdm</option><option value="nn">norsk nynorsk</option><option value="nod">á¨£á©¤á©´á¨¾á©®á©¬á©¥á¨¦</option><option value="nog">Ğ½Ğ¾Ğ³Ğ°Ğ¹ÑˆĞ°</option><option value="nov">Novial</option><option value="nqo">ß’ßß</option><option value="nr">isiNdebele seSewula</option><option value="nrm">Nouormand</option><option value="nso">Sesotho sa Leboa</option><option value="nup">Nupe</option><option value="nv">DinÃ© bizaad</option><option value="ny">Chi-Chewa</option><option value="nyn">runyankore</option><option value="nyo">Orunyoro</option><option value="nys">Nyunga</option><option value="oc">occitan</option><option value="ojb">Ojibwemowin</option><option value="olo">livvinkarjala</option><option value="om">Oromoo</option><option value="or">à¬“à¬¡à¬¼à¬¿à¬†</option><option value="os">Ğ¸Ñ€Ğ¾Ğ½</option><option value="pa">à¨ªà©°à¨œà¨¾à¨¬à©€</option><option value="pag">Pangasinan</option><option value="pam">Kapampangan</option><option value="pap">Papiamentu</option><option value="pcd">Picard</option><option value="pcm">NaijÃ¡</option><option value="pdc">Deitsch</option><option value="pdt">Plautdietsch</option><option value="pfl">PÃ¤lzisch</option><option value="pi">à¤ªà¤¾à¤²à¤¿</option><option value="pih">Norfuk / Pitkern</option><option value="pl">polski</option><option value="pms">PiemontÃ¨is</option><option value="pnb">Ù¾Ù†Ø¬Ø§Ø¨ÛŒ</option><option value="pnt">Î Î¿Î½Ï„Î¹Î±ÎºÎ¬</option><option value="prg">prÅ«siskan</option><option value="ps">Ù¾ÚšØªÙˆ</option><option value="pt">portuguÃªs</option><option value="pt-br">portuguÃªs do Brasil</option><option value="pwn">pinayuanan</option><option value="qqq">Message documentation</option><option value="qu">Runa Simi</option><option value="qug">Runa shimi</option><option value="rgn">RumagnÃ´l</option><option value="rif">Tarifit</option><option value="rki">á€›á€á€­á€¯á€„á€º</option><option value="rm">rumantsch</option><option value="rmc">romaÅˆi Ähib</option><option value="rmy">romani Ähib</option><option value="rn">ikirundi</option><option value="ro">romÃ¢nÄƒ</option><option value="roa-tara">tarandÃ­ne</option><option value="rsk">Ñ€ÑƒÑĞºĞ¸</option><option value="ru">Ñ€ÑƒÑÑĞºĞ¸Ğ¹</option><option value="rue">Ñ€ÑƒÑĞ¸Ğ½ÑŒÑĞºÑ‹Ğ¹</option><option value="rup">armÃ£neashti</option><option value="ruq">VlÄƒheÅŸte</option><option value="ruq-cyrl">Ğ’Ğ»Ğ°Ñ…ĞµÑÑ‚Ğµ</option><option value="ruq-latn">VlÄƒheÅŸte</option><option value="rut">Ğ¼Ñ‹Ñ…Ğ°Ó€Ğ±Ğ¸ÑˆĞ´Ñ‹</option><option value="rw">Ikinyarwanda</option><option value="ryu">ã†ã¡ãªãƒ¼ãã¡</option><option value="sa">à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤®à¥</option><option value="sah">ÑĞ°Ñ…Ğ° Ñ‚Ñ‹Ğ»Ğ°</option><option value="sat">á±¥á±Ÿá±±á±›á±Ÿá±²á±¤</option><option value="sc">sardu</option><option value="scn">sicilianu</option><option value="sco">Scots</option><option value="sd">Ø³Ù†ÚŒÙŠ</option><option value="sdc">Sassaresu</option><option value="sdh">Ú©ÙˆØ±Ø¯ÛŒ Ø®ÙˆØ§Ø±Ú¯</option><option value="se">davvisÃ¡megiella</option><option value="se-fi">davvisÃ¡megiella (Suoma bealde)</option><option value="se-no">davvisÃ¡megiella (Norgga bealde)</option><option value="se-se">davvisÃ¡megiella (RuoÅ§a bealde)</option><option value="sei">Cmique Itom</option><option value="ses">Koyraboro Senni</option><option value="sg">SÃ¤ngÃ¶</option><option value="sgs">Å¾emaitÄ—Å¡ka</option><option value="sh">srpskohrvatski / ÑÑ€Ğ¿ÑĞºĞ¾Ñ…Ñ€Ğ²Ğ°Ñ‚ÑĞºĞ¸</option><option value="sh-cyrl">ÑÑ€Ğ¿ÑĞºĞ¾Ñ…Ñ€Ğ²Ğ°Ñ‚ÑĞºĞ¸ (Ñ›Ğ¸Ñ€Ğ¸Ğ»Ğ¸Ñ†Ğ°)</option><option value="sh-latn">srpskohrvatski (latinica)</option><option value="shi">Taclá¸¥it</option><option value="shn">á½á‚ƒá‚‡á€á‚ƒá‚‡á€á‚†á€¸ </option><option value="shy">tacawit</option><option value="shy-latn">tacawit</option><option value="si">à·ƒà·’à¶‚à·„à¶½</option><option value="sjd">ĞºÓ£Ğ»Ğ»Ñ‚ ÑĞ°Ì„Ğ¼ÑŒ ĞºÓ£Ğ»Ğ»</option><option value="sje">bidumsÃ¡megiella</option><option value="sk">slovenÄina</option><option value="skr">Ø³Ø±Ø§Ø¦ÛŒÚ©ÛŒ</option><option value="skr-arab">Ø³Ø±Ø§Ø¦ÛŒÚ©ÛŒ</option><option value="sl">slovenÅ¡Äina</option><option value="sli">SchlÃ¤sch</option><option value="sm">Gagana Samoa</option><option value="sma">Ã¥arjelsaemien</option><option value="smn">anarÃ¢Å¡kielÃ¢</option><option value="sms">nuÃµrttsÃ¤Ã¤Ê¹mÇ©iÃµll</option><option value="sn">chiShona</option><option value="so">Soomaaliga</option><option value="sq">shqip</option><option value="sr">ÑÑ€Ğ¿ÑĞºĞ¸ / srpski</option><option value="sr-ec">ÑÑ€Ğ¿ÑĞºĞ¸ (Ñ›Ğ¸Ñ€Ğ¸Ğ»Ğ¸Ñ†Ğ°)</option><option value="sr-el">srpski (latinica)</option><option value="srn">Sranantongo</option><option value="sro">sardu campidanesu</option><option value="ss">SiSwati</option><option value="st">Sesotho</option><option value="stq">Seeltersk</option><option value="sty">ÑĞµĞ±ĞµÑ€Ñ‚Ğ°Ñ‚Ğ°Ñ€</option><option value="su">Sunda</option><option value="sv">svenska</option><option value="sw">Kiswahili</option><option value="syl">ê ê ¤ê Ÿê ê ¤</option><option value="szl">Å›lÅ¯nski</option><option value="szy">Sakizaya</option><option value="ta">à®¤à®®à®¿à®´à¯</option><option value="tay">Tayal</option><option value="tcy">à²¤à³à²³à³</option><option value="tdd">á¥–á¥­á¥° á¥–á¥¬á¥² á¥‘á¥¨á¥’á¥°</option><option value="te">à°¤à±†à°²à±à°—à±</option><option value="tet">tetun</option><option value="tg">Ñ‚Ğ¾Ò·Ğ¸ĞºÓ£</option><option value="tg-cyrl">Ñ‚Ğ¾Ò·Ğ¸ĞºÓ£</option><option value="tg-latn">tojikÄ«</option><option value="th">à¹„à¸—à¸¢</option><option value="ti">á‰µáŒáˆ­áŠ›</option><option value="tig">á‰µáŒáˆ¬</option><option value="tk">TÃ¼rkmenÃ§e</option><option value="tl">Tagalog</option><option value="tly">tolÄ±ÅŸi</option><option value="tn">Setswana</option><option value="to">lea faka-Tonga</option><option value="tok">toki pona</option><option value="tpi">Tok Pisin</option><option value="tr">TÃ¼rkÃ§e</option><option value="tru">á¹ªuroyo</option><option value="trv">Seediq</option><option value="ts">Xitsonga</option><option value="tt">Ñ‚Ğ°Ñ‚Ğ°Ñ€Ñ‡Ğ° / tatarÃ§a</option><option value="tt-cyrl">Ñ‚Ğ°Ñ‚Ğ°Ñ€Ñ‡Ğ°</option><option value="tt-latn">tatarÃ§a</option><option value="ttj">Orutooro</option><option value="tum">chiTumbuka</option><option value="tw">Twi</option><option value="ty">reo tahiti</option><option value="tyv">Ñ‚Ñ‹Ğ²Ğ° Ğ´Ñ‹Ğ»</option><option value="tzm">âµœâ´°âµâ´°âµ£âµ‰âµ–âµœ</option><option value="udm">ÑƒĞ´Ğ¼ÑƒÑ€Ñ‚</option><option value="ug">Ø¦Û‡ÙŠØºÛ‡Ø±Ú†Û• / Uyghurche</option><option value="ug-arab">Ø¦Û‡ÙŠØºÛ‡Ø±Ú†Û•</option><option value="ug-latn">Uyghurche</option><option value="uk">ÑƒĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°</option><option value="ur">Ø§Ø±Ø¯Ùˆ</option><option value="uz">oÊ»zbekcha / ÑĞ·Ğ±ĞµĞºÑ‡Ğ°</option><option value="ve">Tshivenda</option><option value="vec">vÃ¨neto</option><option value="vep">vepsÃ¤n kelâ€™</option><option value="vi">Tiáº¿ng Viá»‡t</option><option value="vls">West-Vlams</option><option value="vmf">MainfrÃ¤nkisch</option><option value="vmw">emakhuwa</option><option value="vo">VolapÃ¼k</option><option value="vot">VaÄÄa</option><option value="vro">vÃµro</option><option value="wa">walon</option><option value="wal">wolaytta</option><option value="war">Winaray</option><option value="wls">FakaÊ»uvea</option><option value="wo">Wolof</option><option value="wuu">å´è¯­</option><option value="wuu-hans">å´è¯­ï¼ˆç®€ä½“ï¼‰</option><option value="wuu-hant">å³èªï¼ˆæ­£é«”ï¼‰</option><option value="xal">Ñ…Ğ°Ğ»ÑŒĞ¼Ğ³</option><option value="xh">isiXhosa</option><option value="xmf">áƒ›áƒáƒ áƒ’áƒáƒšáƒ£áƒ áƒ˜</option><option value="xsy">saisiyat</option><option value="yi">×™×™Ö´×“×™×©</option><option value="yo">YorÃ¹bÃ¡</option><option value="yrl">Nháº½áº½gatÃº</option><option value="yue">ç²µèª</option><option value="yue-hans">ç²µè¯­ï¼ˆç®€ä½“ï¼‰</option><option value="yue-hant">ç²µèªï¼ˆç¹é«”ï¼‰</option><option value="za">Vahcuengh</option><option value="zea">ZeÃªuws</option><option value="zgh">âµœâ´°âµâ´°âµ£âµ‰âµ–âµœ âµœâ´°âµâ´°âµ¡â´°âµ¢âµœ</option><option value="zh">ä¸­æ–‡</option><option value="zh-cn">ä¸­æ–‡ï¼ˆä¸­å›½å¤§é™†ï¼‰</option><option value="zh-hans">ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰</option><option value="zh-hant">ä¸­æ–‡ï¼ˆç¹é«”ï¼‰</option><option value="zh-hk">ä¸­æ–‡ï¼ˆé¦™æ¸¯ï¼‰</option><option value="zh-mo">ä¸­æ–‡ï¼ˆæ¾³é–€ï¼‰</option><option value="zh-my">ä¸­æ–‡ï¼ˆé©¬æ¥è¥¿äºšï¼‰</option><option value="zh-sg">ä¸­æ–‡ï¼ˆæ–°åŠ å¡ï¼‰</option><option value="zh-tw">ä¸­æ–‡ï¼ˆè‡ºç£ï¼‰</option><option value="zu">isiZulu</option></select><input id="languageselector-commit-1" style="" type="submit" value="set"></form></span></div>
	</div>
	<h1 id="firstHeading" class="firstHeading mw-first-heading"><span class="mw-page-title-main">GROMACS</span></h1>
	<div id="bodyContent" class="vector-body">
		<div id="siteSub" class="noprint">From Alliance Doc</div>
		<div id="contentSub"><div id="mw-content-subtitle"></div></div>
		<div id="contentSub2"></div>
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#searchInput">Jump to search</a>
		<div id="mw-content-text" class="mw-body-content"><div class="mw-content-ltr mw-parser-output" lang="en" dir="ltr"><div class="mw-pt-languages noprint navigation-not-searchable" lang="en" dir="ltr"><div class="mw-pt-languages-label">Other languages:</div><ul class="mw-pt-languages-list"><li><span class="mw-pt-languages-ui mw-pt-languages-selected mw-pt-progress mw-pt-progress--complete" lang="en" dir="ltr">English</span></li>
<li><a href="/wiki/GROMACS/fr" class="mw-pt-progress mw-pt-progress--complete" title="GROMACS (100% translated)" lang="fr" dir="ltr">franÃ§ais</a></li></ul></div>
<div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#General"><span class="tocnumber">1</span> <span class="toctext">General</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#Strengths"><span class="tocnumber">1.1</span> <span class="toctext">Strengths</span></a></li>
<li class="toclevel-2 tocsection-3"><a href="#Weak_points"><span class="tocnumber">1.2</span> <span class="toctext">Weak points</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#GPU_support"><span class="tocnumber">1.3</span> <span class="toctext">GPU support</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-5"><a href="#Quickstart_guide"><span class="tocnumber">2</span> <span class="toctext">Quickstart guide</span></a>
<ul>
<li class="toclevel-2 tocsection-6"><a href="#Environment_modules"><span class="tocnumber">2.1</span> <span class="toctext">Environment modules</span></a></li>
<li class="toclevel-2 tocsection-7"><a href="#Suffixes"><span class="tocnumber">2.2</span> <span class="toctext">Suffixes</span></a>
<ul>
<li class="toclevel-3 tocsection-8"><a href="#GROMACS_5.x,_2016.x_and_newer"><span class="tocnumber">2.2.1</span> <span class="toctext">GROMACS 5.x, 2016.x and newer</span></a></li>
<li class="toclevel-3 tocsection-9"><a href="#GROMACS_4.6.7"><span class="tocnumber">2.2.2</span> <span class="toctext">GROMACS 4.6.7</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-10"><a href="#Submission_scripts"><span class="tocnumber">2.3</span> <span class="toctext">Submission scripts</span></a>
<ul>
<li class="toclevel-3 tocsection-11"><a href="#Serial_jobs"><span class="tocnumber">2.3.1</span> <span class="toctext">Serial jobs</span></a></li>
<li class="toclevel-3 tocsection-12"><a href="#Whole_nodes"><span class="tocnumber">2.3.2</span> <span class="toctext">Whole nodes</span></a></li>
<li class="toclevel-3 tocsection-13"><a href="#GPU_job"><span class="tocnumber">2.3.3</span> <span class="toctext">GPU job</span></a>
<ul>
<li class="toclevel-4 tocsection-14"><a href="#Notes_on_running_GROMACS_on_GPUs"><span class="tocnumber">2.3.3.1</span> <span class="toctext">Notes on running GROMACS on GPUs</span></a></li>
<li class="toclevel-4 tocsection-15"><a href="#Running_multiple_simulations_on_a_GPU"><span class="tocnumber">2.3.3.2</span> <span class="toctext">Running multiple simulations on a GPU</span></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-16"><a href="#Usage"><span class="tocnumber">3</span> <span class="toctext">Usage</span></a>
<ul>
<li class="toclevel-2 tocsection-17"><a href="#System_preparation"><span class="tocnumber">3.1</span> <span class="toctext">System preparation</span></a></li>
<li class="toclevel-2 tocsection-18"><a href="#Running_simulations"><span class="tocnumber">3.2</span> <span class="toctext">Running simulations</span></a>
<ul>
<li class="toclevel-3 tocsection-19"><a href="#Restarting_simulations"><span class="tocnumber">3.2.1</span> <span class="toctext">Restarting simulations</span></a></li>
<li class="toclevel-3 tocsection-20"><a href="#Checkpointing_simulations"><span class="tocnumber">3.2.2</span> <span class="toctext">Checkpointing simulations</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-21"><a href="#Performance_and_benchmarking"><span class="tocnumber">4</span> <span class="toctext">Performance and benchmarking</span></a>
<ul>
<li class="toclevel-2 tocsection-22"><a href="#MPI_processes_/_Slurm_tasks_/_Domain_decomposition"><span class="tocnumber">4.1</span> <span class="toctext">MPI processes / Slurm tasks / Domain decomposition</span></a>
<ul>
<li class="toclevel-3 tocsection-23"><a href="#Long-range_interactions_with_PME"><span class="tocnumber">4.1.1</span> <span class="toctext">Long-range interactions with PME</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-24"><a href="#OpenMP_threads_/_CPUs-per-task"><span class="tocnumber">4.2</span> <span class="toctext">OpenMP threads / CPUs-per-task</span></a></li>
<li class="toclevel-2 tocsection-25"><a href="#CPU_architecture"><span class="tocnumber">4.3</span> <span class="toctext">CPU architecture</span></a></li>
<li class="toclevel-2 tocsection-26"><a href="#GPUs"><span class="tocnumber">4.4</span> <span class="toctext">GPUs</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-27"><a href="#Analyzing_results"><span class="tocnumber">5</span> <span class="toctext">Analyzing results</span></a>
<ul>
<li class="toclevel-2 tocsection-28"><a href="#GROMACS_tools"><span class="tocnumber">5.1</span> <span class="toctext">GROMACS tools</span></a></li>
<li class="toclevel-2 tocsection-29"><a href="#VMD"><span class="tocnumber">5.2</span> <span class="toctext">VMD</span></a></li>
<li class="toclevel-2 tocsection-30"><a href="#Using_Python"><span class="tocnumber">5.3</span> <span class="toctext">Using Python</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-31"><a href="#Related_modules"><span class="tocnumber">6</span> <span class="toctext">Related modules</span></a>
<ul>
<li class="toclevel-2 tocsection-32"><a href="#GROMACS-Plumed"><span class="tocnumber">6.1</span> <span class="toctext">GROMACS-Plumed</span></a></li>
<li class="toclevel-2 tocsection-33"><a href="#GROMACS-Colvars"><span class="tocnumber">6.2</span> <span class="toctext">GROMACS-Colvars</span></a></li>
<li class="toclevel-2 tocsection-34"><a href="#GROMACS-CP2K"><span class="tocnumber">6.3</span> <span class="toctext">GROMACS-CP2K</span></a></li>
<li class="toclevel-2 tocsection-35"><a href="#GROMACS-LS"><span class="tocnumber">6.4</span> <span class="toctext">GROMACS-LS</span></a></li>
<li class="toclevel-2 tocsection-36"><a href="#GROMACS-RAMD"><span class="tocnumber">6.5</span> <span class="toctext">GROMACS-RAMD</span></a></li>
<li class="toclevel-2 tocsection-37"><a href="#GROMACS-SWAXS"><span class="tocnumber">6.6</span> <span class="toctext">GROMACS-SWAXS</span></a></li>
<li class="toclevel-2 tocsection-38"><a href="#G_MMPBSA"><span class="tocnumber">6.7</span> <span class="toctext">G_MMPBSA</span></a></li>
<li class="toclevel-2 tocsection-39"><a href="#gmx_MMPBSA"><span class="tocnumber">6.8</span> <span class="toctext">gmx_MMPBSA</span></a>
<ul>
<li class="toclevel-3 tocsection-40"><a href="#Submission_scripts_2"><span class="tocnumber">6.8.1</span> <span class="toctext">Submission scripts</span></a></li>
<li class="toclevel-3 tocsection-41"><a href="#Installing_gmx_MMPBSA_into_a_virtualenv"><span class="tocnumber">6.8.2</span> <span class="toctext">Installing gmx_MMPBSA into a virtualenv</span></a>
<ul>
<li class="toclevel-4 tocsection-42"><a href="#Installing_for_gromacs/2024_(StdEnv/2023)"><span class="tocnumber">6.8.2.1</span> <span class="toctext">Installing for gromacs/2024 (StdEnv/2023)</span></a></li>
<li class="toclevel-4 tocsection-43"><a href="#Installing_for_gromacs/2021_(StdEnv/2020)"><span class="tocnumber">6.8.2.2</span> <span class="toctext">Installing for gromacs/2021 (StdEnv/2020)</span></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-44"><a href="#Links"><span class="tocnumber">7</span> <span class="toctext">Links</span></a></li>
<li class="toclevel-1 tocsection-45"><a href="#References"><span class="tocnumber">8</span> <span class="toctext">References</span></a></li>
</ul>
</div>

<h1><span class="mw-headline" id="General">General</span></h1>
<p><a rel="nofollow" class="external text" href="http://www.gromacs.org/">GROMACS</a> is a versatile package to perform molecular dynamics for systems with hundreds to millions of particles.
It is primarily designed for biochemical molecules like proteins, lipids and nucleic acids that have a lot of complicated bonded interactions, 
but since GROMACS is extremely fast at calculating the nonbonded interactions 
(that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers.
</p>
<h2><span class="mw-headline" id="Strengths">Strengths</span></h2>
<ul><li>GROMACS provides extremely high performance compared to all other programs.</li>
<li>Since GROMACS 4.6, we have excellent CUDA-based GPU acceleration on GPUs that have Nvidia compute capability &gt;= 2.0 (e.g. Fermi or later).</li>
<li>GROMACS comes with a large selection of flexible tools for trajectory analysis.</li>
<li>GROMACS can be run in parallel, using either the standard MPI communication protocol, or via our own "Thread MPI" library for single-node workstations.</li>
<li>GROMACS is free software, available under the GNU Lesser General Public License (LGPL), version 2.1.</li></ul>
<h2><span class="mw-headline" id="Weak_points">Weak points</span></h2>
<ul><li>To get very high simulation speed, GROMACS does not do much additional analysis and / or data collection on the fly. It may be a challenge to obtain somewhat non-standard information about the simulated system from a GROMACS simulation.</li></ul>
<ul><li>Different versions may have significant differences in simulation methods and default parameters. Reproducing results of older versions with a newer version may not be straightforward.</li></ul>
<ul><li>Additional tools and utilities that come with GROMACS are not always of the highest quality, may contain bugs and may implement poorly documented methods. Reconfirming the results of such tools with independent methods is always a good idea.</li></ul>
<h2><span class="mw-headline" id="GPU_support">GPU support</span></h2>
<p>The top part of any log file will describe the configuration, 
and in particular whether your version has GPU support compiled in. 
GROMACS will automatically use any GPUs it finds. 
</p><p>GROMACS uses both CPUs and GPUs; it relies on a reasonable balance between CPU and GPU performance.
</p><p>The new neighbour structure required the introduction of a new variable called "cutoff-scheme" in the mdp file.
The behaviour of older GROMACS versions (before 4.6) corresponds to <code>cutoff-scheme = group</code>, while in order to use
GPU acceleration you must change it to <code>cutoff-scheme = verlet</code>, which has become the new default in version 5.0.
</p>
<h1><span class="mw-headline" id="Quickstart_guide">Quickstart guide</span></h1>
<p>This section summarizes configuration details.
</p>
<h2><span class="mw-headline" id="Environment_modules">Environment modules</span></h2>
<p>The following versions have been installed:
</p>
<form id="tabs-inputform" class="tabs tabs-inputform" action="#"></form><div class="tabs tabs-tabbox"><input type="radio" form="tabs-inputform" id="tabs-input-1-0" name="tabs-1" class="tabs-input tabs-input-0" checked="" /><input type="radio" form="tabs-inputform" id="tabs-input-1-1" name="tabs-1" class="tabs-input tabs-input-1" /><label class="tabs-label" for="tabs-input-1-1" data-tabpos="1">StdEnv/2023</label><wbr /><input type="radio" form="tabs-inputform" id="tabs-input-1-2" name="tabs-1" class="tabs-input tabs-input-2" /><label class="tabs-label" for="tabs-input-1-2" data-tabpos="2">StdEnv/2020</label><wbr /><input type="radio" form="tabs-inputform" id="tabs-input-1-3" name="tabs-1" class="tabs-input tabs-input-3" /><label class="tabs-label" for="tabs-input-1-3" data-tabpos="3">StdEnv/2018.3</label><wbr /><input type="radio" form="tabs-inputform" id="tabs-input-1-4" name="tabs-1" class="tabs-input tabs-input-4" /><label class="tabs-label" for="tabs-input-1-4" data-tabpos="4">StdEnv/2016.4</label><wbr /><div class="tabs-container" style="">
<div class="tabs-content tabs-content-1">
<table class="wikitable sortable">

<tbody><tr>
<th>GROMACS version</th>
<th>modules for running on CPUs</th>
<th>modules for running on GPUs (CUDA)</th>
<th>Notes
</th></tr>
<tr>
<td>gromacs/2024.4</td>
<td><code>StdEnv/2023  gcc/12.3  openmpi/4.1.5  gromacs/2024.4</code></td>
<td><code>StdEnv/2023 gcc/12.3  openmpi/4.1.5  cuda/12.2  gromacs/2024.4</code></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr>
<tr>
<td>gromacs/2024.1</td>
<td><code>StdEnv/2023  gcc/12.3  openmpi/4.1.5  gromacs/2024.1</code></td>
<td><code>StdEnv/2023 gcc/12.3  openmpi/4.1.5  cuda/12.2  gromacs/2024.1</code></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr>
<tr>
<td>gromacs/2023.5</td>
<td><code>StdEnv/2023  gcc/12.3  openmpi/4.1.5  gromacs/2023.5</code></td>
<td><code>StdEnv/2023  gcc/12.3  openmpi/4.1.5  cuda/12.2  gromacs/2023.5</code></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr>
<tr>
<td>gromacs/2023.3</td>
<td><code>StdEnv/2023  gcc/12.3  openmpi/4.1.5  gromacs/2023.3</code></td>
<td><code>StdEnv/2023 gcc/12.3  openmpi/4.1.5  cuda/12.2  gromacs/2023.3</code></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr>
</tbody></table></div>
<div class="tabs-content tabs-content-2">
<table class="wikitable sortable">

<tbody><tr>
<th>GROMACS version</th>
<th>modules for running on CPUs</th>
<th>modules for running on GPUs (CUDA)</th>
<th>Notes
</th></tr>
<tr>
<td>gromacs/2023.2</td>
<td><code>StdEnv/2020  gcc/9.3.0  openmpi/4.0.3  gromacs/2023.2</code></td>
<td><code>StdEnv/2020  gcc/9.3.0  cuda/11.4  openmpi/4.0.3  gromacs/2023.2</code></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr>
<tr>
<td>gromacs/2023</td>
<td><code>StdEnv/2020  gcc/9.3.0  openmpi/4.0.3  gromacs/2023</code></td>
<td><code>StdEnv/2020  gcc/9.3.0  cuda/11.4  openmpi/4.0.3  gromacs/2023</code></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr>
<tr>
<td>gromacs/2022.3</td>
<td><code>StdEnv/2020  gcc/9.3.0  openmpi/4.0.3  gromacs/2022.3</code></td>
<td><code>StdEnv/2020  gcc/9.3.0  cuda/11.4  openmpi/4.0.3  gromacs/2022.3</code></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr>
<tr>
<td>gromacs/2022.2</td>
<td><code>StdEnv/2020  gcc/9.3.0  openmpi/4.0.3  gromacs/2022.2</code></td>
<td></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr>
<tr>
<td>gromacs/2021.6</td>
<td><code>StdEnv/2020  gcc/9.3.0  openmpi/4.0.3  gromacs/2021.6</code></td>
<td><code>StdEnv/2020  gcc/9.3.0  cuda/11.4  openmpi/4.0.3  gromacs/2021.6</code></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr>
<tr>
<td>gromacs/2021.4</td>
<td><code>StdEnv/2020  gcc/9.3.0  openmpi/4.0.3  gromacs/2021.4</code></td>
<td><code>StdEnv/2020  gcc/9.3.0  cuda/11.4  openmpi/4.0.3  gromacs/2021.4</code> <span class="mw-default-size" typeof="mw:File/Frameless"><a href="/wiki/File:Warning_sign_16px.png" class="mw-file-description" title="warning"><img alt="warning" src="/mediawiki/images/c/c2/Warning_sign_16px.png" decoding="async" width="18" height="16" class="mw-file-element" /></a></span></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr>
<tr>
<td>gromacs/2021.2</td>
<td></td>
<td><code>StdEnv/2020  gcc/9.3.0  cuda/11.4  openmpi/4.0.3  gromacs/2021.2</code> <span class="mw-default-size" typeof="mw:File/Frameless"><a href="/wiki/File:Warning_sign_16px.png" class="mw-file-description" title="warning"><img alt="warning" src="/mediawiki/images/c/c2/Warning_sign_16px.png" decoding="async" width="18" height="16" class="mw-file-element" /></a></span></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr>
<tr>
<td>gromacs/2021.2</td>
<td><code>StdEnv/2020  gcc/9.3.0  openmpi/4.0.3  gromacs/2021.2</code></td>
<td><code>StdEnv/2020  gcc/9.3.0  cuda/11.0  openmpi/4.0.3  gromacs/2021.2</code> <span class="mw-default-size" typeof="mw:File/Frameless"><a href="/wiki/File:Warning_sign_16px.png" class="mw-file-description" title="warning"><img alt="warning" src="/mediawiki/images/c/c2/Warning_sign_16px.png" decoding="async" width="18" height="16" class="mw-file-element" /></a></span></td>
<td>GCC &amp; MKL
</td></tr>
<tr>
<td>gromacs/2020.6</td>
<td><code>StdEnv/2020  gcc/9.3.0  openmpi/4.0.3  gromacs/2020.6</code></td>
<td><code>StdEnv/2020  gcc/9.3.0  cuda/11.4  openmpi/4.0.3  gromacs/2020.6</code> <span class="mw-default-size" typeof="mw:File/Frameless"><a href="/wiki/File:Warning_sign_16px.png" class="mw-file-description" title="warning"><img alt="warning" src="/mediawiki/images/c/c2/Warning_sign_16px.png" decoding="async" width="18" height="16" class="mw-file-element" /></a></span></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr>
<tr>
<td>gromacs/2020.4</td>
<td></td>
<td><code>StdEnv/2020  gcc/9.3.0  cuda/11.4  openmpi/4.0.3  gromacs/2020.4</code> <span class="mw-default-size" typeof="mw:File/Frameless"><a href="/wiki/File:Warning_sign_16px.png" class="mw-file-description" title="warning"><img alt="warning" src="/mediawiki/images/c/c2/Warning_sign_16px.png" decoding="async" width="18" height="16" class="mw-file-element" /></a></span></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr>
<tr>
<td>gromacs/2020.4</td>
<td><code>StdEnv/2020  gcc/9.3.0  openmpi/4.0.3  gromacs/2020.4</code></td>
<td><code>StdEnv/2020  gcc/9.3.0  cuda/11.0  openmpi/4.0.3  gromacs/2020.4</code> <span class="mw-default-size" typeof="mw:File/Frameless"><a href="/wiki/File:Warning_sign_16px.png" class="mw-file-description" title="warning"><img alt="warning" src="/mediawiki/images/c/c2/Warning_sign_16px.png" decoding="async" width="18" height="16" class="mw-file-element" /></a></span></td>
<td>GCC &amp; MKL
</td></tr></tbody></table></div>
<div class="tabs-content tabs-content-3">
<p><br />
</p>
<div class="panel warning"><div class="panel-heading warning"><div class="panel-heading-icon warning"></div><div class="panel-heading-title">Deprecated</div></div>
<div class="panel-body warning">
<p>This <a href="/wiki/Standard_software_environments" title="Standard software environments">software environment</a> is no longer supported.
</p>
</div></div>
<p><br />
</p><p><br />
</p>
<table class="wikitable sortable">

<tbody><tr>
<th>GROMACS version</th>
<th>modules for running on CPUs</th>
<th>modules for running on GPUs (CUDA)</th>
<th>Notes
</th></tr>
<tr>
<td>gromacs/2020.2</td>
<td><code>StdEnv/2018.3  gcc/7.3.0 openmpi/3.1.2 gromacs/2020.2</code></td>
<td><code>StdEnv/2018.3  gcc/7.3.0 cuda/10.0.130 openmpi/3.1.2  gromacs/2020.2</code> <span class="mw-default-size" typeof="mw:File/Frameless"><a href="/wiki/File:Warning_sign_16px.png" class="mw-file-description" title="warning"><img alt="warning" src="/mediawiki/images/c/c2/Warning_sign_16px.png" decoding="async" width="18" height="16" class="mw-file-element" /></a></span></td>
<td>GCC &amp; MKL
</td></tr>
<tr>
<td>gromacs/2019.6</td>
<td><code>StdEnv/2018.3  gcc/7.3.0 openmpi/3.1.2 gromacs/2019.6</code></td>
<td><code>StdEnv/2018.3  gcc/7.3.0 cuda/10.0.130 openmpi/3.1.2  gromacs/2019.6</code></td>
<td>GCC &amp; MKL
</td></tr>
<tr>
<td>gromacs/2019.3</td>
<td><code>StdEnv/2018.3  gcc/7.3.0 openmpi/3.1.2 gromacs/2019.3</code></td>
<td><code>StdEnv/2018.3  gcc/7.3.0 cuda/10.0.130 openmpi/3.1.2  gromacs/2019.3</code></td>
<td>GCC &amp; MKL &#160;&#8225;
</td></tr>
<tr>
<td>gromacs/2018.7</td>
<td><code>StdEnv/2018.3  gcc/7.3.0 openmpi/3.1.2 gromacs/2018.7</code></td>
<td><code>StdEnv/2018.3  gcc/7.3.0 cuda/10.0.130 openmpi/3.1.2  gromacs/2018.7</code></td>
<td>GCC &amp; MKL
</td></tr></tbody></table></div>
<div class="tabs-content tabs-content-4">
<p><br />
</p>
<div class="panel warning"><div class="panel-heading warning"><div class="panel-heading-icon warning"></div><div class="panel-heading-title">Deprecated</div></div>
<div class="panel-body warning">
<p>This <a href="/wiki/Standard_software_environments" title="Standard software environments">software environment</a> is no longer supported.
</p>
</div></div>
<p><br />
</p><p><br />
</p>
<table class="wikitable sortable">

<tbody><tr>
<th>GROMACS version</th>
<th>modules for running on CPUs</th>
<th>modules for running on GPUs (CUDA)</th>
<th>Notes
</th></tr>
<tr>
<td>gromacs/2018.3</td>
<td><code>StdEnv/2016.4  gcc/6.4.0 openmpi/2.1.1 gromacs/2018.3</code></td>
<td><code>StdEnv/2016.4  gcc/6.4.0 cuda/9.0.176 openmpi/2.1.1  gromacs/2018.3</code></td>
<td>GCC &amp; FFTW
</td></tr>
<tr>
<td>gromacs/2018.2</td>
<td><code>StdEnv/2016.4  gcc/6.4.0 openmpi/2.1.1 gromacs/2018.2</code></td>
<td><code>StdEnv/2016.4  gcc/6.4.0 cuda/9.0.176 openmpi/2.1.1  gromacs/2018.2</code></td>
<td>GCC &amp; FFTW
</td></tr>
<tr>
<td>gromacs/2018.1</td>
<td><code>StdEnv/2016.4  gcc/6.4.0 openmpi/2.1.1 gromacs/2018.1</code></td>
<td><code>StdEnv/2016.4  gcc/6.4.0 cuda/9.0.176 openmpi/2.1.1  gromacs/2018.1</code></td>
<td>GCC &amp; FFTW
</td></tr>
<tr>
<td>gromacs/2018</td>
<td><code>StdEnv/2016.4  gromacs/2018</code></td>
<td><code>StdEnv/2016.4  cuda/9.0.176 gromacs/2018</code></td>
<td>Intel &amp; MKL
</td></tr>
<tr>
<td>gromacs/2016.5</td>
<td><code>StdEnv/2016.4  gcc/6.4.0  openmpi/2.1.1 gromacs/2016.5</code></td>
<td><code>StdEnv/2016.4  gcc/6.4.0  cuda/9.0.176  openmpi/2.1.1 gromacs/2016.5</code></td>
<td>GCC &amp; FFTW
</td></tr>
<tr>
<td>gromacs/2016.3</td>
<td><code>StdEnv/2016.4  gromacs/2016.3</code></td>
<td><code>StdEnv/2016.4  cuda/8.0.44 gromacs/2016.3</code></td>
<td>Intel &amp; MKL
</td></tr>
<tr>
<td>gromacs/5.1.5</td>
<td><code>StdEnv/2016.4  gromacs/5.1.5</code></td>
<td><code>StdEnv/2016.4  cuda/8.0.44 gromacs/5.1.5</code></td>
<td>Intel &amp; MKL
</td></tr>
<tr>
<td>gromacs/5.1.4</td>
<td><code>StdEnv/2016.4  gromacs/5.1.4</code></td>
<td><code>StdEnv/2016.4  cuda/8.0.44 gromacs/5.1.4</code></td>
<td>Intel &amp; MKL
</td></tr>
<tr>
<td>gromacs/5.0.7</td>
<td><code>StdEnv/2016.4  gromacs/5.0.7</code></td>
<td><code>StdEnv/2016.4  cuda/8.0.44 gromacs/5.0.7</code></td>
<td>Intel &amp; MKL
</td></tr>
<tr>
<td>gromacs/4.6.7</td>
<td><code>StdEnv/2016.4  gromacs/4.6.7</code></td>
<td><code>StdEnv/2016.4  cuda/8.0.44 gromacs/4.6.7</code></td>
<td>Intel &amp; MKL
</td></tr>
<tr>
<td>gromacs/4.6.7</td>
<td><code>StdEnv/2016.4  gcc/5.4.0  openmpi/2.1.1 gromacs/4.6.7</code></td>
<td><code>StdEnv/2016.4  gcc/5.4.0  cuda/8.0  openmpi/2.1.1  gromacs/4.6.7</code></td>
<td>GCC &amp; MKL &amp; ThreadMPI
</td></tr></tbody></table></div>
</div></div>
<p><b>Notes:</b>
</p>
<ul><li><span class="mw-default-size" typeof="mw:File/Frameless"><a href="/wiki/File:Warning_sign_16px.png" class="mw-file-description" title="warning"><img alt="warning" src="/mediawiki/images/c/c2/Warning_sign_16px.png" decoding="async" width="18" height="16" class="mw-file-element" /></a></span> GROMACS versions 2020.0 up to and including 2021.5 contain a bug when used on GPUs of Volta or newer generations (i.e. V100, T4 and A100) with <code>mdrun</code> option <code>-update gpu</code> that could have perturbed the virial calculation and, in turn, led to incorrect pressure coupling. The GROMACS developers state in the 2021.6 Release Notes:<sup id="cite_ref-1" class="reference"><a href="#cite_note-1"><span class="cite-bracket">&#91;</span>1<span class="cite-bracket">&#93;</span></a></sup> <blockquote><p><i>The GPU update is not enabled by default, so the error can only appear in simulations where it [the <code>-update gpu</code> option] was manually selected, and even in this case the error might be rare since we have not observed it in practice in the testing we have performed.</i></p></blockquote> Further discussion of this bug can be found in the GitLab issue #4393 of the GROMACS project.<sup id="cite_ref-2" class="reference"><a href="#cite_note-2"><span class="cite-bracket">&#91;</span>2<span class="cite-bracket">&#93;</span></a></sup></li>
<li>Version 2020.4 and newer have been compiled for the new <a href="/wiki/Standard_software_environments" title="Standard software environments">Standard software environment</a> <code>StdEnv/2020</code>.</li>
<li>Version 2018.7 and newer have been compiled with GCC compilers and the MKL-library, as they run a bit faster.</li>
<li>Older versions have been compiled with either with GCC compilers and FFTW or Intel compilers, using Intel MKL and Open MPI 2.1.1 libraries from the default environment as indicated in the table above.</li>
<li>CPU (non-GPU) versions are available in both single- and double precision, with the exception of 2019.3 (<b>&#8225;</b>), where double precision is not available for AVX512.</li></ul>
<p>These modules can be loaded by using a <code>module load</code> command with the modules as stated in the second column in the above table.
For example:
</p>
<pre>$ module load  StdEnv/2023  gcc/12.3   openmpi/4.1.5  gromacs/2024.4
or 
$ module load  StdEnv/2020  gcc/9.3.0  openmpi/4.0.3  gromacs/2023.2
</pre>
<p>These versions are also available with GPU support, albeit only with single precision. In order to load the GPU enabled version, the <code>cuda</code> module needs to be loaded first. The modules needed are listed in the third column of above table, e.g.:
</p>
<pre>$ module load  StdEnv/2023  gcc/12.3  openmpi/4.1.5  cuda/12.2  gromacs/2024.4
or
$ module load  StdEnv/2020  gcc/9.3.0  cuda/11.4  openmpi/4.0.3  gromacs/2023.2 
</pre>
<p>For more information on environment modules, please refer to the <a href="/wiki/Using_modules" class="mw-redirect" title="Using modules">Using modules</a> page.
</p>
<h2><span class="mw-headline" id="Suffixes">Suffixes</span></h2>
<h3><span id="GROMACS_5.x.2C_2016.x_and_newer"></span><span class="mw-headline" id="GROMACS_5.x,_2016.x_and_newer">GROMACS 5.x, 2016.x and newer</span></h3>
<p>GROMACS 5 and newer releases consist of only four binaries that contain the full functionality. 
All GROMACS tools from previous versions have been implemented as sub-commands of the gmx binaries.
Please refer to <a rel="nofollow" class="external text" href="http://www.gromacs.org/Documentation/How-tos/Tool_Changes_for_5.0">GROMACS 5.0 Tool Changes</a> and the <a rel="nofollow" class="external text" href="http://manual.gromacs.org/documentation/">GROMACS documentation manuals</a> for your version.
</p>
<dl><dd><ul><li><b><code>gmx</code></b>       - mixed ("single") precision GROMACS with OpenMP (threading) but without MPI.</li>
<li><b><code>gmx_mpi</code></b>   - mixed ("single") precision GROMACS with OpenMP and MPI.</li>
<li><b><code>gmx_d</code></b>     - double precision GROMACS with OpenMP but without MPI.</li>
<li><b><code>gmx_mpi_d</code></b> - double precision GROMACS with OpenMP and MPI.</li></ul></dd></dl>
<h3><span class="mw-headline" id="GROMACS_4.6.7">GROMACS 4.6.7</span></h3>
<ul><li>The double precision binaries have the suffix <code>_d</code>.</li>
<li>The parallel single and double precision <code>mdrun</code> binaries are:</li></ul>
<dl><dd><ul><li><b><code>mdrun_mpi</code></b></li>
<li><b><code>mdrun_mpi_d</code></b></li></ul></dd></dl>
<h2><span class="mw-headline" id="Submission_scripts">Submission scripts</span></h2>
<p>Please refer to the page <a href="/wiki/Running_jobs" title="Running jobs">Running jobs</a> for help on using the SLURM workload manager.
</p>
<h3><span class="mw-headline" id="Serial_jobs">Serial jobs</span></h3>
<p>Here's a simple job script for serial mdrun:
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> serial_gromacs_job.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--time%3D0-0%3A30+++++++++%23+time+limit+%28D-HH%3AMM%29%0A%23SBATCH+--mem-per-cpu%3D1000M+++%23+memory+per+CPU+%28in+MB%29%0Amodule+purge++%0Amodule+load++StdEnv%2F2023++gcc%2F12.3++openmpi%2F4.1.5++gromacs%2F2024.4%0Aexport+OMP_NUM_THREADS%3D%22%24%7BSLURM_CPUS_PER_TASK%3A-1%7D%22%0Agmx+mdrun+-nt+1+-deffnm+em" />
<input type="hidden" name="filename" value="serial_gromacs_job.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-sh mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --time=0-0:30         # time limit (D-HH:MM)</span>
<span class="c1">#SBATCH --mem-per-cpu=1000M   # memory per CPU (in MB)</span>
module<span class="w"> </span>purge<span class="w">  </span>
module<span class="w"> </span>load<span class="w">  </span>StdEnv/2023<span class="w">  </span>gcc/12.3<span class="w">  </span>openmpi/4.1.5<span class="w">  </span>gromacs/2024.4
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="k">:-</span><span class="nv">1</span><span class="si">}</span><span class="s2">&quot;</span>
gmx<span class="w"> </span>mdrun<span class="w"> </span>-nt<span class="w"> </span><span class="m">1</span><span class="w"> </span>-deffnm<span class="w"> </span>em
</pre></div>
</div>
<p><br />
</p><p>This will run the simulation of the molecular system in the file <code>em.tpr</code>.
</p>
<h3><span class="mw-headline" id="Whole_nodes">Whole nodes</span></h3>
<p>Commonly the systems which are being simulated with GROMACS are so large that you want to use a number of whole nodes for the simulation.
</p><p>Generally, the product of <code>--ntasks-per-node=</code> and <code>--cpus-per-task</code> has to match the number of CPU cores in the
compute nodes of the cluster. Please see section <a class="mw-selflink-fragment" href="#Performance_and_benchmarking">Performance and benchmarking</a> below.
</p>
<div class="tabs tabs-tabbox"><input type="radio" form="tabs-inputform" id="tabs-input-2-0" name="tabs-2" class="tabs-input tabs-input-0" checked="" /><input type="radio" form="tabs-inputform" id="tabs-input-2-1" name="tabs-2" class="tabs-input tabs-input-1" /><label class="tabs-label" for="tabs-input-2-1" data-tabpos="1">Graham</label><wbr /><input type="radio" form="tabs-inputform" id="tabs-input-2-2" name="tabs-2" class="tabs-input tabs-input-2" /><label class="tabs-label" for="tabs-input-2-2" data-tabpos="2">Cedar</label><wbr /><input type="radio" form="tabs-inputform" id="tabs-input-2-3" name="tabs-2" class="tabs-input tabs-input-3" /><label class="tabs-label" for="tabs-input-2-3" data-tabpos="3">BÃ©luga</label><wbr /><input type="radio" form="tabs-inputform" id="tabs-input-2-4" name="tabs-2" class="tabs-input tabs-input-4" /><label class="tabs-label" for="tabs-input-2-4" data-tabpos="4">Narval</label><wbr /><input type="radio" form="tabs-inputform" id="tabs-input-2-5" name="tabs-2" class="tabs-input tabs-input-5" /><label class="tabs-label" for="tabs-input-2-5" data-tabpos="5">Niagara</label><wbr /><div class="tabs-container" style="">
<div class="tabs-content tabs-content-1">
<div class="code-file">
<div class="filename"><b>File&#160;:</b> gromacs_whole_node_graham.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes%3D1++++++++++++++++%23+number+of+nodes%0A%23SBATCH+--ntasks-per-node%3D16+++++%23+request+16+MPI+tasks+per+node%0A%23SBATCH+--cpus-per-task%3D2++++++++%23+2+OpenMP+threads+per+MPI+task+%3D%3E+total%3A+16+x+2+%3D+32+CPUs%2Fnode%0A%23SBATCH+--mem-per-cpu%3D2000M++++++%23+memory+per+CPU+%28in+MB%29%0A%23SBATCH+--time%3D0-01%3A00+++++++++++%23+time+limit+%28D-HH%3AMM%29%0Amodule+purge++%0Amodule+load++StdEnv%2F2023++gcc%2F12.3++openmpi%2F4.1.5++gromacs%2F2024.4%0Aexport+OMP_NUM_THREADS%3D%22%24%7BSLURM_CPUS_PER_TASK%3A-1%7D%22%0A%0Asrun+--cpus-per-task%3D%24OMP_NUM_THREADS+gmx_mpi+mdrun+-deffnm+md" />
<input type="hidden" name="filename" value="gromacs_whole_node_graham.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-sh mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes=1                # number of nodes</span>
<span class="c1">#SBATCH --ntasks-per-node=16     # request 16 MPI tasks per node</span>
<span class="c1">#SBATCH --cpus-per-task=2        # 2 OpenMP threads per MPI task =&gt; total: 16 x 2 = 32 CPUs/node</span>
<span class="c1">#SBATCH --mem-per-cpu=2000M      # memory per CPU (in MB)</span>
<span class="c1">#SBATCH --time=0-01:00           # time limit (D-HH:MM)</span>
module<span class="w"> </span>purge<span class="w">  </span>
module<span class="w"> </span>load<span class="w">  </span>StdEnv/2023<span class="w">  </span>gcc/12.3<span class="w">  </span>openmpi/4.1.5<span class="w">  </span>gromacs/2024.4
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="k">:-</span><span class="nv">1</span><span class="si">}</span><span class="s2">&quot;</span>

srun<span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="nv">$OMP_NUM_THREADS</span><span class="w"> </span>gmx_mpi<span class="w"> </span>mdrun<span class="w"> </span>-deffnm<span class="w"> </span>md
</pre></div>
</div>
</div>
<div class="tabs-content tabs-content-2">
<div class="code-file">
<div class="filename"><b>File&#160;:</b> gromacs_whole_node_cedar.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes%3D1++++++++++++++++++++++++%23+number+of+nodes%0A%23SBATCH+--ntasks-per-node%3D24+++++++++++++%23+request+24+MPI+tasks+per+node%0A%23SBATCH+--cpus-per-task%3D2++++++++++++++++%23+2+OpenMP+threads+per+MPI+task+%3D%3E+total%3A+24+x+2+%3D+48+CPUs%2Fnode%0A%23SBATCH+--constraint%3D%22%5Bskylake%7Ccascade%5D%22+%23+restrict+to+AVX512+capable+nodes.%0A%23SBATCH+--mem-per-cpu%3D2000M++++++++++++++%23+memory+per+CPU+%28in+MB%29%0A%23SBATCH+--time%3D0-01%3A00+++++++++++++++++++%23+time+limit+%28D-HH%3AMM%29%0Amodule+purge%0Amodule+load+arch%2Favx512+++%23+switch+architecture+for+up+to+30%25+speedup%0Amodule+load++StdEnv%2F2023++gcc%2F12.3++openmpi%2F4.1.5++gromacs%2F2024.4%0Aexport+OMP_NUM_THREADS%3D%22%24%7BSLURM_CPUS_PER_TASK%3A-1%7D%22%0A+%0Asrun+--cpus-per-task%3D%24OMP_NUM_THREADS+gmx_mpi+mdrun+-deffnm+md" />
<input type="hidden" name="filename" value="gromacs_whole_node_cedar.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-sh mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes=1                        # number of nodes</span>
<span class="c1">#SBATCH --ntasks-per-node=24             # request 24 MPI tasks per node</span>
<span class="c1">#SBATCH --cpus-per-task=2                # 2 OpenMP threads per MPI task =&gt; total: 24 x 2 = 48 CPUs/node</span>
<span class="c1">#SBATCH --constraint=&quot;[skylake|cascade]&quot; # restrict to AVX512 capable nodes.</span>
<span class="c1">#SBATCH --mem-per-cpu=2000M              # memory per CPU (in MB)</span>
<span class="c1">#SBATCH --time=0-01:00                   # time limit (D-HH:MM)</span>
module<span class="w"> </span>purge
module<span class="w"> </span>load<span class="w"> </span>arch/avx512<span class="w">   </span><span class="c1"># switch architecture for up to 30% speedup</span>
module<span class="w"> </span>load<span class="w">  </span>StdEnv/2023<span class="w">  </span>gcc/12.3<span class="w">  </span>openmpi/4.1.5<span class="w">  </span>gromacs/2024.4
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="k">:-</span><span class="nv">1</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="w"> </span>
srun<span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="nv">$OMP_NUM_THREADS</span><span class="w"> </span>gmx_mpi<span class="w"> </span>mdrun<span class="w"> </span>-deffnm<span class="w"> </span>md
</pre></div>
</div>
</div>
<div class="tabs-content tabs-content-3">
<div class="code-file">
<div class="filename"><b>File&#160;:</b> gromacs_whole_node_beluga.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes%3D1++++++++++++++++%23+number+of+nodes%0A%23SBATCH+--ntasks-per-node%3D20+++++%23+request+20+MPI+tasks+per+node%0A%23SBATCH+--cpus-per-task%3D2++++++++%23+2+OpenMP+threads+per+MPI+task+%3D%3E+total%3A+20+x+2+%3D+40+CPUs%2Fnode%0A%23SBATCH+--mem-per-cpu%3D2000M++++++%23+memory+per+CPU+%28in+MB%29%0A%23SBATCH+--time%3D0-01%3A00+++++++++++%23+time+limit+%28D-HH%3AMM%29%0Amodule+purge++%0Amodule+load++StdEnv%2F2023++gcc%2F12.3++openmpi%2F4.1.5++gromacs%2F2024.4%0Aexport+OMP_NUM_THREADS%3D%22%24%7BSLURM_CPUS_PER_TASK%3A-1%7D%22%0A+%0Asrun+--cpus-per-task%3D%24OMP_NUM_THREADS+gmx_mpi+mdrun+-deffnm+md" />
<input type="hidden" name="filename" value="gromacs_whole_node_beluga.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-sh mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes=1                # number of nodes</span>
<span class="c1">#SBATCH --ntasks-per-node=20     # request 20 MPI tasks per node</span>
<span class="c1">#SBATCH --cpus-per-task=2        # 2 OpenMP threads per MPI task =&gt; total: 20 x 2 = 40 CPUs/node</span>
<span class="c1">#SBATCH --mem-per-cpu=2000M      # memory per CPU (in MB)</span>
<span class="c1">#SBATCH --time=0-01:00           # time limit (D-HH:MM)</span>
module<span class="w"> </span>purge<span class="w">  </span>
module<span class="w"> </span>load<span class="w">  </span>StdEnv/2023<span class="w">  </span>gcc/12.3<span class="w">  </span>openmpi/4.1.5<span class="w">  </span>gromacs/2024.4
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="k">:-</span><span class="nv">1</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="w"> </span>
srun<span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="nv">$OMP_NUM_THREADS</span><span class="w"> </span>gmx_mpi<span class="w"> </span>mdrun<span class="w"> </span>-deffnm<span class="w"> </span>md
</pre></div>
</div>
</div>
<div class="tabs-content tabs-content-4">
<div class="code-file">
<div class="filename"><b>File&#160;:</b> gromacs_whole_node_beluga.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes%3D1++++++++++++++++%23+number+of+nodes%0A%23SBATCH+--ntasks-per-node%3D32+++++%23+request+32+MPI+tasks+per+node%0A%23SBATCH+--cpus-per-task%3D2++++++++%23+2+OpenMP+threads+per+MPI+task+%3D%3E+total%3A+32+x+2+%3D+64+CPUs%2Fnode%0A%23SBATCH+--mem-per-cpu%3D2000M++++++%23+memory+per+CPU+%28in+MB%29%0A%23SBATCH+--time%3D0-01%3A00+++++++++++%23+time+limit+%28D-HH%3AMM%29%0Amodule+purge++%0Amodule+load++StdEnv%2F2023++gcc%2F12.3++openmpi%2F4.1.5++gromacs%2F2024.4%0Aexport+OMP_NUM_THREADS%3D%22%24%7BSLURM_CPUS_PER_TASK%3A-1%7D%22%0A+%0Asrun+--cpus-per-task%3D%24OMP_NUM_THREADS+gmx_mpi+mdrun+-deffnm+md" />
<input type="hidden" name="filename" value="gromacs_whole_node_beluga.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-sh mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes=1                # number of nodes</span>
<span class="c1">#SBATCH --ntasks-per-node=32     # request 32 MPI tasks per node</span>
<span class="c1">#SBATCH --cpus-per-task=2        # 2 OpenMP threads per MPI task =&gt; total: 32 x 2 = 64 CPUs/node</span>
<span class="c1">#SBATCH --mem-per-cpu=2000M      # memory per CPU (in MB)</span>
<span class="c1">#SBATCH --time=0-01:00           # time limit (D-HH:MM)</span>
module<span class="w"> </span>purge<span class="w">  </span>
module<span class="w"> </span>load<span class="w">  </span>StdEnv/2023<span class="w">  </span>gcc/12.3<span class="w">  </span>openmpi/4.1.5<span class="w">  </span>gromacs/2024.4
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="k">:-</span><span class="nv">1</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="w"> </span>
srun<span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="nv">$OMP_NUM_THREADS</span><span class="w"> </span>gmx_mpi<span class="w"> </span>mdrun<span class="w"> </span>-deffnm<span class="w"> </span>md
</pre></div>
</div>
</div>
<div class="tabs-content tabs-content-5">
<div class="code-file">
<div class="filename"><b>File&#160;:</b> gromacs_whole_node_niagara.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes%3D1++++++++++++++++%23+number+of+nodes%0A%23SBATCH+--ntasks-per-node%3D10+++++%23+request+10+MPI+tasks+per+node%0A%23SBATCH+--cpus-per-task%3D4++++++++%23+4+OpenMP+threads+per+MPI+task+%3D%3E+total%3A+10+x+4+%3D+40+CPUs%2Fnode%0A%23SBATCH+--mem-per-cpu%3D2000M++++++%23+memory+per+CPU+%28in+MB%29%0A%23SBATCH+--time%3D0-01%3A00+++++++++++%23+time+limit+%28D-HH%3AMM%29%0Amodule+purge+--force%0Amodule+load+CCEnv%0Amodule+load++StdEnv%2F2023++gcc%2F12.3++openmpi%2F4.1.5++gromacs%2F2024.4%0Aexport+OMP_NUM_THREADS%3D%22%24%7BSLURM_CPUS_PER_TASK%3A-1%7D%22%0A+%0Asrun+--cpus-per-task%3D%24OMP_NUM_THREADS+gmx_mpi+mdrun+-deffnm+md" />
<input type="hidden" name="filename" value="gromacs_whole_node_niagara.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-sh mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes=1                # number of nodes</span>
<span class="c1">#SBATCH --ntasks-per-node=10     # request 10 MPI tasks per node</span>
<span class="c1">#SBATCH --cpus-per-task=4        # 4 OpenMP threads per MPI task =&gt; total: 10 x 4 = 40 CPUs/node</span>
<span class="c1">#SBATCH --mem-per-cpu=2000M      # memory per CPU (in MB)</span>
<span class="c1">#SBATCH --time=0-01:00           # time limit (D-HH:MM)</span>
module<span class="w"> </span>purge<span class="w"> </span>--force
module<span class="w"> </span>load<span class="w"> </span>CCEnv
module<span class="w"> </span>load<span class="w">  </span>StdEnv/2023<span class="w">  </span>gcc/12.3<span class="w">  </span>openmpi/4.1.5<span class="w">  </span>gromacs/2024.4
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="k">:-</span><span class="nv">1</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="w"> </span>
srun<span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="nv">$OMP_NUM_THREADS</span><span class="w"> </span>gmx_mpi<span class="w"> </span>mdrun<span class="w"> </span>-deffnm<span class="w"> </span>md
</pre></div>
</div>
</div>
</div></div>
<h3><span class="mw-headline" id="GPU_job">GPU job</span></h3>
<p>Please read <a href="/wiki/Using_GPUs_with_Slurm" title="Using GPUs with Slurm">Using GPUs with Slurm</a> for general information on using GPUs on our systems.
</p><p>This is a job script for mdrun using 4 OpenMP threads and one GPU:
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> gpu_gromacs_job.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--gpus-per-node%3D1++++++++%23+request+1+GPU+per+node%0A%23SBATCH+--cpus-per-task%3D4++++++++%23+number+of+OpenMP+threads+per+MPI+process%0A%23SBATCH+--mem-per-cpu%3D2000M++++++%23+memory+limit+per+CPU+core+%28megabytes%29%0A%23SBATCH+--time%3D0%3A30%3A00+++++++++++%23+time+limit+%28D-HH%3AMM%3Ass%29%0Amodule+purge++%0Amodule+load+StdEnv%2F2023++gcc%2F12.3++openmpi%2F4.1.5++cuda%2F12.2++gromacs%2F2024.4%0Aexport+OMP_NUM_THREADS%3D%22%24%7BSLURM_CPUS_PER_TASK%3A-1%7D%22%0A%0Agmx+mdrun+-ntomp+%24%7BSLURM_CPUS_PER_TASK%3A-1%7D+-deffnm+md" />
<input type="hidden" name="filename" value="gpu_gromacs_job.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-sh mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --gpus-per-node=1        # request 1 GPU per node</span>
<span class="c1">#SBATCH --cpus-per-task=4        # number of OpenMP threads per MPI process</span>
<span class="c1">#SBATCH --mem-per-cpu=2000M      # memory limit per CPU core (megabytes)</span>
<span class="c1">#SBATCH --time=0:30:00           # time limit (D-HH:MM:ss)</span>
module<span class="w"> </span>purge<span class="w">  </span>
module<span class="w"> </span>load<span class="w"> </span>StdEnv/2023<span class="w">  </span>gcc/12.3<span class="w">  </span>openmpi/4.1.5<span class="w">  </span>cuda/12.2<span class="w">  </span>gromacs/2024.4
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="k">:-</span><span class="nv">1</span><span class="si">}</span><span class="s2">&quot;</span>

gmx<span class="w"> </span>mdrun<span class="w"> </span>-ntomp<span class="w"> </span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="k">:-</span><span class="nv">1</span><span class="si">}</span><span class="w"> </span>-deffnm<span class="w"> </span>md
</pre></div>
</div>
<p><br />
</p>
<h4><span class="mw-headline" id="Notes_on_running_GROMACS_on_GPUs">Notes on running GROMACS on GPUs</span></h4>
<p>Note that using more than a single GPU usually leads to poor efficiency. Carefully test and compare multi-GPU and single-GPU performance before deciding to use more than one GPU for your simulations.
</p>
<ul><li><span class="mw-default-size" typeof="mw:File/Frameless"><a href="/wiki/File:Warning_sign_16px.png" class="mw-file-description" title="warning"><img alt="warning" src="/mediawiki/images/c/c2/Warning_sign_16px.png" decoding="async" width="18" height="16" class="mw-file-element" /></a></span> GROMACS versions 2020.0 up to and including 2021.5 contain a bug when used on GPUs of Volta or newer generations (i.e. V100, T4 and A100) with <code>mdrun</code> option <code>-update gpu</code> that could have perturbed the virial calculation and, in turn, led to incorrect pressure coupling. The GROMACS developers state in the 2021.6 Release Notes:<sup id="cite_ref-3" class="reference"><a href="#cite_note-3"><span class="cite-bracket">&#91;</span>3<span class="cite-bracket">&#93;</span></a></sup> <blockquote><p><i>The GPU update is not enabled by default, so the error can only appear in simulations where it was manually selected, and even in this case the error might be rare since we have not observed it in practice in the testing we have performed.</i></p></blockquote> Further discussion of this bug can be found in the GitLab issue #4393 of the GROMACS project.<sup id="cite_ref-4" class="reference"><a href="#cite_note-4"><span class="cite-bracket">&#91;</span>4<span class="cite-bracket">&#93;</span></a></sup></li>
<li>Our clusters (Beluga, Cedar, Graham and Narval) have differently configured GPU nodes.<br />On the page <a href="/wiki/Using_GPUs_with_Slurm#Available_GPUs" title="Using GPUs with Slurm">Using GPUs with Slurm#Available GPUs</a> you can find more information about the different node configurations (GPU models and number of GPUs and CPUs per node).</li>
<li>GROMACS imposes a number of constraints for choosing the number of GPUs, tasks (MPI ranks) and OpenMP threads.<br />For GROMACS 2018.2 the constraints are:</li></ul>
<dl><dd><dl><dd><ul><li>The number of <code>--tasks-per-node</code> always needs to be the same as, or a multiple of the number of GPUs (<code>--gpus-per-node</code>).</li>
<li>GROMACS will not run GPU runs with only 1 OpenMP thread unless forced by setting the <code>-ntomp</code> option.<br />According to GROMACS developers, the optimum number of <code>--cpus-per-task</code> is between 2 and 6.</li></ul></dd></dl></dd></dl>
<ul><li>Avoid using a larger fraction of CPUs and memory than the fraction of GPUs you have requested in a node.</li></ul>
<p>You can explore some benchmark results on our <a rel="nofollow" class="external text" href="https://mdbench.ace-net.ca/mdbench/bform/?software_contains=GROMACS.cuda&amp;software_id=&amp;module_contains=&amp;module_version=&amp;site_contains=&amp;gpu_model=&amp;cpu_model=&amp;arch=&amp;dataset=6n4o">MDBench portal</a>.
</p>
<h4><span class="mw-headline" id="Running_multiple_simulations_on_a_GPU">Running multiple simulations on a GPU</span></h4>
<p>GROMACS and other MD simulation programs are unable to fully use recent GPU models such as the Nvidia A100 and H100 unless the molecular system is very large (millions of atoms). Running a typical simulation on such a GPU wastes a significant fraction of the allocated computational resources.
</p><p>There are two recommended solutions to this problem. The first one is to run multiple simulations on a single GPU using <code>mdrun -multidir</code> as described below. This is the preferred solution if you run multiple similar simulations, for instance:
</p>
<ul><li>Repeating the same simulation to acquire more conformational space sampling</li>
<li>Simulating multiple protein variants, multiple small ligands in complex with the same protein, multiple temperatures or ionic concentrations, etc.</li>
<li>Ensemble-based simulations such as replica exchange</li></ul>
<p>Similar simulations are needed to ensure proper load balancing. If the simulations are dissimilar, some will progress faster and finish earlier than others, leading to idle resources.
</p><p>The following job script runs three similar simulations in separate directories (<code>sim1</code>, <code>sim2</code>, <code>sim3</code>) using a single GPU. If you change the number of simulations, make sure to adjust <code>--ntasks-per-node</code> and <code>--cpus-per-task</code>: there should be one task per simulation, while the total number of CPU cores should remain constant.
</p>
<div class="tabs tabs-tabbox"><input type="radio" form="tabs-inputform" id="tabs-input-3-0" name="tabs-3" class="tabs-input tabs-input-0" checked="" /><input type="radio" form="tabs-inputform" id="tabs-input-3-1" name="tabs-3" class="tabs-input tabs-input-1" /><label class="tabs-label" for="tabs-input-3-1" data-tabpos="1">Narval</label><wbr /><div class="tabs-container" style="">
<div class="tabs-content tabs-content-1">
<div class="code-file">
<div class="filename"><b>File&#160;:</b> gpu_gromacs_job_multidir.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--gpus-per-node%3D1++++++++%23+request+1+GPU+per+node%0A%23SBATCH+--ntasks-per-node%3D3++++++%23+number+of+MPI+processes+and+simulations%0A%23SBATCH+--cpus-per-task%3D4++++++++%23+number+of+OpenMP+threads+per+MPI+process%0A%23SBATCH+--mem-per-cpu%3D2000M++++++%23+memory+limit+per+CPU+core+%28megabytes%29%0A%23SBATCH+--time%3D0%3A30%3A00+++++++++++%23+time+limit+%28D-HH%3AMM%3Ass%29%0A%0Amodule+load+StdEnv%2F2023+gcc%2F12.3+openmpi%2F4.1.5+cuda%2F12.2+gromacs%2F2024.4%0A%0Aexport+OMP_NUM_THREADS%3D%22%24%7BSLURM_CPUS_PER_TASK%3A-1%7D%22%0A%0Asrun+gmx_mpi+mdrun+-ntomp+%24%7BSLURM_CPUS_PER_TASK%3A-1%7D+-deffnm+md+%5C%0A++++-multidir+sim1+sim2+sim3" />
<input type="hidden" name="filename" value="gpu_gromacs_job_multidir.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-sh mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --gpus-per-node=1        # request 1 GPU per node</span>
<span class="c1">#SBATCH --ntasks-per-node=3      # number of MPI processes and simulations</span>
<span class="c1">#SBATCH --cpus-per-task=4        # number of OpenMP threads per MPI process</span>
<span class="c1">#SBATCH --mem-per-cpu=2000M      # memory limit per CPU core (megabytes)</span>
<span class="c1">#SBATCH --time=0:30:00           # time limit (D-HH:MM:ss)</span>

module<span class="w"> </span>load<span class="w"> </span>StdEnv/2023<span class="w"> </span>gcc/12.3<span class="w"> </span>openmpi/4.1.5<span class="w"> </span>cuda/12.2<span class="w"> </span>gromacs/2024.4

<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="k">:-</span><span class="nv">1</span><span class="si">}</span><span class="s2">&quot;</span>

srun<span class="w"> </span>gmx_mpi<span class="w"> </span>mdrun<span class="w"> </span>-ntomp<span class="w"> </span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="k">:-</span><span class="nv">1</span><span class="si">}</span><span class="w"> </span>-deffnm<span class="w"> </span>md<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-multidir<span class="w"> </span>sim1<span class="w"> </span>sim2<span class="w"> </span>sim3
</pre></div>
</div>
</div>
</div></div>
<p>The second solution is to use a <a href="/wiki/Multi-Instance_GPU" title="Multi-Instance GPU"> MIG</a> instance (a fraction of a GPU) rather than a full GPU. This is the preferred solution if you have a single simulation or if your simulations are dissimilar, for instance:
</p>
<ul><li>Systems with different sizes (more than a 10&#160;% difference in the numbers of atoms)</li>
<li>Systems with different shapes or compositions, such as a membrane-bound versus a soluble protein</li></ul>
<p><span class="mw-default-size" typeof="mw:File/Frameless"><a href="/wiki/File:Warning_sign_16px.png" class="mw-file-description" title="warning"><img alt="warning" src="/mediawiki/images/c/c2/Warning_sign_16px.png" decoding="async" width="18" height="16" class="mw-file-element" /></a></span> Note that <a href="/wiki/Hyper-Q_/_MPS" title="Hyper-Q / MPS">Hyper-Q / MPS</a> should never be used with GROMACS. The built-in <code>-multidir</code> option achieves the same functionality more efficiently.
</p>
<h1><span class="mw-headline" id="Usage">Usage</span></h1>
<div style="color: red; border: 1px dashed #2f6fab">
<p>More content for this section will be added at a later time.
</p>
</div>
<h2><span class="mw-headline" id="System_preparation">System preparation</span></h2>
<p>In order to run a simulation, one needs to create a <i>tpr</i> file (portable binary run input file). This file contains the starting structure of the simulation, the molecular topology and all the simulation parameters.
</p><p><i>Tpr </i> files are created with the <code>gmx grompp</code> command (or simply <code>grompp</code> for versions older than 5.0). Therefore one needs the following files:
</p>
<ul><li>The coordinate file with the starting structure. GROMACS can read the starting structure from various file formats, such as <i>.gro</i>, <i>.pdb</i> or <i>.cpt</i> (checkpoint).</li>
<li>The (system) topology (<i>.top</i>)) file. It defines which force field is used and how the force field parameters are applied to the simulated system. Often the topologies for individual parts of the simulated system (e.g. molecules) are placed in separate <i>.itp</i> files and included in the <i>.top</i> file using a <code>#include</code> directive.</li>
<li>The run parameter (<i>.mdp</i>) file.  See the GROMACS user guide for a detailed description of the options.</li></ul>
<p><i>Tpr</i> files are portable, that is they can be <i>grompp'</i>ed on one machine, copied over to a different machine and used as an input file for <i>mdrun</i>.  One should always use the same version for both <i>grompp</i> and <i>mdrun</i>.  Although <i>mdrun</i> is able to use <i>tpr</i> files that have been created with an older version of <i>grompp</i>, this can lead to unexpected simulation results.
</p>
<h2><span class="mw-headline" id="Running_simulations">Running simulations</span></h2>
<p>MD Simulations often take much longer than the maximum walltime for a job
to complete and therefore need to be restarted.  
To minimize the time a job needs to wait before it starts, you should maximize 
<a href="/wiki/Job_scheduling_policies#Percentage_of_the_nodes_you_have_access_to" title="Job scheduling policies">the number of nodes you have access to</a>
by choosing a shorter running time for your job.  Requesting a walltime of 
24 hours or 72 hours (three days) is often a good trade-off between waiting time 
and running time.
</p><p>You should use the <code>mdrun</code> parameter <code>-maxh</code> to tell
the program the requested walltime so that it gracefully finishes the 
current timestep when reaching 99% of this walltime.  
This causes <code>mdrun</code> to create a new checkpoint file at this 
final timestep and gives it the chance to properly close all output files
(trajectories, energy- and log-files, etc.).
</p><p>For example use <code>#SBATCH --time=24:00</code> along with <code>gmx mdrun -maxh 24 ...</code> 
or  <code>#SBATCH --time=3-00:00</code> along with <code>gmx mdrun -maxh 72 ...</code>.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> gromacs_job.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes%3D1++++++++++++++++%23+number+of+Nodes%0A%23SBATCH+--tasks-per-node%3D32++++++%23+number+of+MPI+processes+per+node%0A%23SBATCH+--mem-per-cpu%3D4000+++++++%23+memory+limit+per+CPU+%28megabytes%29%0A%23SBATCH+--time%3D24%3A00%3A00++++++++++%23+time+limit+%28D-HH%3AMM%3Ass%29%0Amodule+purge%0Amodule+load+StdEnv%2F2023+gcc%2F12.3+openmpi%2F4.1.5+gromacs%2F2024.4%0Aexport+OMP_NUM_THREADS%3D%22%24%7BSLURM_CPUS_PER_TASK%3A-1%7D%22%0A%0Asrun++gmx_mpi++mdrun++-deffnm+md++-maxh+24" />
<input type="hidden" name="filename" value="gromacs_job.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-sh mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes=1                # number of Nodes</span>
<span class="c1">#SBATCH --tasks-per-node=32      # number of MPI processes per node</span>
<span class="c1">#SBATCH --mem-per-cpu=4000       # memory limit per CPU (megabytes)</span>
<span class="c1">#SBATCH --time=24:00:00          # time limit (D-HH:MM:ss)</span>
module<span class="w"> </span>purge
module<span class="w"> </span>load<span class="w"> </span>StdEnv/2023<span class="w"> </span>gcc/12.3<span class="w"> </span>openmpi/4.1.5<span class="w"> </span>gromacs/2024.4
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="k">:-</span><span class="nv">1</span><span class="si">}</span><span class="s2">&quot;</span>

srun<span class="w">  </span>gmx_mpi<span class="w">  </span>mdrun<span class="w">  </span>-deffnm<span class="w"> </span>md<span class="w">  </span>-maxh<span class="w"> </span><span class="m">24</span>
</pre></div>
</div>
<p><br />
</p>
<h3><span class="mw-headline" id="Restarting_simulations">Restarting simulations</span></h3>
<p>You can restart a simulation by using the same <code>mdrun</code> 
command as the original simulation and adding the <code>-cpi state.cpt</code>
parameter where <code>state.cpt</code> is the filename of the most recent
checkpoint file.  Mdrun will by default (since version 4.5) try to append
to the existing files (trajectories, energy- and log-files, etc.).
GROMACS will check the consistency of the output files and - if needed - 
discard timesteps that are newer than that of the checkpoint file.
</p><p>Using the <code>-maxh</code> parameter ensures that the checkpoint and output
files are written in a consistent state when the simulation reaches the time 
limit.
</p><p>The GROMACS manual contains more detailed information
<sup id="cite_ref-5" class="reference"><a href="#cite_note-5"><span class="cite-bracket">&#91;</span>5<span class="cite-bracket">&#93;</span></a></sup>
<sup id="cite_ref-6" class="reference"><a href="#cite_note-6"><span class="cite-bracket">&#91;</span>6<span class="cite-bracket">&#93;</span></a></sup>.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> gromacs_job_restart.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes%3D1++++++++++++++++%23+number+of+Nodes%0A%23SBATCH+--tasks-per-node%3D32++++++%23+number+of+MPI+processes+per+node%0A%23SBATCH+--mem-per-cpu%3D4000+++++++%23+memory+limit+per+CPU+%28megabytes%29%0A%23SBATCH+--time%3D24%3A00%3A00++++++++++%23+time+limit+%28D-HH%3AMM%3Ass%29%0Amodule+purge%0Amodule+load+StdEnv%2F2023+gcc%2F12.3+openmpi%2F4.1.5+gromacs%2F2024.4%0Aexport+OMP_NUM_THREADS%3D%22%24%7BSLURM_CPUS_PER_TASK%3A-1%7D%22%0A%0Asrun++gmx_mpi++mdrun++-deffnm+md++-maxh+24.0++-cpi+md.cpt" />
<input type="hidden" name="filename" value="gromacs_job_restart.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-sh mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes=1                # number of Nodes</span>
<span class="c1">#SBATCH --tasks-per-node=32      # number of MPI processes per node</span>
<span class="c1">#SBATCH --mem-per-cpu=4000       # memory limit per CPU (megabytes)</span>
<span class="c1">#SBATCH --time=24:00:00          # time limit (D-HH:MM:ss)</span>
module<span class="w"> </span>purge
module<span class="w"> </span>load<span class="w"> </span>StdEnv/2023<span class="w"> </span>gcc/12.3<span class="w"> </span>openmpi/4.1.5<span class="w"> </span>gromacs/2024.4
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="k">:-</span><span class="nv">1</span><span class="si">}</span><span class="s2">&quot;</span>

srun<span class="w">  </span>gmx_mpi<span class="w">  </span>mdrun<span class="w">  </span>-deffnm<span class="w"> </span>md<span class="w">  </span>-maxh<span class="w"> </span><span class="m">24</span>.0<span class="w">  </span>-cpi<span class="w"> </span>md.cpt
</pre></div>
</div>
<p><br />
</p>
<h3><span class="mw-headline" id="Checkpointing_simulations">Checkpointing simulations</span></h3>
<p>You can use GROMACSâ€™ ability to restart a simulation to split a long simulation over multiple short jobs. Shorter jobs wait less in the queue. In particular, those that request 3 hours or less are eligible for backfill scheduling. (See our <a href="/wiki/Job_scheduling_policies" title="Job scheduling policies"> job scheduling policies</a>.) This is especially useful if your research group has only a default resource allocation (e.g. <code>def-sponsor</code>) on the cluster, but will benefit even those with competitive resource allocations (e.g. <code>rrg-sponsor</code>).
</p><p>By using a <a href="/wiki/Job_arrays" title="Job arrays"> job array</a>, you can automate checkpointing. With an array job script such as the following, a single <code>sbatch</code> call submits multiple short jobs, but only the first one is eligible to start. As soon as this first job has completed, the next one becomes eligible to start and resume your simulation. This process repeats until all jobs are complete or the simulation is finished, at which point any remaining pending jobs are automatically cancelled.
</p>
<div class="tabs tabs-tabbox"><input type="radio" form="tabs-inputform" id="tabs-input-4-0" name="tabs-4" class="tabs-input tabs-input-0" checked="" /><input type="radio" form="tabs-inputform" id="tabs-input-4-1" name="tabs-4" class="tabs-input tabs-input-1" /><label class="tabs-label" for="tabs-input-4-1" data-tabpos="1">Whole nodes (Narval)</label><wbr /><input type="radio" form="tabs-inputform" id="tabs-input-4-2" name="tabs-4" class="tabs-input tabs-input-2" /><label class="tabs-label" for="tabs-input-4-2" data-tabpos="2">GPU job</label><wbr /><div class="tabs-container" style="">
<div class="tabs-content tabs-content-1">
<div class="code-file">
<div class="filename"><b>File&#160;:</b> gromacs_job_checkpoint.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes%3D1++++++++++++++++%23+number+of+nodes%0A%23SBATCH+--ntasks-per-node%3D32+++++%23+request+32+MPI+tasks+per+node%0A%23SBATCH+--cpus-per-task%3D2++++++++%23+2+OpenMP+threads+per+MPI+task%0A%23SBATCH+--mem-per-cpu%3D2000M++++++%23+memory+per+CPU+%28in+MB%29%0A%23SBATCH+--time%3D03%3A00%3A00++++++++++%23+time+limit+%28D-HH%3AMM%3Ass%29%0A%23SBATCH+--array%3D1-20%251+++++++++++%23+job+range%2C+running+only+1+at+a+time%0A%0Amodule+load+StdEnv%2F2023+gcc%2F12.3+openmpi%2F4.1.5+gromacs%2F2024.4%0A%0A%23+Adjust+simulation+name+%28default+filename+for+-deffnm+option%29%0Asim_name%3Dmd%0A%0Ant%3D%24%7BSLURM_CPUS_PER_TASK%3A-1%7D%0Aexport+OMP_NUM_THREADS%3D%24nt%0A%0Anhours%3D%24%28squeue+-h+-j+%24SLURM_JOB_ID+-O+TimeLimit+%7C+cut+-d%3A+-f1%29%0A%0Asrun+gmx_mpi+mdrun+-ntomp+%24nt+-deffnm+%24simname+-cpi+%22%24simname.cpt%22+-maxh+%24nhours%0A%0Aexit_code%3D%24%3F%0Aif+%28%28+exit_code+%21%3D+0+%29%29%3B+then%0A%09echo+%22Simulation+exited+with+an+error%2C+cancelling+pending+jobs%22%0A%09scancel+-t+pending+%24SLURM_ARRAY_JOB_ID%0A%09exit+%24exit_code%0Afi%0A%0Ansteps_expr%3D%27%5E%5B%5B%3Aspace%3A%5D%5D%2Ansteps%5B%5B%3Aspace%3A%5D%5D%2A%3D%5B%5B%3Aspace%3A%5D%5D%2A%5B%5B%3Adigit%3A%5D%5D%2A%24%27%0Ansteps%3D%24%28grep+%22%24nsteps_expr%22+%22%24simname.log%22+%7C+awk+%27%7B+print+%243+%7D%27%29%0Aif+grep+%22%5EWriting+checkpoint%2C+step+%24nsteps+at+%22+%22%24simname.log%22%3B+then%0A%09echo+%22Simulation+finished%2C+cancelling+pending+jobs%22%0A%09scancel+-t+pending+%24SLURM_ARRAY_JOB_ID%0Afi" />
<input type="hidden" name="filename" value="gromacs_job_checkpoint.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-sh mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes=1                # number of nodes</span>
<span class="c1">#SBATCH --ntasks-per-node=32     # request 32 MPI tasks per node</span>
<span class="c1">#SBATCH --cpus-per-task=2        # 2 OpenMP threads per MPI task</span>
<span class="c1">#SBATCH --mem-per-cpu=2000M      # memory per CPU (in MB)</span>
<span class="c1">#SBATCH --time=03:00:00          # time limit (D-HH:MM:ss)</span>
<span class="c1">#SBATCH --array=1-20%1           # job range, running only 1 at a time</span>

module<span class="w"> </span>load<span class="w"> </span>StdEnv/2023<span class="w"> </span>gcc/12.3<span class="w"> </span>openmpi/4.1.5<span class="w"> </span>gromacs/2024.4

<span class="c1"># Adjust simulation name (default filename for -deffnm option)</span>
<span class="nv">sim_name</span><span class="o">=</span>md

<span class="nv">nt</span><span class="o">=</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="k">:-</span><span class="nv">1</span><span class="si">}</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="nv">$nt</span>

<span class="nv">nhours</span><span class="o">=</span><span class="k">$(</span>squeue<span class="w"> </span>-h<span class="w"> </span>-j<span class="w"> </span><span class="nv">$SLURM_JOB_ID</span><span class="w"> </span>-O<span class="w"> </span>TimeLimit<span class="w"> </span><span class="p">|</span><span class="w"> </span>cut<span class="w"> </span>-d:<span class="w"> </span>-f1<span class="k">)</span>

srun<span class="w"> </span>gmx_mpi<span class="w"> </span>mdrun<span class="w"> </span>-ntomp<span class="w"> </span><span class="nv">$nt</span><span class="w"> </span>-deffnm<span class="w"> </span><span class="nv">$simname</span><span class="w"> </span>-cpi<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$simname</span><span class="s2">.cpt&quot;</span><span class="w"> </span>-maxh<span class="w"> </span><span class="nv">$nhours</span>

<span class="nv">exit_code</span><span class="o">=</span><span class="nv">$?</span>
<span class="k">if</span><span class="w"> </span><span class="o">((</span><span class="w"> </span>exit_code<span class="w"> </span>!<span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="o">))</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">	</span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Simulation exited with an error, cancelling pending jobs&quot;</span>
<span class="w">	</span>scancel<span class="w"> </span>-t<span class="w"> </span>pending<span class="w"> </span><span class="nv">$SLURM_ARRAY_JOB_ID</span>
<span class="w">	</span><span class="nb">exit</span><span class="w"> </span><span class="nv">$exit_code</span>
<span class="k">fi</span>

<span class="nv">nsteps_expr</span><span class="o">=</span><span class="s1">&#39;^[[:space:]]*nsteps[[:space:]]*=[[:space:]]*[[:digit:]]*$&#39;</span>
<span class="nv">nsteps</span><span class="o">=</span><span class="k">$(</span>grep<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$nsteps_expr</span><span class="s2">&quot;</span><span class="w"> </span><span class="s2">&quot;</span><span class="nv">$simname</span><span class="s2">.log&quot;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;{ print $3 }&#39;</span><span class="k">)</span>
<span class="k">if</span><span class="w"> </span>grep<span class="w"> </span><span class="s2">&quot;^Writing checkpoint, step </span><span class="nv">$nsteps</span><span class="s2"> at &quot;</span><span class="w"> </span><span class="s2">&quot;</span><span class="nv">$simname</span><span class="s2">.log&quot;</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">	</span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Simulation finished, cancelling pending jobs&quot;</span>
<span class="w">	</span>scancel<span class="w"> </span>-t<span class="w"> </span>pending<span class="w"> </span><span class="nv">$SLURM_ARRAY_JOB_ID</span>
<span class="k">fi</span>
</pre></div>
</div>
</div>
<div class="tabs-content tabs-content-2">
<div class="code-file">
<div class="filename"><b>File&#160;:</b> gromacs_job_checkpoint.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--gpus-per-node%3D1++++++++%23+request+1+GPU+per+node%0A%23SBATCH+--cpus-per-task%3D4++++++++%23+number+of+OpenMP+threads+per+MPI+process%0A%23SBATCH+--mem-per-cpu%3D2000M++++++%23+memory+limit+per+CPU+core+%28megabytes%29%0A%23SBATCH+--time%3D03%3A00%3A00++++++++++%23+time+limit+%28D-HH%3AMM%3Ass%29%0A%23SBATCH+--array%3D1-20%251+++++++++++%23+job+range%2C+running+only+1+at+a+time%0A%0Amodule+load+StdEnv%2F2023+gcc%2F12.3+openmpi%2F4.1.5+cuda%2F12.2+gromacs%2F2024.4%0A%0A%23+Adjust+simulation+name+%28default+filename+for+-deffnm+option%29%0Asim_name%3Dmd%0A%0Ant%3D%24%7BSLURM_CPUS_PER_TASK%3A-1%7D%0Aexport+OMP_NUM_THREADS%3D%24nt%0A%0Anhours%3D%24%28squeue+-h+-j+%24SLURM_JOB_ID+-O+TimeLimit+%7C+cut+-d%3A+-f1%29%0A%0Asrun+gmx+mdrun+-ntomp+%24nt+-deffnm+%24simname+-cpi+%22%24simname.cpt%22+-maxh+%24nhours%0A%0Aexit_code%3D%24%3F%0Aif+%28%28+exit_code+%21%3D+0+%29%29%3B+then%0A%09echo+%22Simulation+exited+with+an+error%2C+cancelling+pending+jobs%22%0A%09scancel+-t+pending+%24SLURM_ARRAY_JOB_ID%0A%09exit+%24exit_code%0Afi%0A%0Ansteps_expr%3D%27%5E%5B%5B%3Aspace%3A%5D%5D%2Ansteps%5B%5B%3Aspace%3A%5D%5D%2A%3D%5B%5B%3Aspace%3A%5D%5D%2A%5B%5B%3Adigit%3A%5D%5D%2A%24%27%0Ansteps%3D%24%28grep+%22%24nsteps_expr%22+%22%24simname.log%22+%7C+awk+%27%7B+print+%243+%7D%27%29%0Aif+grep+%22%5EWriting+checkpoint%2C+step+%24nsteps+at+%22+%22%24simname.log%22%3B+then%0A%09echo+%22Simulation+finished%2C+cancelling+pending+jobs%22%0A%09scancel+-t+pending+%24SLURM_ARRAY_JOB_ID%0Afi" />
<input type="hidden" name="filename" value="gromacs_job_checkpoint.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-sh mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --gpus-per-node=1        # request 1 GPU per node</span>
<span class="c1">#SBATCH --cpus-per-task=4        # number of OpenMP threads per MPI process</span>
<span class="c1">#SBATCH --mem-per-cpu=2000M      # memory limit per CPU core (megabytes)</span>
<span class="c1">#SBATCH --time=03:00:00          # time limit (D-HH:MM:ss)</span>
<span class="c1">#SBATCH --array=1-20%1           # job range, running only 1 at a time</span>

module<span class="w"> </span>load<span class="w"> </span>StdEnv/2023<span class="w"> </span>gcc/12.3<span class="w"> </span>openmpi/4.1.5<span class="w"> </span>cuda/12.2<span class="w"> </span>gromacs/2024.4

<span class="c1"># Adjust simulation name (default filename for -deffnm option)</span>
<span class="nv">sim_name</span><span class="o">=</span>md

<span class="nv">nt</span><span class="o">=</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="k">:-</span><span class="nv">1</span><span class="si">}</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="nv">$nt</span>

<span class="nv">nhours</span><span class="o">=</span><span class="k">$(</span>squeue<span class="w"> </span>-h<span class="w"> </span>-j<span class="w"> </span><span class="nv">$SLURM_JOB_ID</span><span class="w"> </span>-O<span class="w"> </span>TimeLimit<span class="w"> </span><span class="p">|</span><span class="w"> </span>cut<span class="w"> </span>-d:<span class="w"> </span>-f1<span class="k">)</span>

srun<span class="w"> </span>gmx<span class="w"> </span>mdrun<span class="w"> </span>-ntomp<span class="w"> </span><span class="nv">$nt</span><span class="w"> </span>-deffnm<span class="w"> </span><span class="nv">$simname</span><span class="w"> </span>-cpi<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$simname</span><span class="s2">.cpt&quot;</span><span class="w"> </span>-maxh<span class="w"> </span><span class="nv">$nhours</span>

<span class="nv">exit_code</span><span class="o">=</span><span class="nv">$?</span>
<span class="k">if</span><span class="w"> </span><span class="o">((</span><span class="w"> </span>exit_code<span class="w"> </span>!<span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="o">))</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">	</span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Simulation exited with an error, cancelling pending jobs&quot;</span>
<span class="w">	</span>scancel<span class="w"> </span>-t<span class="w"> </span>pending<span class="w"> </span><span class="nv">$SLURM_ARRAY_JOB_ID</span>
<span class="w">	</span><span class="nb">exit</span><span class="w"> </span><span class="nv">$exit_code</span>
<span class="k">fi</span>

<span class="nv">nsteps_expr</span><span class="o">=</span><span class="s1">&#39;^[[:space:]]*nsteps[[:space:]]*=[[:space:]]*[[:digit:]]*$&#39;</span>
<span class="nv">nsteps</span><span class="o">=</span><span class="k">$(</span>grep<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$nsteps_expr</span><span class="s2">&quot;</span><span class="w"> </span><span class="s2">&quot;</span><span class="nv">$simname</span><span class="s2">.log&quot;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;{ print $3 }&#39;</span><span class="k">)</span>
<span class="k">if</span><span class="w"> </span>grep<span class="w"> </span><span class="s2">&quot;^Writing checkpoint, step </span><span class="nv">$nsteps</span><span class="s2"> at &quot;</span><span class="w"> </span><span class="s2">&quot;</span><span class="nv">$simname</span><span class="s2">.log&quot;</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">	</span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Simulation finished, cancelling pending jobs&quot;</span>
<span class="w">	</span>scancel<span class="w"> </span>-t<span class="w"> </span>pending<span class="w"> </span><span class="nv">$SLURM_ARRAY_JOB_ID</span>
<span class="k">fi</span>
</pre></div>
</div>
</div>
</div></div>
<h1><span class="mw-headline" id="Performance_and_benchmarking">Performance and benchmarking</span></h1>
<p>A team at <a rel="nofollow" class="external text" href="https://www.ace-net.ca/">ACENET</a> has created a <a rel="nofollow" class="external text" href="https://mdbench.ace-net.ca/mdbench/">Molecular Dynamics Performance Guide</a> for Alliance clusters.
It can help you determine optimal conditions for AMBER, GROMACS, NAMD, and OpenMM jobs. The present section focuses on GROMACS performance.
</p><p>Getting the best mdrun performance with GROMACS is not a straightforward 
task. The GROMACS developers are maintaining a long section in their user-guide
dedicated to mdrun-performance<sup id="cite_ref-performance_7-0" class="reference"><a href="#cite_note-performance-7"><span class="cite-bracket">&#91;</span>7<span class="cite-bracket">&#93;</span></a></sup>
which explains all relevant options/parameters and strategies.
</p><p>There is no "One size fits all", but the best parameters to choose highly
depend on the size of the system (number of particles as well as size and 
shape of the simulation box) and the simulation parameters (cutoffs, use of 
Particle-Mesh-Ewald<sup id="cite_ref-perf-background_8-0" class="reference"><a href="#cite_note-perf-background-8"><span class="cite-bracket">&#91;</span>8<span class="cite-bracket">&#93;</span></a></sup>
(PME) method for long-range electrostatics).
</p><p>GROMACS prints performance information and statistics at the end of the 
<code>md.log</code> file, which is helpful in identifying bottlenecks.  
This section often contains notes on how to further improve the performance.
</p><p>The <b>simulation performance</b> is typically quantified by the number of 
nanoseconds of MD-trajectory that can be simulated within a day (ns/day).
</p><p><b>Parallel scaling</b> is a measure of how effectively the compute resources 
are used.  It is defined as:
</p>
<dl><dd><span>S = p<sub>N</sub> / ( N * p<sub>1</sub> )</span></dd></dl>
<p>Where <i>p<sub>N</sub></i> is the performance using <i>N</i> CPU cores.
</p><p>Ideally, the performance increases linearly with the number of CPU cores 
("linear scaling"; S = 1).
</p>
<h2><span id="MPI_processes_.2F_Slurm_tasks_.2F_Domain_decomposition"></span><span class="mw-headline" id="MPI_processes_/_Slurm_tasks_/_Domain_decomposition">MPI processes / Slurm tasks / Domain decomposition</span></h2>
<p>The most straightforward way to increase the number of MPI processes (called 
MPI-ranks in the GROMACS documentation), which is done by using Slurm's
<code>--ntasks</code> or <code>--ntasks-per-node</code> in the job script.  
</p><p>GROMACS uses <b>Domain Decomposition</b><sup id="cite_ref-perf-background_8-1" class="reference"><a href="#cite_note-perf-background-8"><span class="cite-bracket">&#91;</span>8<span class="cite-bracket">&#93;</span></a></sup> (DD) 
to distribute the work of solving the non-bonded Particle-Particle (PP) 
interactions across multiple CPU cores. This is done by effectively cutting
the simulation box along the X, Y and/or Z axes into domains and assigning
each domain to one MPI process.
</p><p>This works well until the time needed for communication becomes large in
respect to the size (in respect of <i>number of particles</i> as well as <i>volume</i>) 
of the domain. In that case the parallel scaling will drop significantly 
below 1 and in extreme cases the performance drops when increasing the 
number of domains.
</p><p>GROMACS can use <b>Dynamic Load Balancing</b> to shift the boundaries between
domains to some extent, in order to avoid certain domains taking significantly
longer to solve than others.  The <code>mdrun</code> parameter 
<code>-dlb auto</code> is the default.
</p><p>Domains cannot be smaller in any direction than the longest cutoff radius.
</p>
<h3><span class="mw-headline" id="Long-range_interactions_with_PME">Long-range interactions with PME</span></h3>
<p>The Particle-Mesh-Ewald method (PME) is often used to calculate the long-range
non-bonded interactions (interactions beyond the cutoff radius).  As PME
requires global communication, the performance can degrade quickly when 
many MPI processes are involved that are calculating both the short-range 
(PP) as well as the long-range (PME) interactions.  This is avoided by having
dedicated MPI processes that only perform PME (PME-ranks).
</p><p>GROMACS mdrun by default uses heuristics to dedicate a number of MPI 
processes to PME when the total number of MPI processes 12 or greater.  
The mdrun parameter <code>-npme</code> can be used to select the number of 
PME ranks manually.
</p><p>In case there is a significant "Load Imbalance" between the PP and PME ranks
(e.g. the PP ranks have more work per timestep than the PME ranks), one can
shift work from the PP ranks to the PME ranks by increasing the cutoff radius.
This will not affect the result, as the sum of short-range + long-range forces
(or energies) will be the same for a given timestep.  Mdrun will attempt to
do that automatically since version 4.6 unless the mdrun parameter 
<code>-notunepme</code> is used.
</p><p>Since version 2018, PME can be offloaded to the GPU (see below)
however the implementation as of version 2018.1 has still several limitations
<sup id="cite_ref-gpu-pme-2018.1_9-0" class="reference"><a href="#cite_note-gpu-pme-2018.1-9"><span class="cite-bracket">&#91;</span>9<span class="cite-bracket">&#93;</span></a></sup> among them that only
a single GPU rank can be dedicated to PME.
</p>
<h2><span id="OpenMP_threads_.2F_CPUs-per-task"></span><span class="mw-headline" id="OpenMP_threads_/_CPUs-per-task">OpenMP threads / CPUs-per-task</span></h2>
<p>Once Domain Decomposition with MPI processes reaches the scaling limit 
(parallel scaling starts dropping), performance can be further improved by
using <b>OpenMP threads</b> to spread the work of an MPI process (rank) over more
than one CPU core.  To use OpenMP threads, use Slurm's <code>--cpus-per-task</code>
parameter in the job script (both for <code>#SBATCH&gt;</code> and <code>srun</code>)
and either set the <i>OMP_NUM_THREADS</i> variable with:
<code>export OMP_NUM_THREADS="${SLURM_CPUS_PER_TASK:-1}"</code> (recommended)
or the mdrun parameter <code>-ntomp ${SLURM_CPUS_PER_TASK:-1}</code>.
</p><p>According to GROMACS developers, the optimum is usually between 2 and 6 OpenMP threads
per MPI process (cpus-per-task).  However for jobs running on a very large
number of nodes it might be worth trying even larger number of <i>cpus-per-task</i>.
</p><p>Especially for systems that don't use PME, we don't have to worry about a 
"PP-PME Load Imbalance".  In those cases we can choose 2 or 4 <i>ntasks-per-node</i>
and set <i>cpus-per-task</i> to a value that <i>ntasks-per-node * cpus-per-task</i> 
matches the number of CPU cores in a compute node.
</p>
<h2><span class="mw-headline" id="CPU_architecture">CPU architecture</span></h2>
<p>GROMACS uses optimized kernel functions to compute the real-space portion of short-range, non-bonded interactions. Kernel functions are available for a variety of SIMD instruction sets, such as AVX, AVX2, and AVX512. Kernel functions are chosen when compiling GROMACS, and should match the capabilities of the CPUs that will be used to run the simulations. This is done for you by the Compute Canada team: when you load a GROMACS module into your environment, an appropriate AVX/AVX2/AVX512 version is chosen depending on the architecture of the cluster. GROMACS reports what SIMD instruction set it supports in its log file, and will warn you if the selected kernel function is suboptimal.
</p><p>However, certain clusters contain a mix of CPUs that have different levels of SIMD support. When that is the case, the smallest common denominator is used. For instance, if the cluster has Skylake (AVX/AVX2/AVX512) and Broadwell (AVX/AVX2) CPUs, as Cedar currently (May 2020) does, a version of GROMACS compiled for the AVX2 instruction set will be used. This means that you may end up with a suboptimal choice of kernel function, depending on which compute nodes the scheduler allocates for your job.
</p><p>You can explicitly request nodes that support AVX512 with the <code>--constraint="[cascade|skylake]"</code> SLURM option on clusters that offer these node types. 
This will make sure that your job will be assigned to nodes based on either the "Cascade Lake" or the "Skylake" architecture (but not a mix of both types).
If working on the command line, make sure to not forget the quotation marks (<code>"</code>) to protect the special characters <code>[</code>, <code>|</code> and <code>]</code>. 
You can then explicitly request AVX512 software using <code>module load arch/avx512</code> before loading any other module. 
</p><p>For example, a simple job script could look like the following:
</p><p><br /> 
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> gromacs_job_cedar_avx512.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes%3D4%0A%23SBATCH+--ntasks-per-node%3D48%0A%23SBATCH+--constraint%3D%22%5Bskylake%7Ccascade%5D%22%0A%23SBATCH+--time%3D24%3A00%3A00%0Amodule+load+arch%2Favx512%0Amodule+load+StdEnv%2F2023+gcc%2F12.3+openmpi%2F4.1.5+gromacs%2F2024.4%0Aexport+OMP_NUM_THREADS%3D%22%24%7BSLURM_CPUS_PER_TASK%3A-1%7D%22%0Asrun+gmx_mpi+mdrun" />
<input type="hidden" name="filename" value="gromacs_job_cedar_avx512.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-sh mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes=4</span>
<span class="c1">#SBATCH --ntasks-per-node=48</span>
<span class="c1">#SBATCH --constraint=&quot;[skylake|cascade]&quot;</span>
<span class="c1">#SBATCH --time=24:00:00</span>
module<span class="w"> </span>load<span class="w"> </span>arch/avx512
module<span class="w"> </span>load<span class="w"> </span>StdEnv/2023<span class="w"> </span>gcc/12.3<span class="w"> </span>openmpi/4.1.5<span class="w"> </span>gromacs/2024.4
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="k">:-</span><span class="nv">1</span><span class="si">}</span><span class="s2">&quot;</span>
srun<span class="w"> </span>gmx_mpi<span class="w"> </span>mdrun
</pre></div>
</div>
<p><br />
</p><p>In our measurements, going from AVX2 to AVX512 on Skylake or Cascade nodes resulted in a 20âˆ’30% performance increase. However, you should also consider that restricting yourself to only AVX512-capable nodes will result in longer wait times in the queue.
</p>
<h2><span class="mw-headline" id="GPUs">GPUs</span></h2>
<div style="color: red; border: 1px dashed #2f6fab">
<p>Tips on how to use GPUs efficiently will be added soon.
</p>
</div>
<h1><span class="mw-headline" id="Analyzing_results">Analyzing results</span></h1>
<h2><span class="mw-headline" id="GROMACS_tools">GROMACS tools</span></h2>
<p><i>GROMACS</i> contains a large number of tools that can be used for common tasks of post-processing and analysis. 
The <i>GROMACS</i> manual contains a <a rel="nofollow" class="external text" href="https://manual.gromacs.org/current/user-guide/cmdline.html#commands-by-topic">list of available commands organized by topic</a> as well as <a rel="nofollow" class="external text" href="https://manual.gromacs.org/current/user-guide/cmdline.html#commands-by-name">organized by name</a> that give a short description and link to the corresponding command reference.
</p><p>These commands will typically read the trajectory (in the <i>XTC</i>, <i>TNG</i> or <i>TRR</i> format) as well as a coordinate file (<i>GRO</i>, <i>PDB</i>, <i>TPR</i>, etc.) and write plots in the <a rel="nofollow" class="external text" href="https://manual.gromacs.org/current/reference-manual/file-formats.html#xvg"><i>XVG</i> format</a> which can be used for inputs for the <a rel="nofollow" class="external text" href="https://plasma-gate.weizmann.ac.il/Grace/">plotting tool Grace</a> (module: <code>grace/5.99.0</code>; command <code>xmgrace</code>; <a rel="nofollow" class="external text" href="https://plasma-gate.weizmann.ac.il/Grace/doc/UsersGuide.html">Grace User Guide</a>). As <i>XVG</i> files are simple text files, they can also be processed with scripts or imported into other spreadsheet programs.
</p>
<h2><span class="mw-headline" id="VMD">VMD</span></h2>
<p><a href="/wiki/VMD" title="VMD">VMD</a> is a molecular visualization program for displaying, animating, and analyzing large biomolecular systems using 3-D graphics and built-in scripting.  It can be used to visually inspect GROMACS trajectories and also offers a large number of built-in and external plugins for analysis.
It can also be used in command line mode.
</p>
<h2><span class="mw-headline" id="Using_Python">Using Python</span></h2>
<p><a rel="nofollow" class="external text" href="https://www.mdanalysis.org/">MDAnalysis</a> and <a rel="nofollow" class="external text" href="https://www.mdtraj.org/">MDTraj</a> are two <a href="/wiki/Python" title="Python">Python</a> packages that we provide as <a href="/wiki/Available_Python_wheels" title="Available Python wheels">precompiled Python wheels</a>. They can read and write trajectory and coordinate files of <i>GROMACS</i> (<i>TRR</i> and <i>XTC</i>) and many other MD packages and also include a variety of commonly used analysis functions.  <i>MDAnalysis</i> can also read topology information from <i>GROMACS</i> <i>TPR</i> files, though often not those created by the latest versions of <i>GROMACS</i>.
</p><p>Both packages feature a versatile atom-selection language and expose the coordinates of the trajectories, which makes it very easy to write custom analysis tools that can be tailored to a specific problem and integrate well with Python's data-science packages like <i>NumPy</i>, <i>SciPy</i> and <i>Pandas</i>, as well as plotting libraries like <i>Matplotlib</i>/<i>Pyplot</i> and <i>Seaborn</i>.
</p>
<h1><span class="mw-headline" id="Related_modules">Related modules</span></h1>
<h2><span class="mw-headline" id="GROMACS-Plumed">GROMACS-Plumed</span></h2>
<p>PLUMED<sup id="cite_ref-PLUMED_10-0" class="reference"><a href="#cite_note-PLUMED-10"><span class="cite-bracket">&#91;</span>10<span class="cite-bracket">&#93;</span></a></sup> is an open source library for free energy calculations in molecular systems which works together with some of the most popular molecular dynamics engines.
</p><p>The <code>gromacs-plumed</code> modules are versions of GROMACS that have been patched with PLUMED's modifications  so that they can run meta-dynamics simulations.
</p>
<div class="tabs tabs-tabbox"><input type="radio" form="tabs-inputform" id="tabs-input-5-0" name="tabs-5" class="tabs-input tabs-input-0" checked="" /><input type="radio" form="tabs-inputform" id="tabs-input-5-1" name="tabs-5" class="tabs-input tabs-input-1" /><label class="tabs-label" for="tabs-input-5-1" data-tabpos="1">StdEnv/2023</label><wbr /><input type="radio" form="tabs-inputform" id="tabs-input-5-2" name="tabs-5" class="tabs-input tabs-input-2" /><label class="tabs-label" for="tabs-input-5-2" data-tabpos="2">StdEnv/2020</label><wbr /><input type="radio" form="tabs-inputform" id="tabs-input-5-3" name="tabs-5" class="tabs-input tabs-input-3" /><label class="tabs-label" for="tabs-input-5-3" data-tabpos="3">StdEnv/2018 and StdEnv/2016</label><wbr /><div class="tabs-container" style="">
<div class="tabs-content tabs-content-1">
<table class="wikitable sortable">

<tbody><tr>
<th>GROMACS</th>
<th>PLUMED</th>
<th>modules for running on CPUs</th>
<th>modules for running on GPUs (CUDA)</th>
<th>Notes
</th></tr>
<tr>
<td>v2023.5</td>
<td>v2.9.2</td>
<td><code>StdEnv/2023  gcc/12.3  openmpi/4.1.5  gromacs-plumed/2023.5</code></td>
<td><code>StdEnv/2023  gcc/12.3  openmpi/4.1.5  cuda/12.2  gromacs-plumed/2023.5</code></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr>
<tr>
<td>v2020.7</td>
<td>v2.8.5</td>
<td><code>StdEnv/2023  gcc/12.3  openmpi/4.1.5  gromacs-plumed/2020.7</code></td>
<td><code>StdEnv/2023  gcc/12.3  openmpi/4.1.5  cuda/12.2  gromacs-plumed/2020.7</code></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr></tbody></table></div>
<div class="tabs-content tabs-content-2">
<table class="wikitable sortable">

<tbody><tr>
<th>GROMACS</th>
<th>PLUMED</th>
<th>modules for running on CPUs</th>
<th>modules for running on GPUs (CUDA)</th>
<th>Notes
</th></tr>
<tr>
<td>v2022.6</td>
<td>v2.8.3</td>
<td><code>StdEnv/2020  gcc/9.3.0  openmpi/4.0.3  gromacs-plumed/2022.6</code></td>
<td><code>StdEnv/2020  gcc/9.3.0  cuda/11.4  openmpi/4.0.3  gromacs-plumed/2022.6</code></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr>
<tr>
<td>v2022.3</td>
<td>v2.8.1</td>
<td><code>StdEnv/2020  gcc/9.3.0  openmpi/4.0.3  gromacs-plumed/2022.3</code></td>
<td><code>StdEnv/2020  gcc/9.3.0  cuda/11.4  openmpi/4.0.3  gromacs-plumed/2022.3</code></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr>
<tr>
<td>v2021.6</td>
<td>v2.7.4</td>
<td><code>StdEnv/2020  gcc/9.3.0  openmpi/4.0.3  gromacs-plumed/2021.6</code></td>
<td><code>StdEnv/2020  gcc/9.3.0  cuda/11.4  openmpi/4.0.3  gromacs-plumed/2021.6</code></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr>
<tr>
<td>v2021.4</td>
<td>v2.7.3</td>
<td><code>StdEnv/2020  gcc/9.3.0  openmpi/4.0.3  gromacs-plumed/2021.4</code></td>
<td><code>StdEnv/2020  gcc/9.3.0  cuda/11.4  openmpi/4.0.3  gromacs-plumed/2021.4</code></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr>
<tr>
<td>v2021.2</td>
<td>v2.7.1</td>
<td><code>StdEnv/2020  gcc/9.3.0  openmpi/4.0.3  gromacs-plumed/2021.2</code></td>
<td><code>StdEnv/2020  gcc/9.3.0  cuda/11.0  openmpi/4.0.3  gromacs-plumed/2021.2</code></td>
<td>GCC &amp; MKL
</td></tr>
<tr>
<td>v2019.6</td>
<td>v2.6.2</td>
<td><code>StdEnv/2020  gcc/9.3.0  openmpi/4.0.3  gromacs-plumed/2019.6</code></td>
<td><code>StdEnv/2020  gcc/9.3.0  cuda/11.0  openmpi/4.0.3  gromacs-plumed/2019.6</code></td>
<td>GCC &amp; MKL
</td></tr></tbody></table></div>
<div class="tabs-content tabs-content-3">
<p><br />
</p>
<div class="panel warning"><div class="panel-heading warning"><div class="panel-heading-icon warning"></div><div class="panel-heading-title">Deprecated</div></div>
<div class="panel-body warning">
<p>These <a href="/wiki/Standard_software_environments" title="Standard software environments">software environments</a> are no longer supported.
</p>
</div></div>
<p><br />
</p><p><br />
</p>
<table class="wikitable sortable">

<tbody><tr>
<th>GROMACS</th>
<th>PLUMED</th>
<th>modules for running on CPUs</th>
<th>modules for running on GPUs (CUDA)</th>
<th>Notes
</th></tr>
<tr>
<td>v2019.6</td>
<td>v2.5.4</td>
<td><code>StdEnv/2018.3  gcc/7.3.0  openmpi/3.1.2  gromacs-plumed/2019.6</code></td>
<td><code>StdEnv/2018.3  gcc/7.3.0  cuda/10.0.130  openmpi/3.1.2 gromacs-plumed/2019.6</code></td>
<td>GCC &amp; MKL
</td></tr>
<tr>
<td>v2019.5</td>
<td>v2.5.3</td>
<td><code>StdEnv/2018.3  gcc/7.3.0  openmpi/3.1.2  gromacs-plumed/2019.5</code></td>
<td><code>StdEnv/2018.3  gcc/7.3.0  cuda/10.0.130  openmpi/3.1.2 gromacs-plumed/2019.5</code></td>
<td>GCC &amp; MKL
</td></tr>
<tr>
<td>v2018.1</td>
<td>v2.4.2</td>
<td><code>StdEnv/2016.4  gcc/6.4.0  openmpi/2.1.1  gromacs-plumed/2018.1</code></td>
<td><code>StdEnv/2016.4  gcc/6.4.0  cuda/9.0.176  openmpi/2.1.1 gromacs-plumed/2018.1</code></td>
<td>GCC &amp; FFTW
</td></tr>
<tr>
<td>v2016.3</td>
<td>v2.3.2</td>
<td><code>StdEnv/2016.4  intel/2016.4  openmpi/2.1.1  gromacs-plumed/2016.3</code></td>
<td><code>StdEnv/2016.4  intel/2016.4  cuda/8.0.44  openmpi/2.1.1  gromacs-plumed/2016.3</code></td>
<td>Intel &amp; MKL
</td></tr></tbody></table></div>
</div></div>
<h2><span class="mw-headline" id="GROMACS-Colvars">GROMACS-Colvars</span></h2>
<p>Colvars<sup id="cite_ref-Colvars_11-0" class="reference"><a href="#cite_note-Colvars-11"><span class="cite-bracket">&#91;</span>11<span class="cite-bracket">&#93;</span></a></sup> is a software module for molecular simulation programs, which 
adds additional capabilities of collective variables to apply biasing potentials, calculate potentials-of-mean-force (PMFs) 
along any set of variables, use enhanced sampling methods, such as Adaptive Biasing Force (ABF), metadynamics, steered MD and umbrella sampling.
</p><p>As of GROMACS v2024<sup id="cite_ref-12" class="reference"><a href="#cite_note-12"><span class="cite-bracket">&#91;</span>12<span class="cite-bracket">&#93;</span></a></sup>, 
the Colvars library has been added to the official GROMACS releases and can be used without the need of a patched version.
</p><p>Documentation on how to use Colvars with GROMACS:
</p>
<ul><li><i>Collective Variable simulations with the Colvars module</i><sup id="cite_ref-13" class="reference"><a href="#cite_note-13"><span class="cite-bracket">&#91;</span>13<span class="cite-bracket">&#93;</span></a></sup> in the GROMACS Reference manual,</li>
<li>Molecular dynamics parameters (.mdp options) for the Colvars module<sup id="cite_ref-14" class="reference"><a href="#cite_note-14"><span class="cite-bracket">&#91;</span>14<span class="cite-bracket">&#93;</span></a></sup>,</li>
<li>the Colvars Reference manual for GROMACS<sup id="cite_ref-colvars-gromacs_15-0" class="reference"><a href="#cite_note-colvars-gromacs-15"><span class="cite-bracket">&#91;</span>15<span class="cite-bracket">&#93;</span></a></sup>,</li>
<li>the publication: Fiorin et al. <b>2013</b>, <i>Using collective variables to drive molecular dynamics simulations.</i><sup id="cite_ref-colvars-paper_16-0" class="reference"><a href="#cite_note-colvars-paper-16"><span class="cite-bracket">&#91;</span>16<span class="cite-bracket">&#93;</span></a></sup></li></ul>
<p>GROMACS versions prior to v2024 required that they have been patched with Colvars modifications, so that
the collective variables can be used in simulations.<br />
The <code>gromacs-colvars/2020.6</code> module is such a modified version of GROMACS that includes Colvars 2021-12-20.
</p>
<table class="wikitable sortable">

<tbody><tr>
<th>GROMACS</th>
<th>Colvars</th>
<th>modules for running on CPUs</th>
<th>modules for running on GPUs (CUDA)</th>
<th>Notes
</th></tr>
<tr>
<td>v2020.6</td>
<td>2021-12-20</td>
<td><code>StdEnv/2020  gcc/9.3.0  openmpi/4.0.3  gromacs-colvars/2020.6</code></td>
<td><code>StdEnv/2020  gcc/9.3.0  cuda/11.4  openmpi/4.0.3  gromacs-colvars/2020.6</code></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr></tbody></table>
<h2><span class="mw-headline" id="GROMACS-CP2K">GROMACS-CP2K</span></h2>
<p>CP2K<sup id="cite_ref-cp2k_17-0" class="reference"><a href="#cite_note-cp2k-17"><span class="cite-bracket">&#91;</span>17<span class="cite-bracket">&#93;</span></a></sup> is a quantum chemistry and solid-state physics software package.
Since version 2022 GROMACS can be compiled with CP2K-support<sup id="cite_ref-gromacs-cp2k_18-0" class="reference"><a href="#cite_note-gromacs-cp2k-18"><span class="cite-bracket">&#91;</span>18<span class="cite-bracket">&#93;</span></a></sup> to enable Hybrid Quantum-Classical simulations (QM/MM)<sup id="cite_ref-gromacs-cp2k-qmmm-ref_19-0" class="reference"><a href="#cite_note-gromacs-cp2k-qmmm-ref-19"><span class="cite-bracket">&#91;</span>19<span class="cite-bracket">&#93;</span></a></sup>.
</p><p>The <code>gromacs-cp2k</code> modules are versions of GROMACS that have been compiled with CP2K QM/MM support.
</p><p>Different from other GROMACS modules, these modules are only available for CPU calculations and not for GPUs (CUDA).
Also the modules contain only MPI-enabled executables:
</p>
<ul><li><b><code>gmx_mpi</code></b>   - mixed precision GROMACS with OpenMP and MPI.</li>
<li><b><code>gmx_mpi_d</code></b> - double precision GROMACS with OpenMP and MPI.</li></ul>
<table class="wikitable sortable">

<tbody><tr>
<th>GROMACS</th>
<th>CP2K</th>
<th>modules for running on CPUs</th>
<th>Notes
</th></tr>
<tr>
<td>v2022.2</td>
<td>9.1</td>
<td><code>StdEnv/2020  gcc/9.3.0  openmpi/4.0.3  gromacs-cp2k/2022.2</code></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr></tbody></table>
<p>Here are links to various resources for running QM/MM simulations with this combination of GROMACS and CP2K:
</p>
<ul><li><a rel="nofollow" class="external text" href="https://manual.gromacs.org/documentation/current/reference-manual/special/qmmm.html">Hybrid Quantum-Classical simulations (QM/MM) with CP2K interface</a> in the GROMACS manual.</li>
<li><a rel="nofollow" class="external text" href="https://docs.bioexcel.eu/qmmm_bpg/">CP2K QM/MM Best Practices Guide</a> by BioExcel.</li>
<li><a rel="nofollow" class="external text" href="https://docs.bioexcel.eu/2021-04-22-qmmm-gromacs-cp2k/">QM/MM with GROMACS + CP2K</a> Workshop material from BioExcel.<br />This contains tutorial material for setting up and running QM/MM simulations as well as links to YouTube videos with theory lectures.  This material was written to be used with HPC resources from the European Centre of Excellence for Computational Biomolecular Research (BioExcel), however only small adjustments are needed to use our HPC systems instead.<br />Most notably the command <code>gmx_cp2k</code> needs to be replaced with either <code>gmx_mpi</code> (mixed precision) or <code>gmx_mpi_d</code> (double precision) and the job scripts (which are also using Slurm), need to be adjusted as well.</li></ul>
<dl><dd><ul><li><a rel="nofollow" class="external text" href="https://github.com/bioexcel/2022-06-16-gromacs-cp2k-tutorial">GitHub Repository</a> with example file for BioExcel Tutorial.</li></ul></dd></dl>
<ul><li><a rel="nofollow" class="external text" href="https://www.cp2k.org/tools:gromacs">GROMACS-CP2K integration</a> on CP2K homepage.</li></ul>
<h2><span class="mw-headline" id="GROMACS-LS">GROMACS-LS</span></h2>
<p>GROMACS-LS<sup id="cite_ref-Local_stress_20-0" class="reference"><a href="#cite_note-Local_stress-20"><span class="cite-bracket">&#91;</span>20<span class="cite-bracket">&#93;</span></a></sup> and the MDStress library enable the calculation of local stress fields from molecular dynamics simulations.
The MDStress library is included in the GROMACS-LS module.
</p><p>Please refer to manual for GROMACS-LS at: <a rel="nofollow" class="external text" href="https://vanegaslab.org/files/Local_stress.pdf">Local_stress.pdf</a> and the publications listed therein for information about the method and how to use it.
</p><p>Invoking commands like <code>gmx_LS mdrun -rerun</code> or <code>gmx_LS trjconv</code> needs a <code>.tpr</code> file.
If you want to analyze a trajectory that has been simulated with a newer version of GROMACS (e.g. 2024), then an older version cannot read that .tpr file because new options are added to the format specification with every major release (2018, 2019 ... 2024).
But as the answer to Q14 in the <a rel="nofollow" class="external text" href="https://vanegaslab.org/files/Local_stress.pdf">Local_stress.pdf</a> document suggests, you can use <code>gmx_LS grompp</code> or <code>gmx grompp</code> from the 2016.6 version (which is available as well) to create a new .tpr file using the same input files (*.mdp, topol.top, *.itp, *.gro, etc.) which were used to make the <code>.tpr</code> file for the simulation.
This new .tpr is then compatible with GROMACS-LS 2016.3.<br />
In case the <code>*.mdp</code> files used any keywords or features that were not yet present in 2016 (e.g. <code>pcouple = C-rescale</code>), then you need to either change or remove it (e.g. change to <code>pcouple = Berendsen</code>).
In the case of pcouple, the result will not differ anyway, because the trajectory is processed as with the <code>-rerun</code> option and pressure coupling will not happen in that case.
The mentioning of <code>cutoff-scheme = group</code> in the answer to Q14 can be ignored, because GROMACS 2016 already supports "cutoff-scheme = Verlet" and the "group" scheme was removed for GROMACS 2020.
Therefore GROMACS-LS 2016.3 can be used to process simulations that used either cutoff scheme.
</p><p>Notes:
</p>
<ul><li>Because the manual was written for the older GROMACS-LS v4.5.5 and that the core gromacs commands have changed in version 5, you need to use commands like <code>gmx_LS mdrun</code> and <code>gmx_LS trjconv</code> instead of <code>mdrun_LS</code> and <code>trjconv_LS</code>.</li>
<li>GROMACS-LS requires to be compiled in double precision does not support MPI, SIMD hardware acceleration nor GPUs and is therefore much slower than normal GROMACS. It can only use a single CPU core.</li>
<li>Unlike other patched versions of GROMACS, the modules <code>gromacs-ls/2016.3</code> and <code>gromacs/2016.6</code> can be loaded at the same time.</li></ul>
<table class="wikitable sortable">

<tbody><tr>
<th>module</th>
<th>modules for running on CPUs</th>
<th>Notes
</th></tr>
<tr>
<td>gromacs-ls/2016.3</td>
<td><code>StdEnv/2023  gcc/12.3  gromacs-ls/2016.3</code></td>
<td>GROMACS-LS is a serial application and does not support MPI, OpenMP or GPUs/CUDA.
</td></tr>
<tr>
<td>gromacs/2016.6</td>
<td><code>StdEnv/2023  gcc/12.3  openmpi/4.1.5  gromacs/2016.6</code></td>
<td>This Gromacs module can be used to prepare TPR input files for GROMACS-LS.
</td></tr></tbody></table>
<h2><span class="mw-headline" id="GROMACS-RAMD">GROMACS-RAMD</span></h2>
<p>GROMACS-RAMD is a fork of GROMACS that implements the <i>Random Acceleration Molecular Dynamics</i> (RAMD) method.<sup id="cite_ref-RAMD_21-0" class="reference"><a href="#cite_note-RAMD-21"><span class="cite-bracket">&#91;</span>21<span class="cite-bracket">&#93;</span></a></sup>
This method can be used to identify ligand exit routes from the buried binding pockets of receptors and investigate the mechanism of ligand dissociation 
by running molecular dynamics simulations with an additional randomly oriented force applied to a molecule in the system.
</p><p>Information on RAMD-specific MDP options<sup id="cite_ref-22" class="reference"><a href="#cite_note-22"><span class="cite-bracket">&#91;</span>22<span class="cite-bracket">&#93;</span></a></sup> 
can be found on the <a rel="nofollow" class="external text" href="https://github.com/HITS-MCM/gromacs-ramd">GROMACS-RAMD GitHub page</a>.
</p>
<table class="wikitable sortable">

<tbody><tr>
<th>GROMACS</th>
<th>RAMD</th>
<th>modules for running on CPUs</th>
<th>modules for running on GPUs (CUDA)</th>
<th>Notes
</th></tr>
<tr>
<td>v2024.1</td>
<td>2.1</td>
<td><code>StdEnv/2023  gcc/12.3  openmpi/4.1.5  gromacs-ramd/2024.1-RAMD-2.1</code></td>
<td><code>StdEnv/2023  gcc/12.3  openmpi/4.1.5  cuda/12.2  gromacs-ramd/2024.1-RAMD-2.1</code></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr>
<tr>
<td>v2020.5</td>
<td>2.0</td>
<td><code>StdEnv/2020  gcc/9.3.0  openmpi/4.0.3  gromacs-ramd/2020.5-RAMD-2.0</code></td>
<td><code>StdEnv/2020  gcc/9.3.0  cuda/11.4  openmpi/4.0.3  gromacs-ramd/2020.5-RAMD-2.0</code></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr></tbody></table>
<h2><span class="mw-headline" id="GROMACS-SWAXS">GROMACS-SWAXS</span></h2>
<p>GROMACS-SWAXS<sup id="cite_ref-GROMACS-SWAXS_23-0" class="reference"><a href="#cite_note-GROMACS-SWAXS-23"><span class="cite-bracket">&#91;</span>23<span class="cite-bracket">&#93;</span></a></sup>
is a modified version of GROMACS for computing small- and wide-angle X-ray or neutron scattering curves (SAXS/SANS) 
and for doing SAXS/SANS-driven molecular dynamics simulations.
</p><p>Please refer to the <a rel="nofollow" class="external text" href="https://cbjh.gitlab.io/gromacs-swaxs-docs/">GROMACS-SWAXS Documentation</a> for
a description of the features (mdrun input and output options, mpd options, use of <code>gmx genscatt</code>
and <code>gmx genenv</code> commands) that have been added in addition to normal GROMACS features,
and for a number of tutorials.
</p>
<table class="wikitable sortable">

<tbody><tr>
<th>GROMACS</th>
<th>SWAXS</th>
<th>modules for running on CPUs</th>
<th>modules for running on GPUs (CUDA)</th>
<th>Notes
</th></tr>
<tr>
<td>v2021.7</td>
<td>0.5.1</td>
<td><code>StdEnv/2023  gcc/12.3  openmpi/4.1.5  gromacs-swaxs/2021.7-0.5.1</code></td>
<td><code>StdEnv/2023  gcc/12.3  openmpi/4.1.5  cuda/12.2  gromacs-swaxs/2021.7-0.5.1</code></td>
<td>GCC, FlexiBLAS &amp; FFTW
</td></tr></tbody></table>
<h2><span class="mw-headline" id="G_MMPBSA">G_MMPBSA</span></h2>
<p>G_MMPBSA<sup id="cite_ref-g_mmpbsa_24-0" class="reference"><a href="#cite_note-g_mmpbsa-24"><span class="cite-bracket">&#91;</span>24<span class="cite-bracket">&#93;</span></a></sup> is a tool that calculates components of binding energy using MM-PBSA method except the entropic term and energetic contribution of each residue to the binding using energy decomposition scheme.
</p><p>Development of that tool seems to have stalled in April 2016 and no changes have been made since then.  Therefore it is only compatible with Gromacs 5.1.x.
For newer version of GROMACS consider using gmx_MMPBSA<sup id="cite_ref-gmx_mmpbsa_25-0" class="reference"><a href="#cite_note-gmx_mmpbsa-25"><span class="cite-bracket">&#91;</span>25<span class="cite-bracket">&#93;</span></a></sup> instead (see below).
</p><p>The version installed can be loaded with <code>module load  StdEnv/2016.4  gcc/5.4.0  g_mmpbsa/2016-04-19</code> which is the most up-to-date version and consists of version 1.6 plus the change to make it compatible with Gromacs 5.1.x.  The installed version has been compiled with <code>gromacs/5.1.5</code> and <code>apbs/1.3</code>.
</p><p>Please be aware that G_MMPBSA uses implicit solvents and there have been studies<sup id="cite_ref-Zhang2017_26-0" class="reference"><a href="#cite_note-Zhang2017-26"><span class="cite-bracket">&#91;</span>26<span class="cite-bracket">&#93;</span></a></sup> that conclude that there are issues with the accuracy of these methods for calculating binding free energies.
</p>
<h2><span class="mw-headline" id="gmx_MMPBSA">gmx_MMPBSA</span></h2>
<p>gmx_MMPBSA<sup id="cite_ref-gmx_mmpbsa_25-1" class="reference"><a href="#cite_note-gmx_mmpbsa-25"><span class="cite-bracket">&#91;</span>25<span class="cite-bracket">&#93;</span></a></sup>
is a tool based on <a href="/wiki/AMBER" title="AMBER">AMBER</a>'s MMPBSA.py aiming to perform end-state free energy calculations with GROMACS files.
</p><p>Other than the older G_MMPBSA<sup id="cite_ref-g_mmpbsa_24-1" class="reference"><a href="#cite_note-g_mmpbsa-24"><span class="cite-bracket">&#91;</span>24<span class="cite-bracket">&#93;</span></a></sup>, which is only compatible with older versions of GROMACS,
gmx_MMPBSA can be used with current versions of GROMACS and <a href="/wiki/AMBER#AmberTools_21" title="AMBER">AmberTools</a>.
</p><p>Please be aware that gmx_MMPBSA uses implicit solvents and there have been studies<sup id="cite_ref-Zhang2017_26-1" class="reference"><a href="#cite_note-Zhang2017-26"><span class="cite-bracket">&#91;</span>26<span class="cite-bracket">&#93;</span></a></sup> that conclude that there are issues with the accuracy of these methods for calculating binding free energies.
</p>
<h3><span class="mw-headline" id="Submission_scripts_2">Submission scripts</span></h3>
<p>This submission script installs and executes gmx_MMPBSA in a temporary directory on the local disk of a compute node. All MPI tasks must be on one node. For multi-node submission install a virtual environment on a shared filesystem.
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> job_gmx_MMPBSA.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--ntasks%3D5+%0A%23SBATCH+--nodes%3D1%0A%23SBATCH+--mem-per-cpu%3D4000M+%0A%23SBATCH+--time%3D3%3A0%3A0+%0A+%0Amodule+load+StdEnv%2F2023+ambertools%2F23.5+gromacs%2F2024.4+%0Avirtualenv+%24SLURM_TMPDIR%2Fvenv-gmxMMPBSA%0Asource+%24SLURM_TMPDIR%2Fvenv-gmxMMPBSA%2Fbin%2Factivate%0Apip+install+--no-index+gmx_MMPBSA%3D%3D1.6.3%0A+%0Asrun+gmx_MMPBSA+-O+-nogui+-i+mmpbsa.in+%5C%0A++++-cs+com.tpr+%5C%0A++++-ct+com_traj.xtc+%5C%0A++++-ci+index.ndx+%5C%0A++++-cg+3+4+%5C%0A++++-cp+topol.top+%5C%0A++++-o+FINAL_RESULTS_MMPBSA.dat+%5C%0A++++-eo+FINAL_RESULTS_MMPBSA.csv" />
<input type="hidden" name="filename" value="job_gmx_MMPBSA.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-sh mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --ntasks=5 </span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --mem-per-cpu=4000M </span>
<span class="c1">#SBATCH --time=3:0:0 </span>
<span class="w"> </span>
module<span class="w"> </span>load<span class="w"> </span>StdEnv/2023<span class="w"> </span>ambertools/23.5<span class="w"> </span>gromacs/2024.4<span class="w"> </span>
virtualenv<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/venv-gmxMMPBSA
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/venv-gmxMMPBSA/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>--no-index<span class="w"> </span><span class="nv">gmx_MMPBSA</span><span class="o">==</span><span class="m">1</span>.6.3
<span class="w"> </span>
srun<span class="w"> </span>gmx_MMPBSA<span class="w"> </span>-O<span class="w"> </span>-nogui<span class="w"> </span>-i<span class="w"> </span>mmpbsa.in<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-cs<span class="w"> </span>com.tpr<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-ct<span class="w"> </span>com_traj.xtc<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-ci<span class="w"> </span>index.ndx<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-cg<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-cp<span class="w"> </span>topol.top<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>FINAL_RESULTS_MMPBSA.dat<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-eo<span class="w"> </span>FINAL_RESULTS_MMPBSA.csv
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<h3><span class="mw-headline" id="Installing_gmx_MMPBSA_into_a_virtualenv">Installing gmx_MMPBSA into a virtualenv</span></h3>
<p>gmx_MMPBSA needs to be installed in a permanent directory if you intend to use interactive visualization.
</p>
<h4><span id="Installing_for_gromacs.2F2024_.28StdEnv.2F2023.29"></span><span class="mw-headline" id="Installing_for_gromacs/2024_(StdEnv/2023)">Installing for gromacs/2024 (StdEnv/2023)</span></h4>
<div class="command">
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>module<span class="w"> </span>load<span class="w">  </span>StdEnv/2023<span class="w">  </span>ambertools/23.5<span class="w">  </span>gromacs/2024.4<span class="w">  </span>qt/5.15.11
<span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>virtualenv<span class="w"> </span>venv-gmxMMPBSA
<span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span><span class="nb">source</span><span class="w"> </span>venv-gmxMMPBSA/bin/activate
<span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--no-index<span class="w"> </span><span class="nv">gmx_MMPBSA</span><span class="o">==</span><span class="m">1</span>.6.3
</pre></div></div>
<p><br />
Testing.
</p>
<div class="command">
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/Valdes-Tresanco-MS/gmx_MMPBSA
<span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span><span class="nb">cd</span><span class="w"> </span>gmx_MMPBSA/examples/Protein_DNA
<span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>gmx_MMPBSA<span class="w"> </span>-O<span class="w"> </span>-i<span class="w"> </span>mmpbsa.in<span class="w"> </span>-cs<span class="w"> </span>com.tpr<span class="w"> </span>-ct<span class="w"> </span>com_traj.xtc<span class="w"> </span>-ci<span class="w"> </span>index.ndx<span class="w"> </span>-cg<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">4</span><span class="w"> </span>-cp<span class="w"> </span>topol.top<span class="w"> </span><span class="se">\ </span>
<span class="w">                            </span>-o<span class="w"> </span>FINAL_RESULTS_MMPBSA.dat<span class="w"> </span>-eo<span class="w"> </span>FINAL_RESULTS_MMPBSA.csv<span class="w"> </span>-no<span class="w"> </span>-nogui
</pre></div></div>
<p><br />
</p>
<h4><span id="Installing_for_gromacs.2F2021_.28StdEnv.2F2020.29"></span><span class="mw-headline" id="Installing_for_gromacs/2021_(StdEnv/2020)">Installing for gromacs/2021 (StdEnv/2020)</span></h4>
<p>1. Load required modules and create the virtualenv
</p>
<div class="command">
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>module<span class="w"> </span>purge
<span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>module<span class="w"> </span>load<span class="w"> </span>StdEnv/2020<span class="w"> </span>gcc/9.3.0<span class="w"> </span>python/3.8<span class="w">  </span>gromacs/2021.4
<span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>module<span class="w"> </span>load<span class="w"> </span>ambertools/21
<span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>virtualenv<span class="w"> </span>venv_gmxMMPBSA
<span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span><span class="nb">source</span><span class="w"> </span>venv_gmxMMPBSA/bin/activate
<span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>pip
<span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--no-index<span class="w"> </span><span class="nv">numpy</span><span class="o">==</span><span class="m">1</span>.22.2<span class="w"> </span><span class="nv">seaborn</span><span class="o">==</span><span class="m">0</span>.13.1<span class="w"> </span><span class="nv">gmx_MMPBSA</span><span class="o">==</span><span class="m">1</span>.5.0.3<span class="w"> </span><span class="nv">ParmEd</span><span class="o">==</span><span class="m">3</span>.4.4
</pre></div></div>
<p><br />
2. The Qt/PyQt module needs to be loaded after the virtualenv is ready:
</p>
<div class="command">
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>module<span class="w"> </span>load<span class="w"> </span>qt/5.15.2
</pre></div></div>
<p><br />
3. Test if the main application works:
</p>
<div class="command">
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>gmx_MMPBSA<span class="w"> </span>-h
<span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>gmx_MMPBSA_test<span class="w"> </span>-ng<span class="w"> </span>-n<span class="w"> </span><span class="m">4</span>
</pre></div></div>
<p><br />
Fortunately, running the self-test is very quick, therefore it's permissible to run them on the login node.
</p><p>Later when using gmx_MMPBSA in a job you need to load the modules and activate the virtualenv as follows:
</p>
<div class="command">
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>module<span class="w"> </span>purge
<span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>module<span class="w"> </span>load<span class="w"> </span>StdEnv/2020<span class="w"> </span>gcc/9.3.0<span class="w"> </span>python/3.8<span class="w"> </span>ambertools/21<span class="w"> </span>gromacs/2021.4<span class="w"> </span>qt/5.15.2
<span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span><span class="nb">source</span><span class="w"> </span>venv_gmxMMPBSA/bin/activate
</pre></div></div>
<p><br />
</p>
<h1><span class="mw-headline" id="Links">Links</span></h1>
<p><a href="/wiki/Biomolecular_simulation" title="Biomolecular simulation">Biomolecular simulation</a>
</p>
<ul><li>Project resources
<ul><li>Main Website: <a rel="nofollow" class="external free" href="http://www.gromacs.org/">http://www.gromacs.org/</a></li>
<li>Documentation &amp; GROMACS Manuals: <a rel="nofollow" class="external free" href="http://manual.gromacs.org/documentation/">http://manual.gromacs.org/documentation/</a></li>
<li>GROMACS Community Forums: <a rel="nofollow" class="external free" href="https://gromacs.bioexcel.eu/">https://gromacs.bioexcel.eu/</a> <br />The forums are the successors to the GROMACS email lists.</li></ul></li>
<li>Tutorials
<ul><li>Set of 7 very good Tutorials: <a rel="nofollow" class="external free" href="http://www.mdtutorials.com/gmx/">http://www.mdtutorials.com/gmx/</a></li>
<li>Link collection to more tutorials: <a rel="nofollow" class="external free" href="http://www.gromacs.org/Documentation/Tutorials">http://www.gromacs.org/Documentation/Tutorials</a></li></ul></li>
<li>External resources
<ul><li>Tool to generate small molecule topology files: <a rel="nofollow" class="external free" href="http://www.ccpn.ac.uk/v2-software/software/ACPYPE-folder">http://www.ccpn.ac.uk/v2-software/software/ACPYPE-folder</a></li>
<li>Database with Force Field topologies (CGenFF, GAFF and OPLS/AA) for small molecules: <a rel="nofollow" class="external free" href="http://www.virtualchemistry.org/">http://www.virtualchemistry.org/</a></li>
<li>Web service to generate small molecule topologies for GROMOS force fields: <a rel="nofollow" class="external free" href="https://atb.uq.edu.au/">https://atb.uq.edu.au/</a></li>
<li>Discussion of best GPU configurations for running GROMACS: <a rel="nofollow" class="external text" href="https://arxiv.org/abs/1507.00898">Best bang for your buck: GPU nodes for GROMACS biomolecular simulations</a></li></ul></li></ul>
<h1><span class="mw-headline" id="References">References</span></h1>
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><a href="#cite_ref-1">â†‘</a></span> <span class="reference-text">"Fix missing synchronization in CUDA update kernels" in GROMACS 2021.6 Release Notes <a rel="nofollow" class="external autonumber" href="https://manual.gromacs.org/2021.6/release-notes/2021/2021.6.html#fix-missing-synchronization-in-cuda-update-kernels">[1]</a></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><a href="#cite_ref-2">â†‘</a></span> <span class="reference-text">Issue #4393 in GROMACS Project on GitLab.com <a rel="nofollow" class="external autonumber" href="https://gitlab.com/gromacs/gromacs/-/issues/4393">[2]</a></span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><a href="#cite_ref-3">â†‘</a></span> <span class="reference-text">"Fix missing synchronization in CUDA update kernels" in GROMACS 2021.6 Release Notes <a rel="nofollow" class="external autonumber" href="https://manual.gromacs.org/2021.6/release-notes/2021/2021.6.html#fix-missing-synchronization-in-cuda-update-kernels">[3]</a></span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><a href="#cite_ref-4">â†‘</a></span> <span class="reference-text">Issue #4393 in GROMACS Project on GitLab.com <a rel="nofollow" class="external autonumber" href="https://gitlab.com/gromacs/gromacs/-/issues/4393">[4]</a></span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><a href="#cite_ref-5">â†‘</a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://manual.gromacs.org/documentation/current/user-guide/managing-simulations.html">GROMACS User Guide: Managing long simulations.</a></span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><a href="#cite_ref-6">â†‘</a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://manual.gromacs.org/documentation/current/onlinehelp/gmx-mdrun.html#gmx-mdrun">GROMACS Manual page: gmx mdrun</a></span>
</li>
<li id="cite_note-performance-7"><span class="mw-cite-backlink"><a href="#cite_ref-performance_7-0">â†‘</a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://manual.gromacs.org/documentation/current/user-guide/mdrun-performance.html">GROMACS User-Guide: Getting good performance from mdrun</a></span>
</li>
<li id="cite_note-perf-background-8"><span class="mw-cite-backlink">â†‘ <sup><a href="#cite_ref-perf-background_8-0">8.0</a></sup> <sup><a href="#cite_ref-perf-background_8-1">8.1</a></sup></span> <span class="reference-text"> <a rel="nofollow" class="external text" href="http://manual.gromacs.org/documentation/current/user-guide/mdrun-performance.html#gromacs-background-information">GROMACS User-Guide:  Performance background information</a></span>
</li>
<li id="cite_note-gpu-pme-2018.1-9"><span class="mw-cite-backlink"><a href="#cite_ref-gpu-pme-2018.1_9-0">â†‘</a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://manual.gromacs.org/documentation/2018.1/user-guide/mdrun-performance.html#gpu-accelerated-calculation-of-pme">GROMACS User-Guide: GPU accelerated calculation of PME</a></span>
</li>
<li id="cite_note-PLUMED-10"><span class="mw-cite-backlink"><a href="#cite_ref-PLUMED_10-0">â†‘</a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://www.plumed.org/home">PLUMED Home</a></span>
</li>
<li id="cite_note-Colvars-11"><span class="mw-cite-backlink"><a href="#cite_ref-Colvars_11-0">â†‘</a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://colvars.github.io/">Colvars Home</a></span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><a href="#cite_ref-12">â†‘</a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://manual.gromacs.org/2024.1/release-notes/2024/major/highlights.html">GROMACS 2024 Major Release Highlights</a></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><a href="#cite_ref-13">â†‘</a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://manual.gromacs.org/current/reference-manual/special/colvars.html">Collective Variable simulations with the Colvars module (GROMACS Reference manual)</a></span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><a href="#cite_ref-14">â†‘</a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://manual.gromacs.org/current/user-guide/mdp-options.html#collective-variables-colvars-module">Colvars .mdp Options (GROMACS User guide)</a></span>
</li>
<li id="cite_note-colvars-gromacs-15"><span class="mw-cite-backlink"><a href="#cite_ref-colvars-gromacs_15-0">â†‘</a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://colvars.github.io/colvars-refman-gromacs/colvars-refman-gromacs.html">Colvars Reference manual for GROMACS</a></span>
</li>
<li id="cite_note-colvars-paper-16"><span class="mw-cite-backlink"><a href="#cite_ref-colvars-paper_16-0">â†‘</a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://dx.doi.org/10.1080/00268976.2013.813594">Fiorin et al. <b>2013</b>, <i>Using collective variables to drive molecular dynamics simulations.</i></a></span>
</li>
<li id="cite_note-cp2k-17"><span class="mw-cite-backlink"><a href="#cite_ref-cp2k_17-0">â†‘</a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://www.cp2k.org/">CP2K Home</a></span>
</li>
<li id="cite_note-gromacs-cp2k-18"><span class="mw-cite-backlink"><a href="#cite_ref-gromacs-cp2k_18-0">â†‘</a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://manual.gromacs.org/documentation/current/install-guide/index.html#building-with-cp2k-qm-mm-support">Building GROMACS with CP2K  QM/MM support</a></span>
</li>
<li id="cite_note-gromacs-cp2k-qmmm-ref-19"><span class="mw-cite-backlink"><a href="#cite_ref-gromacs-cp2k-qmmm-ref_19-0">â†‘</a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://manual.gromacs.org/documentation/current/reference-manual/special/qmmm.html">QM/MM with CP2K in the GROMACS Reference manual</a></span>
</li>
<li id="cite_note-Local_stress-20"><span class="mw-cite-backlink"><a href="#cite_ref-Local_stress_20-0">â†‘</a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://vanegaslab.org/software">GROMACS-LS and MDStress library</a></span>
</li>
<li id="cite_note-RAMD-21"><span class="mw-cite-backlink"><a href="#cite_ref-RAMD_21-0">â†‘</a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://kbbox.h-its.org/toolbox/methods/molecular-simulation/random-acceleration-molecular-dynamics-ramd/">Information on the RAMD method</a></span>
</li>
<li id="cite_note-22"><span class="mw-cite-backlink"><a href="#cite_ref-22">â†‘</a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://github.com/HITS-MCM/gromacs-ramd#usage">RAMD-specific MDP options</a></span>
</li>
<li id="cite_note-GROMACS-SWAXS-23"><span class="mw-cite-backlink"><a href="#cite_ref-GROMACS-SWAXS_23-0">â†‘</a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://biophys.uni-saarland.de/software/gromacs-swaxs/">GROMACS-SWAXS Home</a></span>
</li>
<li id="cite_note-g_mmpbsa-24"><span class="mw-cite-backlink">â†‘ <sup><a href="#cite_ref-g_mmpbsa_24-0">24.0</a></sup> <sup><a href="#cite_ref-g_mmpbsa_24-1">24.1</a></sup></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://rashmikumari.github.io/g_mmpbsa/">G_MMPBSA Homepage</a></span>
</li>
<li id="cite_note-gmx_mmpbsa-25"><span class="mw-cite-backlink">â†‘ <sup><a href="#cite_ref-gmx_mmpbsa_25-0">25.0</a></sup> <sup><a href="#cite_ref-gmx_mmpbsa_25-1">25.1</a></sup></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://valdes-tresanco-ms.github.io/gmx_MMPBSA/dev/getting-started/">gmx_MMPBSA Homepage</a></span>
</li>
<li id="cite_note-Zhang2017-26"><span class="mw-cite-backlink">â†‘ <sup><a href="#cite_ref-Zhang2017_26-0">26.0</a></sup> <sup><a href="#cite_ref-Zhang2017_26-1">26.1</a></sup></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://pubs.acs.org/doi/abs/10.1021/acs.jctc.7b00169">Comparison of Implicit and Explicit Solvent Models for the Calculation of Solvation Free Energy in Organic Solvents</a></span>
</li>
</ol></div>
<!-- 
NewPP limit report
Cached time: 20250530235415
Cache expiry: 86400
Reduced expiry: false
Complications: [showâ€toc]
CPU time usage: 0.189 seconds
Real time usage: 2.196 seconds
Preprocessor visited node count: 1936/1000000
Postâ€expand include size: 20267/2097152 bytes
Template argument size: 18146/2097152 bytes
Highest expansion depth: 9/100
Expensive parser function count: 20/100
Unstrip recursion depth: 3/20
Unstrip postâ€expand size: 82211/5000000 bytes
ExtLoops count: 0/100
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00% 2082.622      1 -total
 69.24% 1441.951     14 Template:File
 29.62%  616.776      6 Template:Commands
  0.05%    1.132      3 Template:Warning
  0.02%    0.515      3 Template:Panel
-->

<!-- Saved in parser cache with key ccwiki:pcache:idhash:1881-0!canonical and timestamp 20250530235415 and revision id 176552. Rendering was triggered because: page-view
 -->
</div>
<div class="printfooter" data-nosnippet="">Retrieved from "<a dir="ltr" href="https://docs.alliancecan.ca/mediawiki/index.php?title=GROMACS&amp;oldid=176552">https://docs.alliancecan.ca/mediawiki/index.php?title=GROMACS&amp;oldid=176552</a>"</div></div>
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>: <ul><li><a href="/wiki/Category:Software" title="Category:Software">Software</a></li><li><a href="/wiki/Category:BiomolecularSimulation" title="Category:BiomolecularSimulation">BiomolecularSimulation</a></li></ul></div></div>
	</div>
</div>

<div id="mw-navigation">
	<h2>Navigation menu</h2>
	<div id="mw-head">
		
<nav id="p-personal" class="mw-portlet mw-portlet-personal vector-user-menu-legacy vector-menu" aria-labelledby="p-personal-label"  >
	<h3
		id="p-personal-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Personal tools</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="pt-uls" class="mw-list-item active"><a class="uls-trigger" href="#"><span>English</span></a></li><li id="pt-login" class="mw-list-item"><a href="/mediawiki/index.php?title=Special:UserLogin&amp;returnto=GROMACS" title="You are encouraged to log in; however, it is not mandatory [o]" accesskey="o"><span>Log in</span></a></li>
		</ul>
		
	</div>
</nav>

		<div id="left-navigation">
			
<nav id="p-namespaces" class="mw-portlet mw-portlet-namespaces vector-menu-tabs vector-menu-tabs-legacy vector-menu" aria-labelledby="p-namespaces-label"  >
	<h3
		id="p-namespaces-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Namespaces</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-nstab-main" class="selected mw-list-item"><a href="/wiki/GROMACS" title="View the content page [c]" accesskey="c"><span>Page</span></a></li><li id="ca-talk" class="mw-list-item"><a href="/wiki/Talk:GROMACS" rel="discussion" title="Discussion about the content page [t]" accesskey="t"><span>Discussion</span></a></li>
		</ul>
		
	</div>
</nav>

			
<nav id="p-variants" class="mw-portlet mw-portlet-variants emptyPortlet vector-menu-dropdown vector-menu" aria-labelledby="p-variants-label"  >
	<input type="checkbox"
		id="p-variants-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-p-variants"
		class="vector-menu-checkbox"
		aria-labelledby="p-variants-label"
	>
	<label
		id="p-variants-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">English</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</nav>

		</div>
		<div id="right-navigation">
			
<nav id="p-views" class="mw-portlet mw-portlet-views vector-menu-tabs vector-menu-tabs-legacy vector-menu" aria-labelledby="p-views-label"  >
	<h3
		id="p-views-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Views</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-view" class="selected mw-list-item"><a href="/wiki/GROMACS"><span>Read</span></a></li><li id="ca-viewsource" class="mw-list-item"><a href="/mediawiki/index.php?title=GROMACS&amp;action=edit" title="This page is protected.&#10;You can view its source [e]" accesskey="e"><span>View source</span></a></li><li id="ca-history" class="mw-list-item"><a href="/mediawiki/index.php?title=GROMACS&amp;action=history" title="Past revisions of this page [h]" accesskey="h"><span>View history</span></a></li>
		</ul>
		
	</div>
</nav>

			
<nav id="p-cactions" class="mw-portlet mw-portlet-cactions emptyPortlet vector-menu-dropdown vector-menu" aria-labelledby="p-cactions-label"  title="More options" >
	<input type="checkbox"
		id="p-cactions-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-p-cactions"
		class="vector-menu-checkbox"
		aria-labelledby="p-cactions-label"
	>
	<label
		id="p-cactions-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">More</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</nav>

			
<div id="p-search" role="search" class="vector-search-box-vue  vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box">
	<h3 >Search</h3>
	<form action="/mediawiki/index.php" id="searchform" class="vector-search-box-form">
		<div id="simpleSearch"
			class="vector-search-box-inner"
			 data-search-loc="header-navigation">
			<input class="vector-search-box-input"
				 type="search" name="search" placeholder="Search Alliance Doc" aria-label="Search Alliance Doc" autocapitalize="sentences" title="Search Alliance Doc [f]" accesskey="f" id="searchInput"
			>
			<input type="hidden" name="title" value="Special:Search">
			<input id="mw-searchButton"
				 class="searchButton mw-fallbackSearchButton" type="submit" name="fulltext" title="Search the pages for this text" value="Search">
			<input id="searchButton"
				 class="searchButton" type="submit" name="go" title="Go to a page with this exact name if it exists" value="Go">
		</div>
	</form>
</div>

		</div>
	</div>
	
<div id="mw-panel" class="vector-legacy-sidebar">
	<div id="p-logo" role="banner">
		<a class="mw-wiki-logo" href="/wiki/Technical_documentation"
			title="Visit the main page"></a>
	</div>
	
<nav id="p-navigation" class="mw-portlet mw-portlet-navigation vector-menu-portal portal vector-menu" aria-labelledby="p-navigation-label"  >
	<h3
		id="p-navigation-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Navigation</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-sidebar-wiki-main-page" class="mw-list-item"><a href="/wiki/Technical_documentation"><span>Wiki Main Page</span></a></li>
		</ul>
		
	</div>
</nav>

	
<nav id="p-sidebar-support" class="mw-portlet mw-portlet-sidebar-support vector-menu-portal portal vector-menu" aria-labelledby="p-sidebar-support-label"  >
	<h3
		id="p-sidebar-support-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Support</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-sidebar-getting-started" class="mw-list-item"><a href="/wiki/Getting_started"><span>Getting started</span></a></li><li id="n-sidebar-technical-support" class="mw-list-item"><a href="/wiki/Technical_support"><span>Getting help</span></a></li><li id="n-sidebar-running-jobs" class="mw-list-item"><a href="/wiki/Running_jobs"><span>Running jobs</span></a></li><li id="n-sidebar-known-issues" class="mw-list-item"><a href="/wiki/Known_issues"><span>Known issues</span></a></li><li id="n-sidebar-system-status" class="mw-list-item"><a href="http://status.computecanada.ca" rel="nofollow"><span>System status</span></a></li>
		</ul>
		
	</div>
</nav>

<nav id="p-sidebar-resources" class="mw-portlet mw-portlet-sidebar-resources vector-menu-portal portal vector-menu" aria-labelledby="p-sidebar-resources-label"  >
	<h3
		id="p-sidebar-resources-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Resources</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-BÃ©luga" class="mw-list-item"><a href="/wiki/B%C3%A9luga/en"><span>BÃ©luga</span></a></li><li id="n-Cedar" class="mw-list-item"><a href="/wiki/Cedar"><span>Cedar</span></a></li><li id="n-Graham" class="mw-list-item"><a href="/wiki/Graham"><span>Graham</span></a></li><li id="n-Narval" class="mw-list-item"><a href="/wiki/Narval/en"><span>Narval</span></a></li><li id="n-Niagara" class="mw-list-item"><a href="/wiki/Niagara"><span>Niagara</span></a></li><li id="n-sidebar-cloud" class="mw-list-item"><a href="/wiki/CC-Cloud"><span>Cloud</span></a></li><li id="n-tamIA" class="mw-list-item"><a href="/wiki/TamIA/en"><span>tamIA</span></a></li><li id="n-Killarney" class="mw-list-item"><a href="/wiki/Killarney"><span>Killarney</span></a></li><li id="n-sidebar-quantum-computing" class="mw-list-item"><a href="/wiki/Services_d%27informatique_quantique/en"><span>Quantum computing</span></a></li><li id="n-sidebar-available-software" class="mw-list-item"><a href="/wiki/Available_software"><span>Available software</span></a></li>
		</ul>
		
	</div>
</nav>

<nav id="p-sidebar-alliance" class="mw-portlet mw-portlet-sidebar-alliance vector-menu-portal portal vector-menu" aria-labelledby="p-sidebar-alliance-label"  >
	<h3
		id="p-sidebar-alliance-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">The Alliance</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-sidebar-alliance-main-page" class="mw-list-item"><a href="https://alliancecan.ca/en" rel="nofollow"><span>Alliance main page</span></a></li><li id="n-sidebar-ccdb" class="mw-list-item"><a href="https://ccdb.computecanada.ca/security/login" rel="nofollow"><span>CCDB</span></a></li><li id="n-sidebar-getting-an-account" class="mw-list-item"><a href="https://alliancecan.ca/en/services/advanced-research-computing/account-management/apply-account" rel="nofollow"><span>Getting An Account</span></a></li><li id="n-sidebar-acknowledging-alliance" class="mw-list-item"><a href="https://alliancecan.ca/en/services/advanced-research-computing/research-portal/acknowledging-alliance" rel="nofollow"><span>Acknowledging the Alliance</span></a></li><li id="n-sidebar-aup" class="mw-list-item"><a href="https://alliancecan.ca/en/services/advanced-research-computing/account-management/policies" rel="nofollow"><span>Acceptable Use Policy</span></a></li>
		</ul>
		
	</div>
</nav>

<nav id="p-sidebar-authoring" class="mw-portlet mw-portlet-sidebar-authoring vector-menu-portal portal vector-menu" aria-labelledby="p-sidebar-authoring-label"  >
	<h3
		id="p-sidebar-authoring-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Authoring</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-sidebar-guidelines" class="mw-list-item"><a href="/wiki/Authoring_guidelines"><span>Guidelines</span></a></li><li id="n-sidebar-mediawiki-help" class="mw-list-item"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/Help:Contents"><span>MediaWiki Help</span></a></li><li id="n-recentchanges" class="mw-list-item"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r"><span>Recent changes</span></a></li>
		</ul>
		
	</div>
</nav>

<nav id="p-tb" class="mw-portlet mw-portlet-tb vector-menu-portal portal vector-menu" aria-labelledby="p-tb-label"  >
	<h3
		id="p-tb-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Tools</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="t-whatlinkshere" class="mw-list-item"><a href="/wiki/Special:WhatLinksHere/GROMACS" title="A list of all wiki pages that link here [j]" accesskey="j"><span>What links here</span></a></li><li id="t-recentchangeslinked" class="mw-list-item"><a href="/wiki/Special:RecentChangesLinked/GROMACS" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k"><span>Related changes</span></a></li><li id="t-specialpages" class="mw-list-item"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q"><span>Special pages</span></a></li><li id="t-print" class="mw-list-item"><a href="javascript:print();" rel="alternate" title="Printable version of this page [p]" accesskey="p"><span>Printable version</span></a></li><li id="t-permalink" class="mw-list-item"><a href="/mediawiki/index.php?title=GROMACS&amp;oldid=176552" title="Permanent link to this revision of this page"><span>Permanent link</span></a></li><li id="t-info" class="mw-list-item"><a href="/mediawiki/index.php?title=GROMACS&amp;action=info" title="More information about this page"><span>Page information</span></a></li>
		</ul>
		
	</div>
</nav>

	
</div>

</div>

<footer id="footer" class="mw-footer" >
	<ul id="footer-info">
	<li id="footer-info-lastmod"> This page was last edited on 25 April 2025, at 07:29.</li>
</ul>

	<ul id="footer-places">
	<li id="footer-places-privacy"><a href="/wiki/CCWiki:Privacy_policy">Privacy policy</a></li>
	<li id="footer-places-about"><a href="/wiki/CCWiki:About">About Alliance Doc</a></li>
	<li id="footer-places-disclaimers"><a href="/wiki/CCWiki:General_disclaimer">Disclaimers</a></li>
	<li id="footer-places-mobileview"><a href="https://docs.alliancecan.ca/mediawiki/index.php?title=GROMACS&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
</ul>

	<ul id="footer-icons" class="noprint">
	<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/" class="cdx-button cdx-button--fake-button cdx-button--size-large cdx-button--fake-button--enabled"><img src="/mediawiki/resources/assets/poweredby_mediawiki.svg" alt="Powered by MediaWiki" width="88" height="31" loading="lazy"></a></li>
</ul>

</footer>

<script>(RLQ=window.RLQ||[]).push(function(){mw.log.warn("This page is using the deprecated ResourceLoader module \"codex-search-styles\".\n[1.43] Use a CodexModule with codexComponents to set your specific components used: https://www.mediawiki.org/wiki/Codex#Using_a_limited_subset_of_components");});</script>
<script src="https://www.googletagmanager.com/gtag/js?id=G-TVBPRD78K4" async=""></script><script>
window.dataLayer = window.dataLayer || [];

function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-TVBPRD78K4', {});
</script>

<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":2305,"wgPageParseReport":{"limitreport":{"cputime":"0.189","walltime":"2.196","ppvisitednodes":{"value":1936,"limit":1000000},"postexpandincludesize":{"value":20267,"limit":2097152},"templateargumentsize":{"value":18146,"limit":2097152},"expansiondepth":{"value":9,"limit":100},"expensivefunctioncount":{"value":20,"limit":100},"unstrip-depth":{"value":3,"limit":20},"unstrip-size":{"value":82211,"limit":5000000},"timingprofile":["100.00% 2082.622      1 -total"," 69.24% 1441.951     14 Template:File"," 29.62%  616.776      6 Template:Commands","  0.05%    1.132      3 Template:Warning","  0.02%    0.515      3 Template:Panel"]},"loops":{"limitreport-count-limited":{"value":0,"limit":100}},"cachereport":{"timestamp":"20250530235415","ttl":86400,"transientcontent":false}}});});</script>
</body>
</html>