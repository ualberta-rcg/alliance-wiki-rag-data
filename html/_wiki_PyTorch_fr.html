<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<title>PyTorch - Alliance Doc</title>
<script>(function(){var className="client-js";var cookie=document.cookie.match(/(?:^|; )ccwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split('%2C').forEach(function(pref){className=className.replace(new RegExp('(^| )'+pref.replace(/-clientpref-\w+$|[^\w-]+/g,'')+'-clientpref-\\w+( |$)'),'$1'+pref+'$2');});}document.documentElement.className=className;}());RLCONF={"wgBreakFrames":false,"wgSeparatorTransformTable":[",\t.","Â \t,"],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","janvier","fÃ©vrier","mars","avril","mai","juin","juillet","aoÃ»t","septembre","octobre","novembre","dÃ©cembre"],"wgRequestId":"aDpEzI34nW-BQYizeTZIRwAAA1Y","wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"PyTorch/fr","wgTitle":"PyTorch/fr","wgCurRevisionId":175858,"wgRevisionId":175858,"wgArticleId":4699,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":[
"Pages with syntax highlighting errors","Software","AI and Machine Learning"],"wgPageViewLanguage":"fr","wgPageContentLanguage":"fr","wgPageContentModel":"wikitext","wgRelevantPageName":"PyTorch/fr","wgRelevantArticleId":4699,"wgIsProbablyEditable":false,"wgRelevantPageIsProbablyEditable":false,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgULSAcceptLanguageList":[],"wgMFDisplayWikibaseDescriptions":{"search":false,"watchlist":false,"tagline":false},"wgCiteReferencePreviewsActive":true,"wgTranslatePageTranslation":"translation","wgULSPosition":"personal","wgULSisCompactLinksEnabled":true,"wgVector2022LanguageInHeader":false,"wgULSisLanguageSelectorEmpty":false};RLSTATE={"site.styles":"ready","user.styles":"ready","user":"ready","user.options":"loading","ext.translate.tag.languages":"ready","ext.pygments":"ready","skins.vector.styles.legacy":"ready","ext.translate":"ready","codex-search-styles":"ready","ext.uls.pt":"ready"};RLPAGEMODULES=["ext.pygments.view","ext.tabs","site",
"mediawiki.page.ready","mediawiki.toc","skins.vector.legacy.js","ext.languageSelector","ext.translate.pagetranslation.uls","ext.uls.compactlinks","ext.uls.geoclient","ext.uls.interface","ext.moderation.notify","ext.moderation.notify.desktop"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.impl(function(){return["user.options@12s5i",function($,jQuery,require,module){mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
}];});});</script>
<link rel="stylesheet" href="/mediawiki/load.php?lang=en&amp;modules=codex-search-styles%7Cext.pygments%2Ctranslate%7Cext.translate.tag.languages%7Cext.uls.pt%7Cskins.vector.styles.legacy&amp;only=styles&amp;skin=vector">
<script async="" src="/mediawiki/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content="">
<link rel="stylesheet" href="/mediawiki/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector">
<meta name="generator" content="MediaWiki 1.43.0">
<meta name="robots" content="max-image-preview:standard">
<meta name="format-detection" content="telephone=no">
<meta name="viewport" content="width=1120">
<link rel="icon" href="/mediawiki/resources/assets/Alliance_favicon.png">
<link rel="search" type="application/opensearchdescription+xml" href="/mediawiki/rest.php/v1/search" title="Alliance Doc (en)">
<link rel="EditURI" type="application/rsd+xml" href="https://docs.alliancecan.ca/mediawiki/api.php?action=rsd">
<link rel="alternate" type="application/atom+xml" title="Alliance Doc Atom feed" href="/mediawiki/index.php?title=Special:RecentChanges&amp;feed=atom">
<style type="text/css" id="tabs-dynamic-styles">/*<![CDATA[*/
/* Dynamically generated tabs styles */
.tabs-input-1:checked ~ .tabs-container .tabs-content-1,
.tabs-input-2:checked ~ .tabs-container .tabs-content-2,
.tabs-input-0:checked ~ .tabs-container .tabs-content-1 {display:inline-block;}
.tabs-input-1:checked ~ .tabs-container .tabs-inline.tabs-content-1,
.tabs-input-2:checked ~ .tabs-container .tabs-inline.tabs-content-2,
.tabs-input-0:checked ~ .tabs-container .tabs-inline.tabs-content-1 {display:inline;}
.tabs-input-1:checked ~ .tabs-container .tabs-block.tabs-content-1,
.tabs-input-2:checked ~ .tabs-container .tabs-block.tabs-content-2,
.tabs-input-0:checked ~ .tabs-container .tabs-block.tabs-content-1 {display:block;}
/* The same styles, but with .checked instead of :checked, for browsers that rely on the JavaScript fallback */
.tabs-input-1.checked ~ .tabs-container .tabs-content-1,
.tabs-input-2.checked ~ .tabs-container .tabs-content-2,
.tabs-input-0.checked ~ .tabs-container .tabs-content-1 {display:inline-block;}
.tabs-input-1.checked ~ .tabs-container .tabs-inline.tabs-content-1,
.tabs-input-2.checked ~ .tabs-container .tabs-inline.tabs-content-2,
.tabs-input-0.checked ~ .tabs-container .tabs-inline.tabs-content-1 {display:inline;}
.tabs-input-1.checked ~ .tabs-container .tabs-block.tabs-content-1,
.tabs-input-2.checked ~ .tabs-container .tabs-block.tabs-content-2,
.tabs-input-0.checked ~ .tabs-container .tabs-block.tabs-content-1 {display:block;}
.tabs-dropdown .tabs-content,.tabs-dropdown .tabs-container,.tabs-dropdown li,.tabs-dropdown ul,.tabs-dropdown ol {background-color: white /* Malicious data in tabs-dropdown-bgcolor */}
/*]]>*/</style>
</head>
<body class="skin-vector-legacy mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-PyTorch_fr rootpage-PyTorch_fr skin-vector action-view"><div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice"></div>
	<div class="mw-indicators">
	<div id="mw-indicator-languageselector" class="mw-indicator"><span id="languageselector-box-1" class="languageselector " style=""><form name="languageselector-form-1" id="languageselector-form-1" method="get" action="/mediawiki/index.php" style="display:inline;"><input type="hidden" value="PyTorch/fr" name="title"><select name="setlang" id="languageselector-select-1" style=""><option value="aae">ArbÃ«risht</option><option value="ab">Ğ°Ô¥ÑÑˆÓ™Ğ°</option><option value="abs">bahasa ambon</option><option value="ace">AcÃ¨h</option><option value="acf">KwÃ©yÃ²l Sent Lisi</option><option value="acm">Ø¹Ø±Ø§Ù‚ÙŠ</option><option value="ady">Ğ°Ğ´Ñ‹Ğ³Ğ°Ğ±Ğ·Ñ</option><option value="ady-cyrl">Ğ°Ğ´Ñ‹Ğ³Ğ°Ğ±Ğ·Ñ</option><option value="aeb">ØªÙˆÙ†Ø³ÙŠ / TÃ»nsÃ®</option><option value="aeb-arab">ØªÙˆÙ†Ø³ÙŠ</option><option value="aeb-latn">TÃ»nsÃ®</option><option value="af">Afrikaans</option><option value="aln">GegÃ«</option><option value="alt">Ğ°Ğ»Ñ‚Ğ°Ğ¹ Ñ‚Ğ¸Ğ»</option><option value="am">áŠ áˆ›áˆ­áŠ›</option><option value="ami">Pangcah</option><option value="an">aragonÃ©s</option><option value="ang">Ã†nglisc</option><option value="ann">Obolo</option><option value="anp">à¤…à¤‚à¤—à¤¿à¤•à¤¾</option><option value="apc">Ø´Ø§Ù…ÙŠ</option><option value="ar">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</option><option value="arc">ÜÜªÜ¡ÜÜ</option><option value="arn">mapudungun</option><option value="arq">Ø¬Ø§Ø²Ø§ÙŠØ±ÙŠØ©</option><option value="ary">Ø§Ù„Ø¯Ø§Ø±Ø¬Ø©</option><option value="arz">Ù…ØµØ±Ù‰</option><option value="as">à¦…à¦¸à¦®à§€à¦¯à¦¼à¦¾</option><option value="ase">American sign language</option><option value="ast">asturianu</option><option value="atj">Atikamekw</option><option value="av">Ğ°Ğ²Ğ°Ñ€</option><option value="avk">Kotava</option><option value="awa">à¤…à¤µà¤§à¥€</option><option value="ay">Aymar aru</option><option value="az">azÉ™rbaycanca</option><option value="azb">ØªÛ†Ø±Ú©Ø¬Ù‡</option><option value="ba">Ğ±Ğ°ÑˆÒ¡Ğ¾Ñ€Ñ‚ÑĞ°</option><option value="ban">Basa Bali</option><option value="ban-bali">á¬©á¬²á¬©á¬®á¬¶</option><option value="bar">Boarisch</option><option value="bbc">Batak Toba</option><option value="bbc-latn">Batak Toba</option><option value="bcc">Ø¬Ù‡Ù„Ø³Ø±ÛŒ Ø¨Ù„ÙˆÚ†ÛŒ</option><option value="bci">wawle</option><option value="bcl">Bikol Central</option><option value="bdr">Bajau Sama</option><option value="be">Ğ±ĞµĞ»Ğ°Ñ€ÑƒÑĞºĞ°Ñ</option><option value="be-tarask">Ğ±ĞµĞ»Ğ°Ñ€ÑƒÑĞºĞ°Ñ (Ñ‚Ğ°Ñ€Ğ°ÑˆĞºĞµĞ²Ñ–Ñ†Ğ°)</option><option value="bew">Betawi</option><option value="bg">Ğ±ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸</option><option value="bgc">à¤¹à¤°à¤¿à¤¯à¤¾à¤£à¤µà¥€</option><option value="bgn">Ø±ÙˆÚ† Ú©Ù¾ØªÛŒÙ† Ø¨Ù„ÙˆÚ†ÛŒ</option><option value="bh">à¤­à¥‹à¤œà¤ªà¥à¤°à¥€</option><option value="bho">à¤­à¥‹à¤œà¤ªà¥à¤°à¥€</option><option value="bi">Bislama</option><option value="bjn">Banjar</option><option value="blk">á€•á€¡á€­á€¯á€á€ºá‚á€˜á€¬á‚á€á€¬á‚</option><option value="bm">bamanankan</option><option value="bn">à¦¬à¦¾à¦‚à¦²à¦¾</option><option value="bo">à½–à½¼à½‘à¼‹à½¡à½²à½‚</option><option value="bpy">à¦¬à¦¿à¦·à§à¦£à§à¦ªà§à¦°à¦¿à¦¯à¦¼à¦¾ à¦®à¦£à¦¿à¦ªà§à¦°à§€</option><option value="bqi">Ø¨Ø®ØªÛŒØ§Ø±ÛŒ</option><option value="br">brezhoneg</option><option value="brh">BrÃ¡huÃ­</option><option value="bs">bosanski</option><option value="btm">Batak Mandailing</option><option value="bto">Iriga Bicolano</option><option value="bug">Basa Ugi</option><option value="bxr">Ğ±ÑƒÑ€ÑĞ°Ğ´</option><option value="ca">catalÃ </option><option value="cbk-zam">Chavacano de Zamboanga</option><option value="ccp">ğ‘„Œğ‘„‹ğ‘„´ğ‘„Ÿğ‘„³ğ‘„¦</option><option value="cdo">é–©æ±èª / MÃ¬ng-dÄ•Ì¤ng-ngá¹³Ì„</option><option value="ce">Ğ½Ğ¾Ñ…Ñ‡Ğ¸Ğ¹Ğ½</option><option value="ceb">Cebuano</option><option value="ch">Chamoru</option><option value="chn">chinuk wawa</option><option value="chr">á£á³á©</option><option value="chy">TsetsÃªhestÃ¢hese</option><option value="ckb">Ú©ÙˆØ±Ø¯ÛŒ</option><option value="co">corsu</option><option value="cps">CapiceÃ±o</option><option value="cpx">è†ä»™èª / PÃ³-sing-gá¹³Ì‚</option><option value="cpx-hans">è†ä»™è¯­ï¼ˆç®€ä½“ï¼‰</option><option value="cpx-hant">è†ä»™èªï¼ˆç¹é«”ï¼‰</option><option value="cr">NÄ“hiyawÄ“win / á“€á¦áƒá”­ááá£</option><option value="crh">qÄ±rÄ±mtatarca</option><option value="crh-cyrl">ĞºÑŠÑ‹Ñ€Ñ‹Ğ¼Ñ‚Ğ°Ñ‚Ğ°Ñ€Ğ´Ğ¶Ğ° (ĞšĞ¸Ñ€Ğ¸Ğ»Ğ»)</option><option value="crh-latn">qÄ±rÄ±mtatarca (Latin)</option><option value="crh-ro">tatarÅŸa</option><option value="cs">ÄeÅ¡tina</option><option value="csb">kaszÃ«bsczi</option><option value="cu">ÑĞ»Ğ¾Ğ²Ñ£Ğ½ÑŒÑĞºÑŠ / â°”â°â°‘â°‚â°¡â°â° â°”â°â°Ÿ</option><option value="cv">Ñ‡Ó‘Ğ²Ğ°ÑˆĞ»Ğ°</option><option value="cy">Cymraeg</option><option value="da">dansk</option><option value="dag">dagbanli</option><option value="de">Deutsch</option><option value="de-at">Ã–sterreichisches Deutsch</option><option value="de-ch">Schweizer Hochdeutsch</option><option value="de-formal">Deutsch (Sie-Form)</option><option value="dga">Dagaare</option><option value="din">ThuÉ”Å‹jÃ¤Å‹</option><option value="diq">Zazaki</option><option value="dsb">dolnoserbski</option><option value="dtp">Kadazandusun</option><option value="dty">à¤¡à¥‹à¤Ÿà¥‡à¤²à¥€</option><option value="dua">DuÃ¡lÃ¡</option><option value="dv">Ş‹Ş¨ŞˆŞ¬Ş€Ş¨Ş„Ş¦ŞŞ°</option><option value="dz">à½‡à½¼à½„à¼‹à½</option><option value="ee">eÊ‹egbe</option><option value="efi">Efá»‹k</option><option value="egl">emiliÃ n e rumagnÃ²l</option><option value="el">Î•Î»Î»Î·Î½Î¹ÎºÎ¬</option><option value="eml">emiliÃ n e rumagnÃ²l</option><option value="en" selected="">English</option><option value="en-ca">Canadian English</option><option value="en-gb">British English</option><option value="eo">Esperanto</option><option value="es">espaÃ±ol</option><option value="es-formal">espaÃ±ol (formal)</option><option value="et">eesti</option><option value="eu">euskara</option><option value="ext">estremeÃ±u</option><option value="fa">ÙØ§Ø±Ø³ÛŒ</option><option value="fat">mfantse</option><option value="ff">Fulfulde</option><option value="fi">suomi</option><option value="fit">meÃ¤nkieli</option><option value="fj">Na Vosa Vakaviti</option><option value="fo">fÃ¸royskt</option><option value="fon">fÉ”Ì€ngbÃ¨</option><option value="fr">franÃ§ais</option><option value="frc">franÃ§ais cadien</option><option value="frp">arpetan</option><option value="frr">Nordfriisk</option><option value="fur">furlan</option><option value="fy">Frysk</option><option value="ga">Gaeilge</option><option value="gaa">Ga</option><option value="gag">Gagauz</option><option value="gan">è´›èª</option><option value="gan-hans">èµ£è¯­ï¼ˆç®€ä½“ï¼‰</option><option value="gan-hant">è´›èªï¼ˆç¹é«”ï¼‰</option><option value="gcf">krÃ©yÃ²l Gwadloup</option><option value="gcr">kriyÃ²l gwiyannen</option><option value="gd">GÃ idhlig</option><option value="gl">galego</option><option value="gld">Ğ½Ğ°Ì„Ğ½Ğ¸</option><option value="glk">Ú¯ÛŒÙ„Ú©ÛŒ</option><option value="gn">AvaÃ±e'áº½</option><option value="gom">à¤—à¥‹à¤‚à¤¯à¤šà¥€ à¤•à¥‹à¤‚à¤•à¤£à¥€ / GÃµychi Konknni</option><option value="gom-deva">à¤—à¥‹à¤‚à¤¯à¤šà¥€ à¤•à¥‹à¤‚à¤•à¤£à¥€</option><option value="gom-latn">GÃµychi Konknni</option><option value="gor">Bahasa Hulontalo</option><option value="got">ğŒ²ğŒ¿ğ„ğŒ¹ğƒğŒº</option><option value="gpe">Ghanaian Pidgin</option><option value="grc">á¼ˆÏÏ‡Î±Î¯Î± á¼‘Î»Î»Î·Î½Î¹Îºá½´</option><option value="gsw">Alemannisch</option><option value="gu">àª—à«àªœàª°àª¾àª¤à«€</option><option value="guc">wayuunaiki</option><option value="gur">farefare</option><option value="guw">gungbe</option><option value="gv">Gaelg</option><option value="ha">Hausa</option><option value="hak">å®¢å®¶èª / Hak-kÃ¢-ngÃ®</option><option value="haw">HawaiÊ»i</option><option value="he">×¢×‘×¨×™×ª</option><option value="hi">à¤¹à¤¿à¤¨à¥à¤¦à¥€</option><option value="hif">Fiji Hindi</option><option value="hif-latn">Fiji Hindi</option><option value="hil">Ilonggo</option><option value="hno">ÛÙ†Ø¯Ú©Ùˆ</option><option value="hr">hrvatski</option><option value="hrx">Hunsrik</option><option value="hsb">hornjoserbsce</option><option value="hsn">æ¹˜èª</option><option value="ht">KreyÃ²l ayisyen</option><option value="hu">magyar</option><option value="hu-formal">magyar (formal)</option><option value="hy">Õ°Õ¡ÕµÕ¥Ö€Õ¥Õ¶</option><option value="hyw">Ô±Ö€Õ¥Ö‚Õ´Õ¿Õ¡Õ°Õ¡ÕµÕ¥Ö€Õ§Õ¶</option><option value="ia">interlingua</option><option value="iba">Jaku Iban</option><option value="ibb">ibibio</option><option value="id">Bahasa Indonesia</option><option value="ie">Interlingue</option><option value="ig">Igbo</option><option value="igl">Igala</option><option value="ii">ê†‡ê‰™</option><option value="ik">IÃ±upiatun</option><option value="ike-cans">áƒá“„á’ƒá‘á‘á‘¦</option><option value="ike-latn">inuktitut</option><option value="ilo">Ilokano</option><option value="inh">Ğ³Ó€Ğ°Ğ»Ğ³Ó€Ğ°Ğ¹</option><option value="io">Ido</option><option value="is">Ã­slenska</option><option value="isv-cyrl">Ğ¼ĞµĞ´Ğ¶ÑƒÑĞ»Ğ¾Ğ²Ñ˜Ğ°Ğ½ÑĞºÑ‹</option><option value="isv-latn">medÅ¾uslovjansky</option><option value="it">italiano</option><option value="iu">áƒá“„á’ƒá‘á‘á‘¦ / inuktitut</option><option value="ja">æ—¥æœ¬èª</option><option value="jam">Patois</option><option value="jbo">la .lojban.</option><option value="jut">jysk</option><option value="jv">Jawa</option><option value="ka">áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜</option><option value="kaa">Qaraqalpaqsha</option><option value="kab">Taqbaylit</option><option value="kai">Karai-karai</option><option value="kbd">Ğ°Ğ´Ñ‹Ğ³ÑĞ±Ğ·Ñ</option><option value="kbd-cyrl">Ğ°Ğ´Ñ‹Ğ³ÑĞ±Ğ·Ñ</option><option value="kbp">KabÉ©yÉ›</option><option value="kcg">Tyap</option><option value="kea">kabuverdianu</option><option value="kg">Kongo</option><option value="kge">Kumoring</option><option value="khw">Ú©Ú¾ÙˆØ§Ø±</option><option value="ki">GÄ©kÅ©yÅ©</option><option value="kiu">KÄ±rmancki</option><option value="kjh">Ñ…Ğ°ĞºĞ°Ñ</option><option value="kjp">á€–á á€¯á€¶á€œá€­á€€á€º</option><option value="kk">Ò›Ğ°Ğ·Ğ°Ò›ÑˆĞ°</option><option value="kk-arab">Ù‚Ø§Ø²Ø§Ù‚Ø´Ø§ (ØªÙ´ÙˆØªÛ•)</option><option value="kk-cn">Ù‚Ø§Ø²Ø§Ù‚Ø´Ø§ (Ø¬Û‡Ù†Ú¯Ùˆ)</option><option value="kk-cyrl">Ò›Ğ°Ğ·Ğ°Ò›ÑˆĞ° (ĞºĞ¸Ñ€Ğ¸Ğ»)</option><option value="kk-kz">Ò›Ğ°Ğ·Ğ°Ò›ÑˆĞ° (ÒšĞ°Ğ·Ğ°Ò›ÑÑ‚Ğ°Ğ½)</option><option value="kk-latn">qazaqÅŸa (latÄ±n)</option><option value="kk-tr">qazaqÅŸa (TÃ¼rkÃ¯ya)</option><option value="kl">kalaallisut</option><option value="km">á—á¶áŸá¶ááŸ’á˜áŸ‚áš</option><option value="kn">à²•à²¨à³à²¨à²¡</option><option value="knc">Yerwa Kanuri</option><option value="ko">í•œêµ­ì–´</option><option value="ko-kp">ì¡°ì„ ë§</option><option value="koi">Ğ¿ĞµÑ€ĞµĞ¼ ĞºĞ¾Ğ¼Ğ¸</option><option value="kr">kanuri</option><option value="krc">ĞºÑŠĞ°Ñ€Ğ°Ñ‡Ğ°Ğ¹-Ğ¼Ğ°Ğ»ĞºÑŠĞ°Ñ€</option><option value="kri">Krio</option><option value="krj">Kinaray-a</option><option value="krl">karjal</option><option value="ks">à¤•à¥‰à¤¶à¥à¤° / Ú©Ù²Ø´ÙØ±</option><option value="ks-arab">Ú©Ù²Ø´ÙØ±</option><option value="ks-deva">à¤•à¥‰à¤¶à¥à¤°</option><option value="ksh">Ripoarisch</option><option value="ksw">á€…á€¾á€®á¤</option><option value="ku">kurdÃ®</option><option value="ku-arab">Ú©ÙˆØ±Ø¯ÛŒ (Ø¹Û•Ø±Û•Ø¨ÛŒ)</option><option value="ku-latn">kurdÃ® (latÃ®nÃ®)</option><option value="kum">ĞºÑŠÑƒĞ¼ÑƒĞºÑŠ</option><option value="kus">KÊ‹saal</option><option value="kv">ĞºĞ¾Ğ¼Ğ¸</option><option value="kw">kernowek</option><option value="ky">ĞºÑ‹Ñ€Ğ³Ñ‹Ğ·Ñ‡Ğ°</option><option value="la">Latina</option><option value="lad">Ladino</option><option value="lb">LÃ«tzebuergesch</option><option value="lbe">Ğ»Ğ°ĞºĞºÑƒ</option><option value="lez">Ğ»ĞµĞ·Ğ³Ğ¸</option><option value="lfn">Lingua Franca Nova</option><option value="lg">Luganda</option><option value="li">Limburgs</option><option value="lij">Ligure</option><option value="liv">LÄ«vÃµ kÄ“Ä¼</option><option value="lki">Ù„Û•Ú©ÛŒ</option><option value="lld">Ladin</option><option value="lmo">lombard</option><option value="ln">lingÃ¡la</option><option value="lo">àº¥àº²àº§</option><option value="loz">Silozi</option><option value="lrc">Ù„ÛŠØ±ÛŒ Ø´ÙˆÙ…Ø§Ù„ÛŒ</option><option value="lt">lietuviÅ³</option><option value="ltg">latgaÄ¼u</option><option value="lua">ciluba</option><option value="lus">Mizo Å£awng</option><option value="luz">Ù„Ø¦Ø±ÛŒ Ø¯ÙˆÙ™Ù…ÛŒÙ†ÛŒ</option><option value="lv">latvieÅ¡u</option><option value="lzh">æ–‡è¨€</option><option value="lzz">Lazuri</option><option value="mad">MadhurÃ¢</option><option value="mag">à¤®à¤—à¤¹à¥€</option><option value="mai">à¤®à¥ˆà¤¥à¤¿à¤²à¥€</option><option value="map-bms">Basa Banyumasan</option><option value="mdf">Ğ¼Ğ¾ĞºÑˆĞµĞ½ÑŒ</option><option value="mg">Malagasy</option><option value="mhr">Ğ¾Ğ»Ñ‹Ğº Ğ¼Ğ°Ñ€Ğ¸Ğ¹</option><option value="mi">MÄori</option><option value="min">Minangkabau</option><option value="mk">Ğ¼Ğ°ĞºĞµĞ´Ğ¾Ğ½ÑĞºĞ¸</option><option value="ml">à´®à´²à´¯à´¾à´³à´‚</option><option value="mn">Ğ¼Ğ¾Ğ½Ğ³Ğ¾Ğ»</option><option value="mnc">manju gisun</option><option value="mnc-latn">manju gisun</option><option value="mnc-mong">á ®á  á ¨á µá¡  á¡¤á¡³á °á¡ á ¨</option><option value="mni">ê¯ƒê¯¤ê¯‡ê¯© ê¯‚ê¯£ê¯Ÿ</option><option value="mnw">á€˜á€¬á€á€¬á€™á€”á€º</option><option value="mo">Ğ¼Ğ¾Ğ»Ğ´Ğ¾Ğ²ĞµĞ½ÑÑĞºÑ</option><option value="mos">moore</option><option value="mr">à¤®à¤°à¤¾à¤ à¥€</option><option value="mrh">Mara</option><option value="mrj">ĞºÑ‹Ñ€Ñ‹Ğº Ğ¼Ğ°Ñ€Ñ‹</option><option value="ms">Bahasa Melayu</option><option value="ms-arab">Ø¨Ù‡Ø§Ø³ Ù…Ù„Ø§ÙŠÙˆ</option><option value="mt">Malti</option><option value="mui">Baso Palembang</option><option value="mwl">MirandÃ©s</option><option value="my">á€™á€¼á€”á€ºá€™á€¬á€˜á€¬á€á€¬</option><option value="myv">ÑÑ€Ğ·ÑĞ½ÑŒ</option><option value="mzn">Ù…Ø§Ø²ÙØ±ÙˆÙ†ÛŒ</option><option value="na">Dorerin Naoero</option><option value="nah">NÄhuatl</option><option value="nan">é–©å—èª / BÃ¢n-lÃ¢m-gÃº</option><option value="nan-hant">é–©å—èªï¼ˆå‚³çµ±æ¼¢å­—ï¼‰</option><option value="nan-latn-pehoeji">BÃ¢n-lÃ¢m-gÃº (PeÌh-Åe-jÄ«)</option><option value="nan-latn-tailo">BÃ¢n-lÃ¢m-gÃº (TÃ¢i-lÃ´)</option><option value="nap">Napulitano</option><option value="nb">norsk bokmÃ¥l</option><option value="nds">PlattdÃ¼Ã¼tsch</option><option value="nds-nl">Nedersaksies</option><option value="ne">à¤¨à¥‡à¤ªà¤¾à¤²à¥€</option><option value="new">à¤¨à¥‡à¤ªà¤¾à¤² à¤­à¤¾à¤·à¤¾</option><option value="nia">Li Niha</option><option value="nit">à°•à±Šà°²à°¾à°®à°¿</option><option value="niu">NiuÄ“</option><option value="nl">Nederlands</option><option value="nl-informal">Nederlands (informeel)</option><option value="nmz">nawdm</option><option value="nn">norsk nynorsk</option><option value="nod">á¨£á©¤á©´á¨¾á©®á©¬á©¥á¨¦</option><option value="nog">Ğ½Ğ¾Ğ³Ğ°Ğ¹ÑˆĞ°</option><option value="nov">Novial</option><option value="nqo">ß’ßß</option><option value="nr">isiNdebele seSewula</option><option value="nrm">Nouormand</option><option value="nso">Sesotho sa Leboa</option><option value="nup">Nupe</option><option value="nv">DinÃ© bizaad</option><option value="ny">Chi-Chewa</option><option value="nyn">runyankore</option><option value="nyo">Orunyoro</option><option value="nys">Nyunga</option><option value="oc">occitan</option><option value="ojb">Ojibwemowin</option><option value="olo">livvinkarjala</option><option value="om">Oromoo</option><option value="or">à¬“à¬¡à¬¼à¬¿à¬†</option><option value="os">Ğ¸Ñ€Ğ¾Ğ½</option><option value="pa">à¨ªà©°à¨œà¨¾à¨¬à©€</option><option value="pag">Pangasinan</option><option value="pam">Kapampangan</option><option value="pap">Papiamentu</option><option value="pcd">Picard</option><option value="pcm">NaijÃ¡</option><option value="pdc">Deitsch</option><option value="pdt">Plautdietsch</option><option value="pfl">PÃ¤lzisch</option><option value="pi">à¤ªà¤¾à¤²à¤¿</option><option value="pih">Norfuk / Pitkern</option><option value="pl">polski</option><option value="pms">PiemontÃ¨is</option><option value="pnb">Ù¾Ù†Ø¬Ø§Ø¨ÛŒ</option><option value="pnt">Î Î¿Î½Ï„Î¹Î±ÎºÎ¬</option><option value="prg">prÅ«siskan</option><option value="ps">Ù¾ÚšØªÙˆ</option><option value="pt">portuguÃªs</option><option value="pt-br">portuguÃªs do Brasil</option><option value="pwn">pinayuanan</option><option value="qqq">Message documentation</option><option value="qu">Runa Simi</option><option value="qug">Runa shimi</option><option value="rgn">RumagnÃ´l</option><option value="rif">Tarifit</option><option value="rki">á€›á€á€­á€¯á€„á€º</option><option value="rm">rumantsch</option><option value="rmc">romaÅˆi Ähib</option><option value="rmy">romani Ähib</option><option value="rn">ikirundi</option><option value="ro">romÃ¢nÄƒ</option><option value="roa-tara">tarandÃ­ne</option><option value="rsk">Ñ€ÑƒÑĞºĞ¸</option><option value="ru">Ñ€ÑƒÑÑĞºĞ¸Ğ¹</option><option value="rue">Ñ€ÑƒÑĞ¸Ğ½ÑŒÑĞºÑ‹Ğ¹</option><option value="rup">armÃ£neashti</option><option value="ruq">VlÄƒheÅŸte</option><option value="ruq-cyrl">Ğ’Ğ»Ğ°Ñ…ĞµÑÑ‚Ğµ</option><option value="ruq-latn">VlÄƒheÅŸte</option><option value="rut">Ğ¼Ñ‹Ñ…Ğ°Ó€Ğ±Ğ¸ÑˆĞ´Ñ‹</option><option value="rw">Ikinyarwanda</option><option value="ryu">ã†ã¡ãªãƒ¼ãã¡</option><option value="sa">à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤®à¥</option><option value="sah">ÑĞ°Ñ…Ğ° Ñ‚Ñ‹Ğ»Ğ°</option><option value="sat">á±¥á±Ÿá±±á±›á±Ÿá±²á±¤</option><option value="sc">sardu</option><option value="scn">sicilianu</option><option value="sco">Scots</option><option value="sd">Ø³Ù†ÚŒÙŠ</option><option value="sdc">Sassaresu</option><option value="sdh">Ú©ÙˆØ±Ø¯ÛŒ Ø®ÙˆØ§Ø±Ú¯</option><option value="se">davvisÃ¡megiella</option><option value="se-fi">davvisÃ¡megiella (Suoma bealde)</option><option value="se-no">davvisÃ¡megiella (Norgga bealde)</option><option value="se-se">davvisÃ¡megiella (RuoÅ§a bealde)</option><option value="sei">Cmique Itom</option><option value="ses">Koyraboro Senni</option><option value="sg">SÃ¤ngÃ¶</option><option value="sgs">Å¾emaitÄ—Å¡ka</option><option value="sh">srpskohrvatski / ÑÑ€Ğ¿ÑĞºĞ¾Ñ…Ñ€Ğ²Ğ°Ñ‚ÑĞºĞ¸</option><option value="sh-cyrl">ÑÑ€Ğ¿ÑĞºĞ¾Ñ…Ñ€Ğ²Ğ°Ñ‚ÑĞºĞ¸ (Ñ›Ğ¸Ñ€Ğ¸Ğ»Ğ¸Ñ†Ğ°)</option><option value="sh-latn">srpskohrvatski (latinica)</option><option value="shi">Taclá¸¥it</option><option value="shn">á½á‚ƒá‚‡á€á‚ƒá‚‡á€á‚†á€¸ </option><option value="shy">tacawit</option><option value="shy-latn">tacawit</option><option value="si">à·ƒà·’à¶‚à·„à¶½</option><option value="sjd">ĞºÓ£Ğ»Ğ»Ñ‚ ÑĞ°Ì„Ğ¼ÑŒ ĞºÓ£Ğ»Ğ»</option><option value="sje">bidumsÃ¡megiella</option><option value="sk">slovenÄina</option><option value="skr">Ø³Ø±Ø§Ø¦ÛŒÚ©ÛŒ</option><option value="skr-arab">Ø³Ø±Ø§Ø¦ÛŒÚ©ÛŒ</option><option value="sl">slovenÅ¡Äina</option><option value="sli">SchlÃ¤sch</option><option value="sm">Gagana Samoa</option><option value="sma">Ã¥arjelsaemien</option><option value="smn">anarÃ¢Å¡kielÃ¢</option><option value="sms">nuÃµrttsÃ¤Ã¤Ê¹mÇ©iÃµll</option><option value="sn">chiShona</option><option value="so">Soomaaliga</option><option value="sq">shqip</option><option value="sr">ÑÑ€Ğ¿ÑĞºĞ¸ / srpski</option><option value="sr-ec">ÑÑ€Ğ¿ÑĞºĞ¸ (Ñ›Ğ¸Ñ€Ğ¸Ğ»Ğ¸Ñ†Ğ°)</option><option value="sr-el">srpski (latinica)</option><option value="srn">Sranantongo</option><option value="sro">sardu campidanesu</option><option value="ss">SiSwati</option><option value="st">Sesotho</option><option value="stq">Seeltersk</option><option value="sty">ÑĞµĞ±ĞµÑ€Ñ‚Ğ°Ñ‚Ğ°Ñ€</option><option value="su">Sunda</option><option value="sv">svenska</option><option value="sw">Kiswahili</option><option value="syl">ê ê ¤ê Ÿê ê ¤</option><option value="szl">Å›lÅ¯nski</option><option value="szy">Sakizaya</option><option value="ta">à®¤à®®à®¿à®´à¯</option><option value="tay">Tayal</option><option value="tcy">à²¤à³à²³à³</option><option value="tdd">á¥–á¥­á¥° á¥–á¥¬á¥² á¥‘á¥¨á¥’á¥°</option><option value="te">à°¤à±†à°²à±à°—à±</option><option value="tet">tetun</option><option value="tg">Ñ‚Ğ¾Ò·Ğ¸ĞºÓ£</option><option value="tg-cyrl">Ñ‚Ğ¾Ò·Ğ¸ĞºÓ£</option><option value="tg-latn">tojikÄ«</option><option value="th">à¹„à¸—à¸¢</option><option value="ti">á‰µáŒáˆ­áŠ›</option><option value="tig">á‰µáŒáˆ¬</option><option value="tk">TÃ¼rkmenÃ§e</option><option value="tl">Tagalog</option><option value="tly">tolÄ±ÅŸi</option><option value="tn">Setswana</option><option value="to">lea faka-Tonga</option><option value="tok">toki pona</option><option value="tpi">Tok Pisin</option><option value="tr">TÃ¼rkÃ§e</option><option value="tru">á¹ªuroyo</option><option value="trv">Seediq</option><option value="ts">Xitsonga</option><option value="tt">Ñ‚Ğ°Ñ‚Ğ°Ñ€Ñ‡Ğ° / tatarÃ§a</option><option value="tt-cyrl">Ñ‚Ğ°Ñ‚Ğ°Ñ€Ñ‡Ğ°</option><option value="tt-latn">tatarÃ§a</option><option value="ttj">Orutooro</option><option value="tum">chiTumbuka</option><option value="tw">Twi</option><option value="ty">reo tahiti</option><option value="tyv">Ñ‚Ñ‹Ğ²Ğ° Ğ´Ñ‹Ğ»</option><option value="tzm">âµœâ´°âµâ´°âµ£âµ‰âµ–âµœ</option><option value="udm">ÑƒĞ´Ğ¼ÑƒÑ€Ñ‚</option><option value="ug">Ø¦Û‡ÙŠØºÛ‡Ø±Ú†Û• / Uyghurche</option><option value="ug-arab">Ø¦Û‡ÙŠØºÛ‡Ø±Ú†Û•</option><option value="ug-latn">Uyghurche</option><option value="uk">ÑƒĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°</option><option value="ur">Ø§Ø±Ø¯Ùˆ</option><option value="uz">oÊ»zbekcha / ÑĞ·Ğ±ĞµĞºÑ‡Ğ°</option><option value="ve">Tshivenda</option><option value="vec">vÃ¨neto</option><option value="vep">vepsÃ¤n kelâ€™</option><option value="vi">Tiáº¿ng Viá»‡t</option><option value="vls">West-Vlams</option><option value="vmf">MainfrÃ¤nkisch</option><option value="vmw">emakhuwa</option><option value="vo">VolapÃ¼k</option><option value="vot">VaÄÄa</option><option value="vro">vÃµro</option><option value="wa">walon</option><option value="wal">wolaytta</option><option value="war">Winaray</option><option value="wls">FakaÊ»uvea</option><option value="wo">Wolof</option><option value="wuu">å´è¯­</option><option value="wuu-hans">å´è¯­ï¼ˆç®€ä½“ï¼‰</option><option value="wuu-hant">å³èªï¼ˆæ­£é«”ï¼‰</option><option value="xal">Ñ…Ğ°Ğ»ÑŒĞ¼Ğ³</option><option value="xh">isiXhosa</option><option value="xmf">áƒ›áƒáƒ áƒ’áƒáƒšáƒ£áƒ áƒ˜</option><option value="xsy">saisiyat</option><option value="yi">×™×™Ö´×“×™×©</option><option value="yo">YorÃ¹bÃ¡</option><option value="yrl">Nháº½áº½gatÃº</option><option value="yue">ç²µèª</option><option value="yue-hans">ç²µè¯­ï¼ˆç®€ä½“ï¼‰</option><option value="yue-hant">ç²µèªï¼ˆç¹é«”ï¼‰</option><option value="za">Vahcuengh</option><option value="zea">ZeÃªuws</option><option value="zgh">âµœâ´°âµâ´°âµ£âµ‰âµ–âµœ âµœâ´°âµâ´°âµ¡â´°âµ¢âµœ</option><option value="zh">ä¸­æ–‡</option><option value="zh-cn">ä¸­æ–‡ï¼ˆä¸­å›½å¤§é™†ï¼‰</option><option value="zh-hans">ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰</option><option value="zh-hant">ä¸­æ–‡ï¼ˆç¹é«”ï¼‰</option><option value="zh-hk">ä¸­æ–‡ï¼ˆé¦™æ¸¯ï¼‰</option><option value="zh-mo">ä¸­æ–‡ï¼ˆæ¾³é–€ï¼‰</option><option value="zh-my">ä¸­æ–‡ï¼ˆé©¬æ¥è¥¿äºšï¼‰</option><option value="zh-sg">ä¸­æ–‡ï¼ˆæ–°åŠ å¡ï¼‰</option><option value="zh-tw">ä¸­æ–‡ï¼ˆè‡ºç£ï¼‰</option><option value="zu">isiZulu</option></select><input id="languageselector-commit-1" style="" type="submit" value="set"></form></span></div>
	</div>
	<h1 id="firstHeading" class="firstHeading mw-first-heading">PyTorch</h1>
	<div id="bodyContent" class="vector-body">
		<div id="siteSub" class="noprint">From Alliance Doc</div>
		<div id="contentSub"><div id="mw-content-subtitle"></div></div>
		<div id="contentSub2"></div>
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#searchInput">Jump to search</a>
		<div id="mw-content-text" class="mw-body-content"><div class="mw-pt-translate-header noprint" dir="ltr" lang="en">This page is a <span class="plainlinks"><a rel="nofollow" class="external text" href="https://docs.alliancecan.ca/mediawiki/index.php?title=Special:Translate&amp;group=page-PyTorch&amp;action=page&amp;filter=&amp;language=fr">translated version</a></span> of the page <a href="/wiki/PyTorch" title="PyTorch">PyTorch</a> and the translation is 100% complete.</div><div class="mw-content-ltr mw-parser-output" lang="fr" dir="ltr"><div class="mw-pt-languages noprint navigation-not-searchable" lang="en" dir="ltr"><div class="mw-pt-languages-label">Other languages:</div><ul class="mw-pt-languages-list"><li><a href="/wiki/PyTorch" class="mw-pt-languages-ui mw-pt-progress mw-pt-progress--complete" title="PyTorch (100% translated)" lang="en" dir="ltr">English</a></li>
<li><span class="mw-pt-languages-selected mw-pt-progress mw-pt-progress--complete" lang="fr" dir="ltr">franÃ§ais</span></li></ul></div>
<p><a rel="nofollow" class="external text" href="http://pytorch.org/">PyTorch</a> est un paquet Python qui offre deux fonctionnalitÃ©s de haut niveau&#160;: 
</p>
<ul><li>le calcul tensoriel (semblable Ã  celui effectuÃ© par NumPy) avec forte accÃ©lÃ©ration de GPU,</li>
<li>des rÃ©seaux de neurones dâ€™apprentissage profond dans un systÃ¨me de gradients conÃ§u sur le modÃ¨le dâ€™un magnÃ©tophone.</li></ul>
<p>Si vous voulez porter un programme PyTorch sur une de nos grappes, il serait bon de prendre connaissance <a href="/wiki/Tutoriel_Apprentissage_machine" title="Tutoriel Apprentissage machine"> de ce tutoriel</a>.
</p><p><span id="Disambiguation"></span>
</p>
<div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Clarification"><span class="tocnumber">1</span> <span class="toctext">Clarification</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Installation"><span class="tocnumber">2</span> <span class="toctext">Installation</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Wheels_rÃ©cemment_ajoutÃ©s"><span class="tocnumber">2.1</span> <span class="toctext">Wheels rÃ©cemment ajoutÃ©s</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Installation_du_wheel"><span class="tocnumber">2.2</span> <span class="toctext">Installation du wheel</span></a>
<ul>
<li class="toclevel-3 tocsection-5"><a href="#GPU_et_CPU"><span class="tocnumber">2.2.1</span> <span class="toctext">GPU et CPU</span></a></li>
<li class="toclevel-3 tocsection-6"><a href="#En_supplÃ©ment"><span class="tocnumber">2.2.2</span> <span class="toctext">En supplÃ©ment</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-7"><a href="#Soumettre_une_tÃ¢che"><span class="tocnumber">3</span> <span class="toctext">Soumettre une tÃ¢che</span></a></li>
<li class="toclevel-1 tocsection-8"><a href="#Haute_performance"><span class="tocnumber">4</span> <span class="toctext">Haute performance</span></a>
<ul>
<li class="toclevel-2 tocsection-9"><a href="#TF32_:_Performance_vs_prÃ©cision"><span class="tocnumber">4.1</span> <span class="toctext">TF32&#160;: Performance vs prÃ©cision</span></a></li>
<li class="toclevel-2 tocsection-10"><a href="#Travailler_avec_plusieurs_CPU"><span class="tocnumber">4.2</span> <span class="toctext">Travailler avec plusieurs CPU</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="#Travailler_avec_un_seul_GPU"><span class="tocnumber">4.3</span> <span class="toctext">Travailler avec un seul GPU</span></a>
<ul>
<li class="toclevel-3 tocsection-12"><a href="#ParallÃ©lisme_des_donnÃ©es_avec_un_seul_GPU"><span class="tocnumber">4.3.1</span> <span class="toctext">ParallÃ©lisme des donnÃ©es avec un seul GPU</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-13"><a href="#Travailler_avec_plusieurs_GPU"><span class="tocnumber">4.4</span> <span class="toctext">Travailler avec plusieurs GPU</span></a>
<ul>
<li class="toclevel-3 tocsection-14"><a href="#ProblÃ¨me_avec_DistributedDataParallel_et_PyTorch_1.10"><span class="tocnumber">4.4.1</span> <span class="toctext">ProblÃ¨me avec DistributedDataParallel et PyTorch 1.10</span></a></li>
<li class="toclevel-3 tocsection-15"><a href="#ParallÃ©liser_les_donnÃ©es_avec_plusieurs_GPU"><span class="tocnumber">4.4.2</span> <span class="toctext">ParallÃ©liser les donnÃ©es avec plusieurs GPU</span></a>
<ul>
<li class="toclevel-4 tocsection-16"><a href="#DistributedDataParallel"><span class="tocnumber">4.4.2.1</span> <span class="toctext">DistributedDataParallel</span></a></li>
<li class="toclevel-4 tocsection-17"><a href="#PyTorch_Lightning"><span class="tocnumber">4.4.2.2</span> <span class="toctext">PyTorch Lightning</span></a></li>
<li class="toclevel-4 tocsection-18"><a href="#Horovod"><span class="tocnumber">4.4.2.3</span> <span class="toctext">Horovod</span></a></li>
</ul>
</li>
<li class="toclevel-3 tocsection-19"><a href="#ParallÃ©liser_un_modÃ¨le_avec_plusieurs_GPU"><span class="tocnumber">4.4.3</span> <span class="toctext">ParallÃ©liser un modÃ¨le avec plusieurs GPU</span></a></li>
<li class="toclevel-3 tocsection-20"><a href="#ParallÃ©liser_modÃ¨le_et_donnÃ©es_avec_plusieurs_GPU"><span class="tocnumber">4.4.4</span> <span class="toctext">ParallÃ©liser modÃ¨le et donnÃ©es avec plusieurs GPU</span></a>
<ul>
<li class="toclevel-4 tocsection-21"><a href="#Utiliser_Torch_RPC_et_DDP"><span class="tocnumber">4.4.4.1</span> <span class="toctext">Utiliser Torch RPC et DDP</span></a></li>
</ul>
</li>
<li class="toclevel-3 tocsection-22"><a href="#DeepSpeed"><span class="tocnumber">4.4.5</span> <span class="toctext">DeepSpeed</span></a>
<ul>
<li class="toclevel-4 tocsection-23"><a href="#ZeRO_avec_GPU"><span class="tocnumber">4.4.5.1</span> <span class="toctext">ZeRO avec GPU</span></a></li>
<li class="toclevel-4 tocsection-24"><a href="#ZeRO_avec_CPU"><span class="tocnumber">4.4.5.2</span> <span class="toctext">ZeRO avec CPU</span></a></li>
<li class="toclevel-4 tocsection-25"><a href="#ZeRO_avec_utilisation_de_disques_NVMe"><span class="tocnumber">4.4.5.3</span> <span class="toctext">ZeRO avec utilisation de disques NVMe</span></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-26"><a href="#CrÃ©er_des_points_de_contrÃ´le"><span class="tocnumber">5</span> <span class="toctext">CrÃ©er des points de contrÃ´le</span></a>
<ul>
<li class="toclevel-2 tocsection-27"><a href="#Avec_PyTorch_Lightning"><span class="tocnumber">5.1</span> <span class="toctext">Avec PyTorch Lightning</span></a></li>
<li class="toclevel-2 tocsection-28"><a href="#Avec_des_boucles_d&#39;entraÃ®nement_personnalisÃ©es"><span class="tocnumber">5.2</span> <span class="toctext">Avec des boucles d'entraÃ®nement personnalisÃ©es</span></a></li>
<li class="toclevel-2 tocsection-29"><a href="#Pendant_lâ€™entraÃ®nement_distribuÃ©"><span class="tocnumber">5.3</span> <span class="toctext">Pendant lâ€™entraÃ®nement distribuÃ©</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-30"><a href="#DÃ©pannage"><span class="tocnumber">6</span> <span class="toctext">DÃ©pannage</span></a>
<ul>
<li class="toclevel-2 tocsection-31"><a href="#Fuites_de_mÃ©moire"><span class="tocnumber">6.1</span> <span class="toctext">Fuites de mÃ©moire</span></a></li>
<li class="toclevel-2 tocsection-32"><a href="#c10::Error"><span class="tocnumber">6.2</span> <span class="toctext">c10::Error</span></a></li>
<li class="toclevel-2 tocsection-33"><a href="#Erreur_CUDA_:_no_kernel_image_is_available_for_execution_on_the_device"><span class="tocnumber">6.3</span> <span class="toctext">Erreur CUDA&#160;: no kernel image is available for execution on the device</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-34"><a href="#LibTorch"><span class="tocnumber">7</span> <span class="toctext">LibTorch</span></a>
<ul>
<li class="toclevel-2 tocsection-35"><a href="#Utiliser_LibTorch"><span class="tocnumber">7.1</span> <span class="toctext">Utiliser LibTorch</span></a>
<ul>
<li class="toclevel-3 tocsection-36"><a href="#Configurer_l&#39;environnement"><span class="tocnumber">7.1.1</span> <span class="toctext">Configurer l'environnement</span></a></li>
<li class="toclevel-3 tocsection-37"><a href="#Compiler_un_exemple_simple"><span class="tocnumber">7.1.2</span> <span class="toctext">Compiler un exemple simple</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-38"><a href="#Ressources"><span class="tocnumber">8</span> <span class="toctext">Ressources</span></a></li>
</ul>
</div>

<h1><span class="mw-headline" id="Clarification">Clarification</span></h1>
<p>Il y a une certaine ressemblance entre PyTorch et <a href="/wiki/Torch/fr" title="Torch/fr">Torch</a>, mais pour des raisons pratiques vous pouvez considÃ©rer que ce sont des projets diffÃ©rents.
</p><p>Les dÃ©veloppeurs PyTorch offrent aussi <a class="mw-selflink-fragment" href="#LibTorch">LibTorch</a> qui permet d'implÃ©menter des extensions Ã  PyTorch Ã  l'aide de C++ et d'implÃ©menter des applications d'apprentissage machine en C++ pur. Les modÃ¨les Python Ã©crits avec PyTorch peuvent Ãªtre convertis et utilisÃ©s en C++ avec <a rel="nofollow" class="external text" href="https://pytorch.org/tutorials/advanced/cpp_export.html">TorchScript</a>.
</p>
<h1><span class="mw-headline" id="Installation">Installation</span></h1>
<h2><span id="Wheels_r.C3.A9cemment_ajout.C3.A9s"></span><span class="mw-headline" id="Wheels_rÃ©cemment_ajoutÃ©s">Wheels rÃ©cemment ajoutÃ©s</span></h2>
<p>Pour connaÃ®tre la derniÃ¨re version de PyTorch, utilisez
</p>
<div>
<div style="float:right; margin-left:8px">
<p><span typeof="mw:File"><a href="https://explainshell.com/explain?cmd=avail_wheels+torch" rel="nofollow"><img src="/mediawiki/images/thumb/3/30/Question.png/40px-Question.png" decoding="async" width="40" height="40" class="mw-file-element" srcset="/mediawiki/images/thumb/3/30/Question.png/60px-Question.png 1.5x, /mediawiki/images/thumb/3/30/Question.png/80px-Question.png 2x" /></a></span>
</p>
</div>
<div class="command">
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>avail_wheels<span class="w"> </span>torch
</pre></div>
</div>
</div>
<p>Pour plus d'information, voyez <a href="/wiki/Python/fr#Wheels_disponibles" title="Python/fr">Wheels disponibles</a>.
</p><p><span id="Installing_our_wheel"></span>
</p>
<h2><span class="mw-headline" id="Installation_du_wheel">Installation du wheel</span></h2>
<p>La meilleure option est d'installer avec <a rel="nofollow" class="external text" href="https://pythonwheels.com/">Python wheels</a> comme suit&#160;:
</p>
<dl><dd><dl><dd>1. <a href="/wiki/Utiliser_des_modules#Sous-commande_load" title="Utiliser des modules">Chargez un module</a>  Python avec <code>module load python</code>.</dd>
<dd>2. CrÃ©ez et dÃ©marrez un  <a href="/wiki/Python/fr#CrÃ©er_et_utiliser_un_environnement_virtuel" title="Python/fr">environnement virtuel</a>.</dd>
<dd>3. Installez PyTorch dans l'environnement virtuel avec <code>pip install</code>.</dd></dl></dd></dl>
<h4><span class="mw-headline" id="GPU_et_CPU">GPU et CPU</span></h4>
<dl><dd><div></div></dd></dl>
<div style="float:right; margin-left:8px">
<p><span typeof="mw:File"><a href="https://explainshell.com/explain?cmd=pip+install+--no-index+torch" rel="nofollow"><img src="/mediawiki/images/thumb/3/30/Question.png/40px-Question.png" decoding="async" width="40" height="40" class="mw-file-element" srcset="/mediawiki/images/thumb/3/30/Question.png/60px-Question.png 1.5x, /mediawiki/images/thumb/3/30/Question.png/80px-Question.png 2x" /></a></span>
</p>
</div>
<div class="command">
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="o">(</span>venv<span class="o">)</span><span class="w"> </span><span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--no-index<span class="w"> </span>torch
</pre></div>
</div>

<table class="wikitable" width="600px">
<tbody><tr>
<td>
<p><span class="mw-default-size" typeof="mw:File/Frameless"><a href="/wiki/File:Light-bulb.png" class="mw-file-description"><img src="/mediawiki/images/8/8b/Light-bulb.png" decoding="async" width="25" height="25" class="mw-file-element" /></a></span>With H100 gpus, torch 2.3 and higher is required.
</p>
</td>
</tr>
</tbody></table>
<p><br />
<b>Remarque&#160;: </b>PyTorch 1.10 cause des problÃ¨mes connus sur nos grappes (Ã  l'exception de Narval). Si l'entraÃ®nement distribuÃ© produit des erreurs ou si vous obtenez une erreur qui inclut <code>c10::Error</code>, nous vous recommandons d'installer PyTorch 1.9.1 avec <code>pip install --no-index torch==1.9.1</code>.
</p>
<h4><span id="En_suppl.C3.A9ment"></span><span class="mw-headline" id="En_supplÃ©ment">En supplÃ©ment</span></h4>
<p>En plus de  <code>torch</code>, vous pouvez aussi installer <code>torchvision</code>, <code>torchtext</code> et <code>torchaudio</code>.
</p>
<div>
<div style="float:right; margin-left:8px">
<p><span typeof="mw:File"><a href="https://explainshell.com/explain?cmd=pip+install+--no-index+torch+torchvision+torchtext+torchaudio" rel="nofollow"><img src="/mediawiki/images/thumb/3/30/Question.png/40px-Question.png" decoding="async" width="40" height="40" class="mw-file-element" srcset="/mediawiki/images/thumb/3/30/Question.png/60px-Question.png 1.5x, /mediawiki/images/thumb/3/30/Question.png/80px-Question.png 2x" /></a></span>
</p>
</div>
<div class="command">
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="o">(</span>venv<span class="o">)</span><span class="w"> </span><span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--no-index<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>torchtext<span class="w"> </span>torchaudio
</pre></div>
</div>
</div>
<p><span id="Job_submission"></span>
</p>
<h1><span id="Soumettre_une_t.C3.A2che"></span><span class="mw-headline" id="Soumettre_une_tÃ¢che">Soumettre une tÃ¢che</span></h1>
<p>Le script suivant est un exemple de soumission d'une tÃ¢che utilisant le wheel Python avec un environnement virtuel.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-test.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--gres%3Dgpu%3A1+++++++%23+Request+GPU+%22generic+resources%22%0A%23SBATCH+--cpus-per-task%3D6++%23+Cores+proportional+to+GPUs%3A+6+on+Cedar%2C+16+on+Graham.%0A%23SBATCH+--mem%3D32000M+++++++%23+Memory+proportional+to+GPUs%3A+32000+Cedar%2C+64000+Graham.%0A%23SBATCH+--time%3D0-03%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%0Amodule+load+python%2F%3Cselect+version%3E+%23+Make+sure+to+choose+a+version+that+suits+your+application%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+--no-index%0A%0Apython+pytorch-test.py" />
<input type="hidden" name="filename" value="pytorch-test.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --gres=gpu:1       # Request GPU &quot;generic resources&quot;</span>
<span class="c1">#SBATCH --cpus-per-task=6  # Cores proportional to GPUs: 6 on Cedar, 16 on Graham.</span>
<span class="c1">#SBATCH --mem=32000M       # Memory proportional to GPUs: 32000 Cedar, 64000 Graham.</span>
<span class="c1">#SBATCH --time=0-03:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>

module<span class="w"> </span>load<span class="w"> </span>python/&lt;<span class="k">select</span><span class="w"> </span>version&gt;<span class="w"> </span><span class="c1"># Make sure to choose a version that suits your application</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>--no-index

python<span class="w"> </span>pytorch-test.py
</pre></div>
</div>
<p><br />
</p><p>Le script Python <code>pytorch-ddp-test.py</code> a la forme suivante&#160;:
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-test.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+torch%0Ax+%3D+torch.Tensor%285%2C+3%29%0Aprint%28x%29%0Ay+%3D+torch.rand%285%2C+3%29%0Aprint%28y%29%0A%23+let+us+run+the+following+only+if+CUDA+is+available%0Aif+torch.cuda.is_available%28%29%3A%0A++++x+%3D+x.cuda%28%29%0A++++y+%3D+y.cuda%28%29%0A++++print%28x+%2B+y%29" />
<input type="hidden" name="filename" value="pytorch-test.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="c1"># let us run the following only if CUDA is available</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p><br />
</p><p>Vous pouvez alors soumettre une tÃ¢che PyTorch avec
</p>
<div>
<div style="float:right; margin-left:8px">
<p><span typeof="mw:File"><a href="https://explainshell.com/explain?cmd=sbatch+pytorch-test.sh" rel="nofollow"><img src="/mediawiki/images/thumb/3/30/Question.png/40px-Question.png" decoding="async" width="40" height="40" class="mw-file-element" srcset="/mediawiki/images/thumb/3/30/Question.png/60px-Question.png 1.5x, /mediawiki/images/thumb/3/30/Question.png/80px-Question.png 2x" /></a></span>
</p>
</div>
<div class="command">
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>sbatch<span class="w"> </span>pytorch-test.sh
</pre></div>
</div>
</div>
<p><span id="High_performance_with_PyTorch"></span>
</p>
<h1><span class="mw-headline" id="Haute_performance">Haute performance</span></h1>
<p><span id="TF32:_Performance_vs_numerical_accuracy"></span>
</p>
<h2><span id="TF32_:_Performance_vs_pr.C3.A9cision"></span><span class="mw-headline" id="TF32_:_Performance_vs_prÃ©cision">TF32&#160;: Performance vs prÃ©cision</span></h2>
<p>Avec sa version 1.7.0, PyTorch a ajoutÃ© le support pour le <a rel="nofollow" class="external text" href="https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/">mode TensorFloat-32 (TF32) de Nvidia</a> et est seulement disponible pour les architectures GPU d'Ampere et de Nvidia. Avec ce mode qui est offert par dÃ©faut dans les versions 1.7.x Ã  1.11.x, les opÃ©rations tensorielles se font jusqu'Ã  20x plus rapidement que les opÃ©rations Ã©quivalentes en simple prÃ©cision (FP32). Cependant, ce gain en performance peut engendrer une baisse dans la prÃ©cision du rÃ©sultat des opÃ©rations, ce qui pose problÃ¨me avec les modÃ¨les d'apprentissage profond qui utilisent Ã  l'occasion des matrices mal conditionnÃ©es ou qui effectuent de longues sÃ©quences d'opÃ©rations tensorielles. Suite aux commentaires de la communautÃ© des utilisateurs, TF32 est <b>dÃ©sactivÃ© par dÃ©faut pour les multiplications matricielle et activÃ© par dÃ©faut pour les convolutions</b> Ã  partir de la version 1.12.0.
</p><p>En date d'octobre 2022, notre seule grappe qui offre des GPU Ampere est <a href="/wiki/Narval" title="Narval">Narval</a>. Quand vous utilisez PyTorch sur Narval,
</p>
<ul><li>Vous pourriez remarquer un fort ralentissement dans l'exÃ©cution sur GPU du mÃªme code avec <code>torch &lt; 1.12.0</code> et <code>torch &gt;= 1.12.0</code>.</li>
<li>Vous pourriez obtenir des rÃ©sultats diffÃ©rents dans l'exÃ©cution sur GPU du mÃªme code avec <code>torch &lt; 1.12.0</code> et <code>torch &gt;= 1.12.0</code>.</li></ul>
<p>Pour activer ou dÃ©sactiver TF32 pour <code>torch &gt;= 1.12.0</code>, donnez la valeur <code>True</code> ou <code>False</code> aux indicateurs suivants&#160;:
</p>
<pre>torch.backends.cuda.matmul.allow_tf32 = False # Enable/disable TF32 for matrix multiplications
torch.backends.cudnn.allow_tf32 = False # Enable/disable TF32 for convolutions
</pre>
<p>Pour plus d'information, consultez <a rel="nofollow" class="external text" href="https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere">ce paragraphe de la documentation PyTorch</a>.
</p><p><span id="PyTorch_with_multiple_CPUs"></span>
</p>
<h2><span class="mw-headline" id="Travailler_avec_plusieurs_CPU">Travailler avec plusieurs CPU</span></h2>
<p>Par dÃ©faut, PyTorch permet le parallÃ©lisme avec plusieurs CPU de deux faÃ§ons&#160;:
</p>
<ul><li><b>intra-op</b>, par lâ€™implÃ©mentation parallÃ¨le dâ€™opÃ©rateurs souvent utilisÃ©s en apprentissage profond comme le produit matriciel ou le produit de convolution, en utilisant <a rel="nofollow" class="external text" href="https://www.openmp.org">OpenMP</a> directement ou avec des bibliothÃ¨ques de bas niveau comme <a rel="nofollow" class="external text" href="https://en.wikipedia.org/wiki/Math_Kernel_Library">MKL</a> et <a rel="nofollow" class="external text" href="https://www.intel.com/content/www/us/en/develop/documentation/oneapi-programming-guide/top/api-based-programming/intel-oneapi-deep-neural-network-library-onednn.html">OneDNN</a>. Quand du code PyTorch doit effectuer de telles opÃ©rations, elles utilisent automatiquement de multiples fils avec tous les cÅ“urs CPU disponibles.</li>
<li><b>inter-op</b>, par la capacitÃ© dâ€™exÃ©cuter diffÃ©rentes parties de code de maniÃ¨re concurrente. Ce mode de parallÃ©lisme nÃ©cessite habituellement que le programme soit conÃ§u de maniÃ¨re Ã  exÃ©cuter plusieurs parties en parallÃ¨le, par exemple en faisant usage du compilateur en temps rÃ©el <tt>torch.jit</tt> pour exÃ©cuter des tÃ¢ches asynchrones dans un programme <a rel="nofollow" class="external text" href="https://pytorch.org/docs/stable/jit.html#built-in-functions-and-modules">TorchScript</a>.</li></ul>
<p>Pour les petits modÃ¨les, nous recommandons fortement <b>dâ€™utiliser plusieurs CPU plutÃ´t quâ€™un GPU</b>. Lâ€™entraÃ®nement sera certainement plus rapide avec un GPU (sauf dans les cas de trÃ¨s petits modÃ¨les), mais si le modÃ¨le et le jeu de donnÃ©es ne sont pas assez grands, la vitesse gagnÃ©e avec le GPU ne sera probablement pas trÃ¨s importante et la tÃ¢che nâ€™utilisera quâ€™une petite part de la capacitÃ© de calcul. Ce nâ€™est peut-Ãªtre pas grave sur votre propre ordinateur, mais dans un environnement partagÃ© comme sur nos grappes, vous bloqueriez une ressource qui pourrait servir Ã  effectuer de calculs de grande Ã©chelle par un autre projet. De plus, lâ€™utilisation dâ€™un GPU contribuerait Ã  la diminution de lâ€™allocation de votre groupe et aurait une incidence sur la prioritÃ© accordÃ©e aux tÃ¢ches de vos collÃ¨gues.
</p><p>Dans le code suivant, il y a plusieurs occasions dâ€™utiliser le parallÃ©lisme intra-op. En demandant plus de CPU et sans changer le code, on peut constater lâ€™effet sur la performance.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-multi-cpu.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1%0A%23SBATCH+--tasks-per-node%3D1+%0A%23SBATCH+--cpus-per-task%3D1+%23+change+this+parameter+to+2%2C4%2C6%2C...+to+see+the+effect+on+performance%0A%0A%23SBATCH+--mem%3D8G++++++%0A%23SBATCH+--time%3D0%3A05%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+torchvision+--no-index%0A%0Aecho+%22starting+training...%22%0A%0Atime+python+cifar10-cpu.py" />
<input type="hidden" name="filename" value="pytorch-multi-cpu.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1</span>
<span class="c1">#SBATCH --tasks-per-node=1 </span>
<span class="c1">#SBATCH --cpus-per-task=1 # change this parameter to 2,4,6,... to see the effect on performance</span>

<span class="c1">#SBATCH --mem=8G      </span>
<span class="c1">#SBATCH --time=0:05:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>--no-index

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;starting training...&quot;</span>

<span class="nb">time</span><span class="w"> </span>python<span class="w"> </span>cifar10-cpu.py
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> cifar10-cpu.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+numpy+as+np%0Aimport+time%0A%0Aimport+torch%0Aimport+torch.nn+as+nn%0Aimport+torch.nn.functional+as+F%0Aimport+torch.optim+as+optim%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+argparse%0Aimport+os%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+cpu+performance+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D512%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0Adef+main%28%29%3A%0A%0A++++args+%3D+parser.parse_args%28%29%0A++++torch.set_num_threads%28int%28os.environ%5B%27SLURM_CPUS_PER_TASK%27%5D%29%29%0A++++class+Net%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A++++++++++x+%3D+F.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+F.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A++++++++++return+x%0A%0A++++net+%3D+Net%28%29%0A%0A++++criterion+%3D+nn.CrossEntropyLoss%28%29%0A++++optimizer+%3D+optim.SGD%28net.parameters%28%29%2C+lr%3Dargs.lr%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++%23%23%23+This+next+line+will+attempt+to+download+the+CIFAR10+dataset+from+the+internet+if+you+don%27t+already+have+it+stored+in+.%2Fdata+%0A++++%23%23%23+Run+this+line+on+a+login+node+with+%22download%3DTrue%22+prior+to+submitting+your+job%2C+or+manually+download+the+data+from+%0A++++%23%23%23+https%3A%2F%2Fwww.cs.toronto.edu%2F%7Ekriz%2Fcifar-10-python.tar.gz+and+place+it+under+.%2Fdata%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+num_workers%3Dargs.num_workers%29%0A%0A++++perf+%3D+%5B%5D%0A%0A++++total_start+%3D+time.time%28%29%0A%0A++++for+batch_idx%2C+%28inputs%2C+targets%29+in+enumerate%28train_loader%29%3A%0A%0A+++++++start+%3D+time.time%28%29%0A%0A+++++++outputs+%3D+net%28inputs%29%0A+++++++loss+%3D+criterion%28outputs%2C+targets%29%0A%0A+++++++optimizer.zero_grad%28%29%0A+++++++loss.backward%28%29%0A+++++++optimizer.step%28%29%0A%0A+++++++batch_time+%3D+time.time%28%29+-+start%0A%0A+++++++images_per_sec+%3D+args.batch_size%2Fbatch_time%0A%0A+++++++perf.append%28images_per_sec%29%0A%0A++++total_time+%3D+time.time%28%29+-+total_start%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="cifar10-cpu.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, cpu performance test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">set_num_threads</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;SLURM_CPUS_PER_TASK&#39;</span><span class="p">]))</span>
    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="c1">### This next line will attempt to download the CIFAR10 dataset from the internet if you don&#39;t already have it stored in ./data </span>
    <span class="c1">### Run this line on a login node with &quot;download=True&quot; prior to submitting your job, or manually download the data from </span>
    <span class="c1">### https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz and place it under ./data</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="n">perf</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">total_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

       <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

       <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
       <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

       <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
       <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
       <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

       <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

       <span class="n">images_per_sec</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="o">/</span><span class="n">batch_time</span>

       <span class="n">perf</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">images_per_sec</span><span class="p">)</span>

    <span class="n">total_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">total_start</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p><p><span id="PyTorch_with_a_single_GPU"></span>
</p>
<h2><span class="mw-headline" id="Travailler_avec_un_seul_GPU">Travailler avec un seul GPU</span></h2>
<p>On entend souvent dire quâ€™il faut absolument entraÃ®ner un modÃ¨le avec un GPU sâ€™il y en a un Ã  notre disposition. Ceci est <i>presque toujours</i> vrai (l'entraÃ®nement de trÃ¨s petits modÃ¨les est souvent plus rapide avec un ou plusieurs CPU) sur un poste de travail local, mais ce nâ€™est pas le cas sur nos grappes.
</p><p>Autrement dit, vous <b>ne devriez pas demander un GPU</b> si votre code ne peut pas faire un usage raisonnable de sa capacitÃ© de calcul.
</p><p>La performance avantageuse des GPU pour les tÃ¢ches dâ€™apprentissage profond provient de deux sources&#160;:
</p>
<ol><li>La capacitÃ© de parallÃ©liser lâ€™exÃ©cution de certaines opÃ©rations clÃ©s, par exemple le <a rel="nofollow" class="external text" href="https://fr.wikipedia.org/wiki/Multiplieur-accumulateur">multiplieur-accumulateur</a>, sur plusieurs milliers de cÅ“urs de calcul, en comparaison du trÃ¨s petit nombre de cÅ“urs disponibles avec la plupart des CPU.</li>
<li>Une bande passante de mÃ©moire beaucoup plus grande que pour un CPU, ce qui permet aux GPU dâ€™utiliser efficacement leur trÃ¨s grand nombre de cÅ“urs pour traiter une plus grande quantitÃ© de donnÃ©es par cycle de calcul.</li></ol>
<p>Comme câ€™est le cas avec plusieurs CPU, PyTorch offre des implÃ©mentations parallÃ¨les  dâ€™opÃ©rateurs souvent utilisÃ©s en apprentissage profond, comme le produit matriciel et le produit de convolution et utilise des bibliothÃ¨ques spÃ©cialisÃ©es pour les GPU comme <a rel="nofollow" class="external text" href="https://developer.nvidia.com/cudnn">CUDNN</a> ou <a rel="nofollow" class="external text" href="https://github.com/ROCmSoftwarePlatform/MIOpen">MIOpen</a>, selon la plateforme matÃ©rielle. Ceci signifie que pour quâ€™il vaille la peine dâ€™utiliser un GPU pour une tÃ¢che dâ€™apprentissage, elle doit Ãªtre composÃ©e dâ€™Ã©lÃ©ments qui peuvent Ãªtre Ã©largis Ã  une application massive du parallÃ©lisme de par le nombre dâ€™opÃ©rations pouvant Ãªtre parallÃ©lisÃ©es, de par la quantitÃ© des donnÃ©es Ã  traiter ou idÃ©alement de par les deux. Un exemple concret serait un grand modÃ¨le qui a un grand nombre dâ€™unitÃ©s et de couches ou qui a beaucoup de donnÃ©es en entrÃ©e, et idÃ©alement qui prÃ©sente ces deux caractÃ©ristiques.
</p><p>Dans lâ€™exemple ci-dessous, nous adaptons le code de la section prÃ©cÃ©dente pour utiliser un GPU et nous examinons la performance. Nous observons que deux paramÃ¨tres jouent un rÃ´le important&#160;: <code>batch_size</code> et <code>num_workers</code>. Le premier paramÃ¨tre amÃ©liore la performance en augmentant la taille des entrÃ©es Ã  chaque itÃ©ration et en utilisant mieux la capacitÃ© du GPU. Dans le cas du second paramÃ¨tre, la performance est amÃ©liorÃ©e en facilitant  le mouvement des donnÃ©es entre la mÃ©moire de lâ€™hÃ´te (le CPU) et la mÃ©moire du GPU, ce qui rÃ©duit la durÃ©e dâ€™inactivitÃ© du GPU en attente de donnÃ©es Ã  traiter.
</p><p>Nous pouvons tirer deux conclusions&#160;:
</p>
<ol><li>Augmenter la valeur de <code>batch_size</code> au maximum quâ€™il est possible pour la mÃ©moire du GPU optimise la performance.</li>
<li>Utiliser un <code>DataLoader</code> avec autant de workers que <code>cpus-per-task</code> facilite lâ€™apport de donnÃ©es au GPU.</li></ol>
<p>Bien entendu, le paramÃ¨tre <code>batch_size</code> a aussi un impact sur la performance dâ€™un modÃ¨le dans une tÃ¢che(c.Ã .d. lâ€™exactitude, lâ€™erreur, etc.) et il existe diffÃ©rentes Ã©coles de pensÃ©e sur lâ€™utilisation de grands lots. Nous nâ€™abordons pas le sujet ici, mais si vous croyez quâ€™un petit lot conviendrait mieux Ã  votre application, allez Ã  la section <a class="mw-selflink-fragment" href="#Travailler_avec_un_seul_GPU">Travailler avec un seul GPU</a>  pour savoir comment maximiser lâ€™utilisation du GPU avec de petites entrÃ©es de donnÃ©es.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-single-gpu.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1%0A%23SBATCH+--gres%3Dgpu%3A1+%23+request+a+GPU%0A%23SBATCH+--tasks-per-node%3D1+%0A%23SBATCH+--cpus-per-task%3D1+%23+change+this+parameter+to+2%2C4%2C6%2C...+and+increase+%22--num_workers%22+accordingly+to+see+the+effect+on+performance%0A%23SBATCH+--mem%3D8G++++++%0A%23SBATCH+--time%3D0%3A05%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+torchvision+--no-index%0A%0Aecho+%22starting+training...%22%0Atime+python+cifar10-gpu.py+--batch_size%3D512+--num_workers%3D0" />
<input type="hidden" name="filename" value="pytorch-single-gpu.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1</span>
<span class="c1">#SBATCH --gres=gpu:1 # request a GPU</span>
<span class="c1">#SBATCH --tasks-per-node=1 </span>
<span class="c1">#SBATCH --cpus-per-task=1 # change this parameter to 2,4,6,... and increase &quot;--num_workers&quot; accordingly to see the effect on performance</span>
<span class="c1">#SBATCH --mem=8G      </span>
<span class="c1">#SBATCH --time=0:05:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>--no-index

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;starting training...&quot;</span>
<span class="nb">time</span><span class="w"> </span>python<span class="w"> </span>cifar10-gpu.py<span class="w"> </span>--batch_size<span class="o">=</span><span class="m">512</span><span class="w"> </span>--num_workers<span class="o">=</span><span class="m">0</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> cifar10-gpu.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+numpy+as+np%0Aimport+time%0A%0Aimport+torch%0Aimport+torch.nn+as+nn%0Aimport+torch.nn.functional+as+F%0Aimport+torch.optim+as+optim%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+single+gpu+performance+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D512%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++class+Net%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A++++++++++x+%3D+F.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+F.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A++++++++++return+x%0A%0A++++net+%3D+Net%28%29.cuda%28%29+%23+Load+model+on+the+GPU%0A%0A++++criterion+%3D+nn.CrossEntropyLoss%28%29.cuda%28%29+%23+Load+the+loss+function+on+the+GPU%0A++++optimizer+%3D+optim.SGD%28net.parameters%28%29%2C+lr%3Dargs.lr%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+num_workers%3Dargs.num_workers%29%0A%0A++++perf+%3D+%5B%5D%0A%0A++++total_start+%3D+time.time%28%29%0A%0A++++for+batch_idx%2C+%28inputs%2C+targets%29+in+enumerate%28train_loader%29%3A%0A%0A+++++++start+%3D+time.time%28%29%0A+++++++%0A+++++++inputs+%3D+inputs.cuda%28%29+%0A+++++++targets+%3D+targets.cuda%28%29%0A%0A+++++++outputs+%3D+net%28inputs%29%0A+++++++loss+%3D+criterion%28outputs%2C+targets%29%0A%0A+++++++optimizer.zero_grad%28%29%0A+++++++loss.backward%28%29%0A+++++++optimizer.step%28%29%0A%0A+++++++batch_time+%3D+time.time%28%29+-+start%0A%0A+++++++images_per_sec+%3D+args.batch_size%2Fbatch_time%0A%0A+++++++perf.append%28images_per_sec%29%0A%0A++++total_time+%3D+time.time%28%29+-+total_start%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="cifar10-gpu.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, single gpu performance test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> <span class="c1"># Load model on the GPU</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> <span class="c1"># Load the loss function on the GPU</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="n">perf</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">total_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

       <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
       
       <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> 
       <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

       <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
       <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

       <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
       <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
       <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

       <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

       <span class="n">images_per_sec</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="o">/</span><span class="n">batch_time</span>

       <span class="n">perf</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">images_per_sec</span><span class="p">)</span>

    <span class="n">total_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">total_start</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p><p><span id="Data_parallelism_with_a_single_GPU"></span>
</p>
<h3><span id="Parall.C3.A9lisme_des_donn.C3.A9es_avec_un_seul_GPU"></span><span class="mw-headline" id="ParallÃ©lisme_des_donnÃ©es_avec_un_seul_GPU">ParallÃ©lisme des donnÃ©es avec un seul GPU</span></h3>
<p>Il <b>nâ€™est pas conseillÃ© dâ€™utiliser un GPU</b> avec un modÃ¨le de taille relativement petite qui nâ€™utilise pas une grande part de la mÃ©moire du GPU et une part raisonnable de sa capacitÃ© de calcul; utilisez plutÃ´t <a class="mw-selflink-fragment" href="#Travailler_avec_plusieurs_CPU">un ou plusieurs CPU</a>. Par contre, profiter du parallÃ©lisme du GPU devient une bonne option si vous avez un tel modÃ¨le avec un trÃ¨s grand jeu de donnÃ©es et que vous voulez effectuer lâ€™entraÃ®nement avec des lots de petite taille. 
</p><p>Dans ce contexte, la parallÃ©lisation des donnÃ©es rÃ©fÃ¨re Ã  des mÃ©thodes pour entraÃ®ner en parallÃ¨le plusieurs copies dâ€™un modÃ¨le oÃ¹ chaque copie reÃ§oit un morceau des donnÃ©es dâ€™entraÃ®nement Ã  chaque itÃ©ration. Ã€ la fin dâ€™une itÃ©ration, les gradients sont agrÃ©gÃ©s et les paramÃ¨tres de chaque copie sont mis Ã  jour de faÃ§on synchrone ou asynchrone, dÃ©pendant de la mÃ©thode. Cette approche peut augmenter la vitesse dâ€™exÃ©cution de faÃ§on importante avec une itÃ©ration qui se fait environ N fois plus rapidement avec un grand jeu de donnÃ©es, N Ã©tant le nombre de copies du modÃ¨le. Pour utiliser cette approche, <b>un avertissement sâ€™impose</b>&#160;:  pour que le modÃ¨le entraÃ®nÃ© soit Ã©quivalent au mÃªme modÃ¨le entraÃ®nÃ© sans parallÃ©lisme, vous devez adapter le taux dâ€™apprentissage ou la taille du lot dÃ©sirÃ©e en fonction du nombre de copies. Pour plus dâ€™information, voyez <a rel="nofollow" class="external text" href="https://discuss.pytorch.org/t/should-we-split-batch-size-according-to-ngpu-per-node-when-distributeddataparallel/72769/13">ces Ã©changes</a>.  
</p><p>PyTorch offre des implÃ©mentations de mÃ©thodes de parallÃ©lisme des donnÃ©es, la classe <code>DistributedDataParallel</code> Ã©tant celle <a rel="nofollow" class="external text" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#comparison-between-dataparallel-and-distributeddataparallel">recommandÃ©e par les dÃ©veloppeurs de PyTorch</a> pour donner la meilleure performance. ConÃ§ue pour le <a class="mw-selflink-fragment" href="#Travailler_avec_plusieurs_GPU">travail avec plusieurs GPU</a>, elle peut aussi Ãªtre employÃ©e avec un seul GPU.
</p><p>Dans lâ€™exemple ci-dessous, nous adaptons le code pour un seul GPU pour utiliser le parallÃ©lisme des donnÃ©es. La tÃ¢che est relativement petite; la taille du lot est de 512 images, le modÃ¨le occupe environ 1Go de la mÃ©moire du GPU et lâ€™entraÃ®nement nâ€™utilise quâ€™environ 6&#160;% de sa capacitÃ© de calcul. Ce modÃ¨le <b>ne devrait pas</b> Ãªtre entraÃ®nÃ© sur un GPU sur nos grappes. Cependant, en parallÃ©lisant les donnÃ©es, un GPU V100 avec 16Go de mÃ©moire peut contenir 14 ou 15 copies du modÃ¨le et augmenter l'utilisation de la ressource en plus dâ€™obtenir une bonne augmentation de vitesse. Nous utilisons <a rel="nofollow" class="external text" href="https://docs.nvidia.com/deploy/mps/index.html">Multi-Process Service (MPS) de NVIDIA</a> avec <a href="/wiki/MPI/fr" title="MPI/fr">MPI</a> pour placer plusieurs copies du modÃ¨le sur un GPU de faÃ§on efficace.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-gpu-mps.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1%0A%23SBATCH+--gres%3Dgpu%3A1+%23+request+a+GPU%0A%23SBATCH+--tasks-per-node%3D8+%23+This+is+the+number+of+model+replicas+we+will+place+on+the+GPU.+Change+this+to+10%2C12%2C14%2C...+to+see+the+effect+on+performance++%0A%23SBATCH+--cpus-per-task%3D1+%23+increase+this+parameter+and+increase+%22--num_workers%22+accordingly+to+see+the+effect+on+performance%0A%23SBATCH+--mem%3D8G++++++%0A%23SBATCH+--time%3D0%3A05%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+torchvision+--no-index%0A%0A%23+Activate+Nvidia+MPS%3A%0Aexport+CUDA_MPS_PIPE_DIRECTORY%3D%2Ftmp%2Fnvidia-mps%0Aexport+CUDA_MPS_LOG_DIRECTORY%3D%2Ftmp%2Fnvidia-log%0Anvidia-cuda-mps-control+-d%0A%0Aecho+%22starting+training...%22%0Atime+srun+--cpus-per-task%3D%24SLURM_CPUS_PER_TASK+python+cifar10-gpu-mps.py+--batch_size%3D512+--num_workers%3D0" />
<input type="hidden" name="filename" value="pytorch-gpu-mps.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1</span>
<span class="c1">#SBATCH --gres=gpu:1 # request a GPU</span>
<span class="c1">#SBATCH --tasks-per-node=8 # This is the number of model replicas we will place on the GPU. Change this to 10,12,14,... to see the effect on performance  </span>
<span class="c1">#SBATCH --cpus-per-task=1 # increase this parameter and increase &quot;--num_workers&quot; accordingly to see the effect on performance</span>
<span class="c1">#SBATCH --mem=8G      </span>
<span class="c1">#SBATCH --time=0:05:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>--no-index

<span class="c1"># Activate Nvidia MPS:</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_MPS_PIPE_DIRECTORY</span><span class="o">=</span>/tmp/nvidia-mps
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_MPS_LOG_DIRECTORY</span><span class="o">=</span>/tmp/nvidia-log
nvidia-cuda-mps-control<span class="w"> </span>-d

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;starting training...&quot;</span>
<span class="nb">time</span><span class="w"> </span>srun<span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="nv">$SLURM_CPUS_PER_TASK</span><span class="w"> </span>python<span class="w"> </span>cifar10-gpu-mps.py<span class="w"> </span>--batch_size<span class="o">=</span><span class="m">512</span><span class="w"> </span>--num_workers<span class="o">=</span><span class="m">0</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> cifar10-gpu-mps.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+os%0Aimport+time%0Aimport+datetime%0Aimport+numpy+as+np%0A%0Aimport+torch%0Aimport+torch.nn+as+nn%0Aimport+torch.nn.functional+as+F%0Aimport+torch.optim+as+optim%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+torch.distributed+as+dist%0Aimport+torch.utils.data.distributed%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+distributed+data+parallel+maps+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D512%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--init_method%27%2C+default%3D%27tcp%3A%2F%2F127.0.0.1%3A3456%27%2C+type%3Dstr%2C+help%3D%27%27%29%0A%0Adef+main%28%29%3A%0A++++print%28%22Starting...%22%29%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++rank+%3D+os.environ.get%28%22SLURM_LOCALID%22%29%0A%0A++++current_device+%3D+0%0A++++torch.cuda.set_device%28current_device%29%0A%0A++++%22%22%22+this+block+initializes+a+process+group+and+initiate+communications%0A++++++++++++++++between+all+processes+that+will+run+a+model+replica+%22%22%22%0A%0A++++print%28%27From+Rank%3A+%7B%7D%2C+%3D%3D%3E+Initializing+Process+Group...%27.format%28rank%29%29%0A%0A++++dist.init_process_group%28backend%3D%22mpi%22%2C+init_method%3Dargs.init_method%29+%23+Use+backend%3D%22mpi%22+or+%22gloo%22.+NCCL+does+not+work+on+a+single+GPU+due+to+a+hard-coded+multi-GPU+topology+check.%0A++++print%28%22process+group+ready%21%22%29%0A%0A++++print%28%27From+Rank%3A+%7B%7D%2C+%3D%3D%3E+Making+model..%27.format%28rank%29%29%0A%0A++++class+Net%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A++++++++++x+%3D+F.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+F.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A++++++++++return+x%0A%0A++++net+%3D+Net%28%29%0A%0A++++net.cuda%28%29%0A++++net+%3D+torch.nn.parallel.DistributedDataParallel%28net%2C+device_ids%3D%5Bcurrent_device%5D%29+%23+Wrap+the+model+with+DistributedDataParallel%0A%0A++++criterion+%3D+nn.CrossEntropyLoss%28%29.cuda%28%29%0A++++optimizer+%3D+optim.SGD%28net.parameters%28%29%2C+lr%3Dargs.lr%29%0A%0A++++print%28%27From+Rank%3A+%7B%7D%2C+%3D%3D%3E+Preparing+data..%27.format%28rank%29%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27%7E%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_sampler+%3D+torch.utils.data.distributed.DistributedSampler%28dataset_train%29%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+shuffle%3D%28train_sampler+is+None%29%2C+num_workers%3Dargs.num_workers%2C+sampler%3Dtrain_sampler%29%0A%0A++++perf+%3D+%5B%5D%0A%0A++++total_start+%3D+time.time%28%29%0A%0A++++for+batch_idx%2C+%28inputs%2C+targets%29+in+enumerate%28train_loader%29%3A%0A%0A+++++++start+%3D+time.time%28%29%0A+++++++%0A+++++++inputs+%3D+inputs.cuda%28%29+%0A+++++++targets+%3D+targets.cuda%28%29%0A%0A+++++++outputs+%3D+net%28inputs%29%0A+++++++loss+%3D+criterion%28outputs%2C+targets%29%0A%0A+++++++optimizer.zero_grad%28%29%0A+++++++loss.backward%28%29%0A+++++++optimizer.step%28%29%0A%0A+++++++batch_time+%3D+time.time%28%29+-+start%0A%0A+++++++images_per_sec+%3D+args.batch_size%2Fbatch_time%0A%0A+++++++perf.append%28images_per_sec%29%0A%0A++++total_time+%3D+time.time%28%29+-+total_start%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="cifar10-gpu-mps.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.utils.data.distributed</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, distributed data parallel maps test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--init_method&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;tcp://127.0.0.1:3456&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting...&quot;</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">rank</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;SLURM_LOCALID&quot;</span><span class="p">)</span>

    <span class="n">current_device</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">current_device</span><span class="p">)</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; this block initializes a process group and initiate communications</span>
<span class="sd">                between all processes that will run a model replica &quot;&quot;&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From Rank: </span><span class="si">{}</span><span class="s1">, ==&gt; Initializing Process Group...&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>

    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;mpi&quot;</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">init_method</span><span class="p">)</span> <span class="c1"># Use backend=&quot;mpi&quot; or &quot;gloo&quot;. NCCL does not work on a single GPU due to a hard-coded multi-GPU topology check.</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;process group ready!&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From Rank: </span><span class="si">{}</span><span class="s1">, ==&gt; Making model..&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>

    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

    <span class="n">net</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">current_device</span><span class="p">])</span> <span class="c1"># Wrap the model with DistributedDataParallel</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From Rank: </span><span class="si">{}</span><span class="s1">, ==&gt; Preparing data..&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;~/data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="p">(</span><span class="n">train_sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>

    <span class="n">perf</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">total_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

       <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
       
       <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> 
       <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

       <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
       <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

       <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
       <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
       <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

       <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

       <span class="n">images_per_sec</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="o">/</span><span class="n">batch_time</span>

       <span class="n">perf</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">images_per_sec</span><span class="p">)</span>

    <span class="n">total_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">total_start</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p><p><span id="PyTorch_with_multiple_GPUs"></span>
</p>
<h2><span class="mw-headline" id="Travailler_avec_plusieurs_GPU">Travailler avec plusieurs GPU</span></h2>
<p><span id="Issue_with_DistributedDataParallel_and_PyTorch_1.10"></span>
</p>
<h3><span id="Probl.C3.A8me_avec_DistributedDataParallel_et_PyTorch_1.10"></span><span class="mw-headline" id="ProblÃ¨me_avec_DistributedDataParallel_et_PyTorch_1.10">ProblÃ¨me avec DistributedDataParallel et PyTorch 1.10</span></h3>
<p>Avec notre wheel PyTorch 1.10 <code>torch-1.10.0+computecanada</code>, le code pour travailler avec plusieurs GPU et qui utilise 
<a class="mw-selflink-fragment" href="#DistributedDataParallel">DistributedDataParallel</a> pourrait Ã©chouer de faÃ§on imprÃ©visible si le backend est dÃ©fini comme Ã©tant <code>'nccl'</code> ou <code>'gloo'</code>. Nous vous recommandons d'utiliser la version la plus rÃ©cente de PyTorch plutÃ´t que la version 1.10 sur toutes les grappes d'usage gÃ©nÃ©ral.
</p>
<h3><span id="Parall.C3.A9liser_les_donn.C3.A9es_avec_plusieurs_GPU"></span><span class="mw-headline" id="ParallÃ©liser_les_donnÃ©es_avec_plusieurs_GPU">ParallÃ©liser les donnÃ©es avec plusieurs GPU</span></h3>
<p>Dans ce contexte, la parallÃ©lisation des donnÃ©es rÃ©fÃ¨re Ã  des mÃ©thodes pour entraÃ®ner en parallÃ¨le plusieurs copies dâ€™un modÃ¨le oÃ¹ chaque copie reÃ§oit une portion des donnÃ©es dâ€™entraÃ®nement Ã  chaque itÃ©ration. Ã€ la fin dâ€™une itÃ©ration, les gradients sont agrÃ©gÃ©s et les paramÃ¨tres de chaque copie sont mis Ã  jour de faÃ§on synchrone ou asynchrone, dÃ©pendant de la mÃ©thode. Cette approche peut augmenter la vitesse dâ€™exÃ©cution de faÃ§on importante avec une itÃ©ration qui se fait environ N fois plus rapidement avec un grand jeu de donnÃ©es, N Ã©tant le nombre de copies du modÃ¨le. Pour utiliser cette approche, un avertissement sâ€™impose&#160;:  pour que le modÃ¨le entraÃ®nÃ© soit Ã©quivalent au mÃªme modÃ¨le entraÃ®nÃ© sans parallÃ©lisme, vous devez adapter le taux dâ€™apprentissage ou la taille du lot dÃ©sirÃ©e en fonction du nombre de copies. Pour plus dâ€™information, voyez <a rel="nofollow" class="external text" href="https://discuss.pytorch.org/t/should-we-split-batch-size-according-to-ngpu-per-node-when-distributeddataparallel/72769/13">ces Ã©changes</a>.
Quand plusieurs GPU sont utilisÃ©s, chacun reÃ§oit une copie du modÃ¨le; il doit donc Ãªtre assez petit pour Ãªtre contenu dans la mÃ©moire dâ€™un GPU. Pour entraÃ®ner un modÃ¨le qui dÃ©passe la quantitÃ© de mÃ©moire dâ€™un GPU, voyez la section <a class="mw-selflink-fragment" href="#ParallÃ©liser_un_modÃ¨le_avec_plusieurs_GPU">ParallÃ©liser un modÃ¨le avec plusieurs GPU</a>.
</p><p>Il existe plusieurs maniÃ¨res de parallÃ©liser les donnÃ©es avec PyTorch. Nous prÃ©sentons ici des tutoriels avec la classe <b>DistributedDataParallel</b>, avec le paquet  <b>PyTorch Lightning</b> et avec le paquet <b>Horovod</b>.
</p><p><span id="Using_DistributedDataParallel"></span>
</p>
<h4><span class="mw-headline" id="DistributedDataParallel">DistributedDataParallel</span></h4>
<p>Avec plusieurs GPU, la classe <b>DistributedDataParallel</b> est  <a rel="nofollow" class="external text" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#comparison-between-dataparallel-and-distributeddataparallel">recommandÃ©e par les dÃ©veloppeurs PyTorch</a>, que ce soit avec un nÅ“ud unique ou avec plusieurs nÅ“uds. Dans le cas qui suit, plusieurs GPU sont distribuÃ©s sur deux nÅ“uds.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-ddp-test.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1+++++++++++++%0A%23SBATCH+--gres%3Dgpu%3A2++++++++++%23+Request+2+GPU+%22generic+resources%E2%80%9D.%0A%23SBATCH+--tasks-per-node%3D2+++%23+Request+1+process+per+GPU.+You+will+get+1+CPU+per+process+by+default.+Request+more+CPUs+with+the+%22cpus-per-task%22+parameter+to+enable+multiple+data-loader+workers+to+load+data+in+parallel.%0A%23SBATCH+--mem%3D8G++++++%0A%23SBATCH+--time%3D0-03%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%0Asrun+-N+%24SLURM_NNODES+-n+%24SLURM_NNODES+bash+%3C%3C+EOF%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+torchvision+--no-index%0AEOF%0A%0Aexport+TORCH_NCCL_ASYNC_HANDLING%3D1%0Aexport+MASTER_ADDR%3D%24%28hostname%29+%23Store+the+master+node%E2%80%99s+IP+address+in+the+MASTER_ADDR+environment+variable.%0A%0Aecho+%22r%24SLURM_NODEID+master%3A+%24MASTER_ADDR%22%0Aecho+%22r%24SLURM_NODEID+Launching+python+script%22%0A%0A%23+The+%24%28%28SLURM_NTASKS_PER_NODE+%2A+SLURM_JOB_NUM_NODES%29%29+variable+tells+the+script+how+many+processes+are+available+for+this+execution.+%E2%80%9Csrun%E2%80%9D+executes+the+script+%3Ctasks-per-node+%2A+nodes%3E+times%0A%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0A%0Asrun+python+pytorch-ddp-test.py+--init_method+tcp%3A%2F%2F%24MASTER_ADDR%3A3456+--world_size+%24%28%28SLURM_NTASKS_PER_NODE+%2A+SLURM_JOB_NUM_NODES%29%29++--batch_size+256" />
<input type="hidden" name="filename" value="pytorch-ddp-test.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1             </span>
<span class="c1">#SBATCH --gres=gpu:2          # Request 2 GPU &quot;generic resourcesâ€.</span>
<span class="c1">#SBATCH --tasks-per-node=2   # Request 1 process per GPU. You will get 1 CPU per process by default. Request more CPUs with the &quot;cpus-per-task&quot; parameter to enable multiple data-loader workers to load data in parallel.</span>
<span class="c1">#SBATCH --mem=8G      </span>
<span class="c1">#SBATCH --time=0-03:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application</span>
srun<span class="w"> </span>-N<span class="w"> </span><span class="nv">$SLURM_NNODES</span><span class="w"> </span>-n<span class="w"> </span><span class="nv">$SLURM_NNODES</span><span class="w"> </span>bash<span class="w"> </span><span class="s">&lt;&lt; EOF</span>
<span class="s">virtualenv --no-download $SLURM_TMPDIR/env</span>
<span class="s">source $SLURM_TMPDIR/env/bin/activate</span>
<span class="s">pip install torch torchvision --no-index</span>
<span class="s">EOF</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">TORCH_NCCL_ASYNC_HANDLING</span><span class="o">=</span><span class="m">1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MASTER_ADDR</span><span class="o">=</span><span class="k">$(</span>hostname<span class="k">)</span><span class="w"> </span><span class="c1">#Store the master nodeâ€™s IP address in the MASTER_ADDR environment variable.</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;r</span><span class="nv">$SLURM_NODEID</span><span class="s2"> master: </span><span class="nv">$MASTER_ADDR</span><span class="s2">&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;r</span><span class="nv">$SLURM_NODEID</span><span class="s2"> Launching python script&quot;</span>

<span class="c1"># The $((SLURM_NTASKS_PER_NODE * SLURM_JOB_NUM_NODES)) variable tells the script how many processes are available for this execution. â€œsrunâ€ executes the script &lt;tasks-per-node * nodes&gt; times</span>

<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate

srun<span class="w"> </span>python<span class="w"> </span>pytorch-ddp-test.py<span class="w"> </span>--init_method<span class="w"> </span>tcp://<span class="nv">$MASTER_ADDR</span>:3456<span class="w"> </span>--world_size<span class="w"> </span><span class="k">$((</span><span class="nv">SLURM_NTASKS_PER_NODE</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nv">SLURM_JOB_NUM_NODES</span><span class="k">))</span><span class="w">  </span>--batch_size<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
<p><br />
</p><p>Le script Python <code>pytorch-ddp-test.py</code> a la forme suivante&#160;:
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-ddp-test.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+os%0Aimport+time%0Aimport+datetime%0A%0Aimport+torch%0Aimport+torch.nn+as+nn%0Aimport+torch.nn.functional+as+F%0Aimport+torch.optim+as+optim%0Aimport+torch.backends.cudnn+as+cudnn%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+torch.distributed+as+dist%0Aimport+torch.utils.data.distributed%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+distributed+data+parallel+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D768%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--max_epochs%27%2C+type%3Dint%2C+default%3D4%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0Aparser.add_argument%28%27--init_method%27%2C+default%3D%27tcp%3A%2F%2F127.0.0.1%3A3456%27%2C+type%3Dstr%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--dist-backend%27%2C+default%3D%27nccl%27%2C+type%3Dstr%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--world_size%27%2C+default%3D1%2C+type%3Dint%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--distributed%27%2C+action%3D%27store_true%27%2C+help%3D%27%27%29%0A%0Adef+main%28%29%3A%0A++++print%28%22Starting...%22%29%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++ngpus_per_node+%3D+torch.cuda.device_count%28%29%0A%0A++++%22%22%22+This+next+line+is+the+key+to+getting+DistributedDataParallel+working+on+SLURM%3A%0A%09%09SLURM_NODEID+is+0+or+1+in+this+example%2C+SLURM_LOCALID+is+the+id+of+the+%0A+%09%09current+process+inside+a+node+and+is+also+0+or+1+in+this+example.%22%22%22%0A%0A++++local_rank+%3D+int%28os.environ.get%28%22SLURM_LOCALID%22%29%29+%0A++++rank+%3D+int%28os.environ.get%28%22SLURM_NODEID%22%29%29%2Angpus_per_node+%2B+local_rank%0A%0A++++current_device+%3D+local_rank%0A%0A++++torch.cuda.set_device%28current_device%29%0A%0A++++%22%22%22+this+block+initializes+a+process+group+and+initiate+communications%0A%09%09between+all+processes+running+on+all+nodes+%22%22%22%0A%0A++++print%28%27From+Rank%3A+%7B%7D%2C+%3D%3D%3E+Initializing+Process+Group...%27.format%28rank%29%29%0A++++%23init+the+process+group%0A++++dist.init_process_group%28backend%3Dargs.dist_backend%2C+init_method%3Dargs.init_method%2C+world_size%3Dargs.world_size%2C+rank%3Drank%29%0A++++print%28%22process+group+ready%21%22%29%0A%0A++++print%28%27From+Rank%3A+%7B%7D%2C+%3D%3D%3E+Making+model..%27.format%28rank%29%29%0A%0A++++class+Net%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A++++++++++x+%3D+F.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+F.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A++++++++++return+x%0A%0A++++net+%3D+Net%28%29%0A%0A++++net.cuda%28%29%0A++++net+%3D+torch.nn.parallel.DistributedDataParallel%28net%2C+device_ids%3D%5Bcurrent_device%5D%29%0A%0A++++print%28%27From+Rank%3A+%7B%7D%2C+%3D%3D%3E+Preparing+data..%27.format%28rank%29%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_sampler+%3D+torch.utils.data.distributed.DistributedSampler%28dataset_train%29%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+shuffle%3D%28train_sampler+is+None%29%2C+num_workers%3Dargs.num_workers%2C+sampler%3Dtrain_sampler%29%0A%0A++++criterion+%3D+nn.CrossEntropyLoss%28%29.cuda%28%29%0A++++optimizer+%3D+optim.SGD%28net.parameters%28%29%2C+lr%3Dargs.lr%2C+momentum%3D0.9%2C+weight_decay%3D1e-4%29%0A%0A++++for+epoch+in+range%28args.max_epochs%29%3A%0A%0A++++++++train_sampler.set_epoch%28epoch%29%0A%0A++++++++train%28epoch%2C+net%2C+criterion%2C+optimizer%2C+train_loader%2C+rank%29%0A%0Adef+train%28epoch%2C+net%2C+criterion%2C+optimizer%2C+train_loader%2C+train_rank%29%3A%0A%0A++++train_loss+%3D+0%0A++++correct+%3D+0%0A++++total+%3D+0%0A%0A++++epoch_start+%3D+time.time%28%29%0A%0A++++for+batch_idx%2C+%28inputs%2C+targets%29+in+enumerate%28train_loader%29%3A%0A%0A+++++++start+%3D+time.time%28%29%0A%0A+++++++inputs+%3D+inputs.cuda%28%29%0A+++++++targets+%3D+targets.cuda%28%29%0A+++++++outputs+%3D+net%28inputs%29%0A+++++++loss+%3D+criterion%28outputs%2C+targets%29%0A%0A+++++++optimizer.zero_grad%28%29%0A+++++++loss.backward%28%29%0A+++++++optimizer.step%28%29%0A%0A+++++++train_loss+%2B%3D+loss.item%28%29%0A+++++++_%2C+predicted+%3D+outputs.max%281%29%0A+++++++total+%2B%3D+targets.size%280%29%0A+++++++correct+%2B%3D+predicted.eq%28targets%29.sum%28%29.item%28%29%0A+++++++acc+%3D+100+%2A+correct+%2F+total%0A%0A+++++++batch_time+%3D+time.time%28%29+-+start%0A%0A+++++++elapse_time+%3D+time.time%28%29+-+epoch_start%0A+++++++elapse_time+%3D+datetime.timedelta%28seconds%3Delapse_time%29%0A+++++++print%28%22From+Rank%3A+%7B%7D%2C+Training+time+%7B%7D%22.format%28train_rank%2C+elapse_time%29%29%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="pytorch-ddp-test.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">datetime</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torch.backends.cudnn</span> <span class="k">as</span> <span class="nn">cudnn</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.utils.data.distributed</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, distributed data parallel test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--init_method&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;tcp://127.0.0.1:3456&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--dist-backend&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--world_size&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--distributed&#39;</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s1">&#39;store_true&#39;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting...&quot;</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">ngpus_per_node</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; This next line is the key to getting DistributedDataParallel working on SLURM:</span>
<span class="sd">		SLURM_NODEID is 0 or 1 in this example, SLURM_LOCALID is the id of the </span>
<span class="sd"> 		current process inside a node and is also 0 or 1 in this example.&quot;&quot;&quot;</span>

    <span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;SLURM_LOCALID&quot;</span><span class="p">))</span> 
    <span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;SLURM_NODEID&quot;</span><span class="p">))</span><span class="o">*</span><span class="n">ngpus_per_node</span> <span class="o">+</span> <span class="n">local_rank</span>

    <span class="n">current_device</span> <span class="o">=</span> <span class="n">local_rank</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">current_device</span><span class="p">)</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; this block initializes a process group and initiate communications</span>
<span class="sd">		between all processes running on all nodes &quot;&quot;&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From Rank: </span><span class="si">{}</span><span class="s1">, ==&gt; Initializing Process Group...&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>
    <span class="c1">#init the process group</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">dist_backend</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">init_method</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;process group ready!&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From Rank: </span><span class="si">{}</span><span class="s1">, ==&gt; Making model..&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>

    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

    <span class="n">net</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">current_device</span><span class="p">])</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From Rank: </span><span class="si">{}</span><span class="s1">, ==&gt; Preparing data..&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="p">(</span><span class="n">train_sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">):</span>

        <span class="n">train_sampler</span><span class="o">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>

        <span class="n">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">train_rank</span><span class="p">):</span>

    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">epoch_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

       <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

       <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
       <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
       <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
       <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

       <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
       <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
       <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

       <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
       <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
       <span class="n">total</span> <span class="o">+=</span> <span class="n">targets</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
       <span class="n">correct</span> <span class="o">+=</span> <span class="n">predicted</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
       <span class="n">acc</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>

       <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

       <span class="n">elapse_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">epoch_start</span>
       <span class="n">elapse_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="n">elapse_time</span><span class="p">)</span>
       <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;From Rank: </span><span class="si">{}</span><span class="s2">, Training time </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_rank</span><span class="p">,</span> <span class="n">elapse_time</span><span class="p">))</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h4><span class="mw-headline" id="PyTorch_Lightning">PyTorch Lightning</span></h4>
<p>Ce paquet fournit des interfaces Ã  PyTorch afin de simplifier plusieurs tÃ¢ches communes exigeant beaucoup de code; ceci inclut les tÃ¢ches d'entraÃ®nement de modÃ¨les avec plusieurs GPU. Dans le tutoriel suivant pour PyTorch Lightning, nous reprenons le mÃªme exemple que ci-dessus, mais sans avoir explicitement recours Ã  la classe DistributedDataParallel.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-ddp-test-pl.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1+++++++++++++%0A%23SBATCH+--gres%3Dgpu%3A2++++++++++%23+Request+2+GPU+%22generic+resources%E2%80%9D.%0A%23SBATCH+--tasks-per-node%3D2++++%23+Request+1+process+per+GPU.+You+will+get+1+CPU+per+process+by+default.+Request+more+CPUs+with+the+%22cpus-per-task%22+parameter+to+enable+multiple+data-loader+workers+to+load+data+in+parallel.%0A%23SBATCH+--mem%3D8G++++++%0A%23SBATCH+--time%3D0-03%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torchvision+pytorch-lightning+--no-index%0A%0Aexport+TORCH_NCCL_ASYNC_HANDLING%3D1%0A%0A%23+PyTorch+Lightning+will+query+the+environment+to+figure+out+if+it+is+running+inside+a+SLURM+batch+job%0A%23+If+it+is%2C+it+expects+the+user+to+have+requested+one+task+per+GPU.%0A%23+If+you+do+not+ask+for+1+task+per+GPU%2C+and+you+do+not+run+your+script+with+%22srun%22%2C+your+job+will+fail%21%0A%0Asrun+python+pytorch-ddp-test-pl.py++--batch_size+256" />
<input type="hidden" name="filename" value="pytorch-ddp-test-pl.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1             </span>
<span class="c1">#SBATCH --gres=gpu:2          # Request 2 GPU &quot;generic resourcesâ€.</span>
<span class="c1">#SBATCH --tasks-per-node=2    # Request 1 process per GPU. You will get 1 CPU per process by default. Request more CPUs with the &quot;cpus-per-task&quot; parameter to enable multiple data-loader workers to load data in parallel.</span>
<span class="c1">#SBATCH --mem=8G      </span>
<span class="c1">#SBATCH --time=0-03:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torchvision<span class="w"> </span>pytorch-lightning<span class="w"> </span>--no-index

<span class="nb">export</span><span class="w"> </span><span class="nv">TORCH_NCCL_ASYNC_HANDLING</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># PyTorch Lightning will query the environment to figure out if it is running inside a SLURM batch job</span>
<span class="c1"># If it is, it expects the user to have requested one task per GPU.</span>
<span class="c1"># If you do not ask for 1 task per GPU, and you do not run your script with &quot;srun&quot;, your job will fail!</span>

srun<span class="w"> </span>python<span class="w"> </span>pytorch-ddp-test-pl.py<span class="w">  </span>--batch_size<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-ddp-test-pl.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+datetime%0A%0Aimport+torch%0Afrom+torch+import+nn%0Aimport+torch.nn.functional+as+F%0A%0Aimport+pytorch_lightning+as+pl%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+pytorch-lightning+parallel+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--max_epochs%27%2C+type%3Dint%2C+default%3D4%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D768%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A++++print%28%22Starting...%22%29%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++class+Net%28pl.LightningModule%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A++++++++++x+%3D+F.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+F.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A++++++++++return+x%0A%0A+++++++def+training_step%28self%2C+batch%2C+batch_idx%29%3A%0A++++++++++x%2C+y+%3D+batch%0A++++++++++y_hat+%3D+self%28x%29%0A++++++++++loss+%3D+F.cross_entropy%28y_hat%2C+y%29%0A++++++++++return+loss%0A%0A+++++++def+configure_optimizers%28self%29%3A%0A++++++++++return+torch.optim.Adam%28self.parameters%28%29%2C+lr%3Dargs.lr%29%0A%0A++++net+%3D+Net%28%29%0A%0A++++%22%22%22+Here+we+initialize+a+Trainer%28%29+explicitly+with+1+node+and+2+GPUs+per+node.%0A++++++++To+make+this+script+more+generic%2C+you+can+use+torch.cuda.device_count%28%29+to+set+the+number+of+GPUs%0A++++++++and+you+can+use+int%28os.environ.get%28%22SLURM_JOB_NUM_NODES%22%29%29+to+set+the+number+of+nodes.+%0A++++++++We+also+set+progress_bar_refresh_rate%3D0+to+avoid+writing+a+progress+bar+to+the+logs%2C+%0A++++++++which+can+cause+issues+due+to+updating+logs+too+frequently.%22%22%22%0A%0A++++trainer+%3D+pl.Trainer%28accelerator%3D%22gpu%22%2C+devices%3D2%2C+num_nodes%3D1%2C+strategy%3D%27ddp%27%2C+max_epochs+%3D+args.max_epochs%2C+enable_progress_bar%3DFalse%29+%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+num_workers%3Dargs.num_workers%29%0A%0A++++trainer.fit%28net%2Ctrain_loader%29%0A%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="pytorch-ddp-test-pl.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">datetime</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, pytorch-lightning parallel test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting...&quot;</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>

       <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
          <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
          <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">loss</span>

       <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; Here we initialize a Trainer() explicitly with 1 node and 2 GPUs per node.</span>
<span class="sd">        To make this script more generic, you can use torch.cuda.device_count() to set the number of GPUs</span>
<span class="sd">        and you can use int(os.environ.get(&quot;SLURM_JOB_NUM_NODES&quot;)) to set the number of nodes. </span>
<span class="sd">        We also set progress_bar_refresh_rate=0 to avoid writing a progress bar to the logs, </span>
<span class="sd">        which can cause issues due to updating logs too frequently.&quot;&quot;&quot;</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_nodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;ddp&#39;</span><span class="p">,</span> <span class="n">max_epochs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">,</span> <span class="n">enable_progress_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> 

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">net</span><span class="p">,</span><span class="n">train_loader</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h4><span class="mw-headline" id="Horovod">Horovod</span></h4>
<p><a rel="nofollow" class="external text" href="https://horovod.readthedocs.io/en/latest/summary_include.html">Horovod</a> est une plateforme d'entraÃ®nement distribuÃ© pour l'apprentissage profond, compatible avec TensorFlow, Keras, PyTorch et Apache MXNet. Son API vous permet d'avoir le mÃªme niveau de contrÃ´le sur votre code d'entraÃ®nement qu'avec <code>DistributedDataParallel</code>, mais simplifie l'Ã©criture de vos scripts en Ã©liminant le besoin de configurer directement les groupes de processus et en prenant en charge les variables d'environnement de l'ordonnanceur. Horovod offre aussi des optimiseurs distribuÃ©s qui dans certains cas amÃ©liorent la performance. L'exemple suivant est le mÃªme que le prÃ©cÃ©dent, cette fois avec Horovod.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch_horovod.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1++++++++++++%0A%23SBATCH+--gres%3Dgpu%3A2+++++++++%23+Request+2+GPU+%22generic+resources%E2%80%9D.%0A%0A%23SBATCH+--tasks-per-node%3D2+++%23+Request+1+process+per+GPU.+You+will+get+1+CPU+per+process+by+default.+Request+more+CPUs+with+the+%22cpus-per-task%22+parameter+to+enable+multiple+data-loader+workers+to+load+data+in+parallel.%0A%0A%23SBATCH+--mem%3D8G++++++%0A%23SBATCH+--time%3D0-03%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+torchvision+horovod+--no-index%0A%0Aexport+TORCH_NCCL_ASYNC_HANDLING%3D1%0A%0Asrun+python+pytorch_horovod.py++--batch_size+256" />
<input type="hidden" name="filename" value="pytorch_horovod.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1            </span>
<span class="c1">#SBATCH --gres=gpu:2         # Request 2 GPU &quot;generic resourcesâ€.</span>

<span class="c1">#SBATCH --tasks-per-node=2   # Request 1 process per GPU. You will get 1 CPU per process by default. Request more CPUs with the &quot;cpus-per-task&quot; parameter to enable multiple data-loader workers to load data in parallel.</span>

<span class="c1">#SBATCH --mem=8G      </span>
<span class="c1">#SBATCH --time=0-03:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>horovod<span class="w"> </span>--no-index

<span class="nb">export</span><span class="w"> </span><span class="nv">TORCH_NCCL_ASYNC_HANDLING</span><span class="o">=</span><span class="m">1</span>

srun<span class="w"> </span>python<span class="w"> </span>pytorch_horovod.py<span class="w">  </span>--batch_size<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch_horovod.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+os%0Aimport+time%0Aimport+datetime%0Aimport+numpy+as+np%0Aimport+horovod.torch+as+hvd%0A%0Aimport+torch%0Aimport+torch.nn+as+nn%0Aimport+torch.nn.functional+as+F%0Aimport+torch.optim+as+optim%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+torch.distributed+as+dist%0Aimport+torch.utils.data.distributed%0A%0Aimport+argparse%0A%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+horovod+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D512%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--max_epochs%27%2C+type%3Dint%2C+default%3D1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++hvd.init%28%29%0A%0A++++print%28%22Starting...%22%29%0A%0A++++local_rank+%3D+hvd.local_rank%28%29%0A++++global_rank+%3D+hvd.rank%28%29%0A%0A++++torch.cuda.set_device%28local_rank%29%0A%0A%0A++++class+Net%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A++++++++++x+%3D+F.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+F.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A++++++++++return+x%0A%0A++++net+%3D+Net%28%29%0A%0A++++net.cuda%28%29%0A%0A++++print%28%27From+Rank%3A+%7B%7D%2C+%3D%3D%3E+Preparing+data..%27.format%28global_rank%29%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_sampler+%3D+torch.utils.data.distributed.DistributedSampler%28dataset_train%2C+num_replicas%3Dhvd.size%28%29%2Crank%3Dglobal_rank%29%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+shuffle%3D%28train_sampler+is+None%29%2C+num_workers%3Dargs.num_workers%2C+sampler%3Dtrain_sampler%29%0A%0A%0A++++criterion+%3D+nn.CrossEntropyLoss%28%29.cuda%28%29%0A++++optimizer+%3D+optim.SGD%28net.parameters%28%29%2C+lr%3Dargs.lr%2C+momentum%3D0.9%2C+weight_decay%3D1e-4%29%0A%0A++++optimizer+%3D+hvd.DistributedOptimizer%28optimizer%2C+named_parameters%3Dnet.named_parameters%28%29%29%0A%0A++++hvd.broadcast_parameters%28net.state_dict%28%29%2C+root_rank%3D0%29%0A%0A++++for+epoch+in+range%28args.max_epochs%29%3A%0A%0A++++++++train_sampler.set_epoch%28epoch%29%0A%0A++++++++train%28args%2Cepoch%2C+net%2C+criterion%2C+optimizer%2C+train_loader%2C+global_rank%29%0A%0A%0Adef+train%28args%2Cepoch%2C+net%2C+criterion%2C+optimizer%2C+train_loader%2C+train_rank%29%3A%0A%0A++++train_loss+%3D+0%0A++++correct+%3D+0%0A++++total+%3D+0%0A%0A++++epoch_start+%3D+time.time%28%29%0A%0A++++for+batch_idx%2C+%28inputs%2C+targets%29+in+enumerate%28train_loader%29%3A%0A%0A+++++++start+%3D+time.time%28%29%0A%0A+++++++inputs+%3D+inputs.cuda%28%29%0A+++++++targets+%3D+targets.cuda%28%29%0A+++++++outputs+%3D+net%28inputs%29%0A+++++++loss+%3D+criterion%28outputs%2C+targets%29%0A%0A+++++++optimizer.zero_grad%28%29%0A+++++++loss.backward%28%29%0A+++++++optimizer.step%28%29%0A%0A+++++++train_loss+%2B%3D+loss.item%28%29%0A+++++++_%2C+predicted+%3D+outputs.max%281%29%0A+++++++total+%2B%3D+targets.size%280%29%0A+++++++correct+%2B%3D+predicted.eq%28targets%29.sum%28%29.item%28%29%0A+++++++acc+%3D+100+%2A+correct+%2F+total%0A%0A+++++++batch_time+%3D+time.time%28%29+-+start%0A%0A+++++++elapse_time+%3D+time.time%28%29+-+epoch_start%0A+++++++elapse_time+%3D+datetime.timedelta%28seconds%3Delapse_time%29%0A+++++++print%28%22From+Rank%3A+%7B%7D%2C+Training+time+%7B%7D%22.format%28train_rank%2C+elapse_time%29%29%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="pytorch_horovod.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">horovod.torch</span> <span class="k">as</span> <span class="nn">hvd</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.utils.data.distributed</span>

<span class="kn">import</span> <span class="nn">argparse</span>


<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, horovod test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting...&quot;</span><span class="p">)</span>

    <span class="n">local_rank</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">local_rank</span><span class="p">()</span>
    <span class="n">global_rank</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>


    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

    <span class="n">net</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From Rank: </span><span class="si">{}</span><span class="s1">, ==&gt; Preparing data..&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">global_rank</span><span class="p">))</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">num_replicas</span><span class="o">=</span><span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span><span class="n">rank</span><span class="o">=</span><span class="n">global_rank</span><span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="p">(</span><span class="n">train_sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>


    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedOptimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">named_parameters</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>

    <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_parameters</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">):</span>

        <span class="n">train_sampler</span><span class="o">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>

        <span class="n">train</span><span class="p">(</span><span class="n">args</span><span class="p">,</span><span class="n">epoch</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">global_rank</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">args</span><span class="p">,</span><span class="n">epoch</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">train_rank</span><span class="p">):</span>

    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">epoch_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

       <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

       <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
       <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
       <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
       <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

       <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
       <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
       <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

       <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
       <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
       <span class="n">total</span> <span class="o">+=</span> <span class="n">targets</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
       <span class="n">correct</span> <span class="o">+=</span> <span class="n">predicted</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
       <span class="n">acc</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>

       <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

       <span class="n">elapse_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">epoch_start</span>
       <span class="n">elapse_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="n">elapse_time</span><span class="p">)</span>
       <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;From Rank: </span><span class="si">{}</span><span class="s2">, Training time </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_rank</span><span class="p">,</span> <span class="n">elapse_time</span><span class="p">))</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h3><span id="Parall.C3.A9liser_un_mod.C3.A8le_avec_plusieurs_GPU"></span><span class="mw-headline" id="ParallÃ©liser_un_modÃ¨le_avec_plusieurs_GPU">ParallÃ©liser un modÃ¨le avec plusieurs GPU</span></h3>
<p>Quand un modÃ¨le est trop grand pour Ãªtre contenu dans <a class="mw-selflink-fragment" href="#Travailler_avec_un_seul_GPU">un seul GPU</a>, vous pouvez le diviser en portions et charger chacune sur des GPU distincts. Dans lâ€™exemple suivant, nous reprenons le code des sections prÃ©cÃ©dentes pour illustrer le fonctionnement. Nous divisons un rÃ©seau de neurones convolutifs en deux&#160;: dâ€™une part les couches convolutionnelles/de regroupement et d'autre part les couches acycliques entiÃ¨rement connectÃ©es. La tÃ¢che demande deux GPU, chacun dâ€™eux Ã©tant chargÃ© avec une des parts.  De plus, nous ajoutons du code pour <a rel="nofollow" class="external text" href="https://pytorch.org/docs/stable/pipeline.html?highlight=pipeline">parallÃ©liser les pipelines</a> et ainsi rÃ©duire au maximum le temps pendant lequel le deuxiÃ¨me GPU est inactif dans lâ€™attente des rÃ©sultats du premier. Pour ce faire, nous crÃ©ons un <code>nn.Module</code> distinct pour chacune des portions du modÃ¨le, puis crÃ©ons une sÃ©quence de modules en enveloppant les portions avec <code>nn.Sequential</code>, et ensuite utilisons <code>torch.distributed.pipeline.sync.Pipe</code> pour morceler chacun des lots en entrÃ©e et les passer en parallÃ¨le aux deux portions du modÃ¨le.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-modelpar-pipelined-rpc.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1%0A%23SBATCH+--gres%3Dgpu%3A2+%23+request+2+GPUs%0A%23SBATCH+--tasks-per-node%3D1+%0A%23SBATCH+--cpus-per-task%3D1+%23+change+this+parameter+to+2%2C4%2C6%2C...+and+increase+%22--num_workers%22+accordingly+to+see+the+effect+on+performance%0A%23SBATCH+--mem%3D8G++++++%0A%23SBATCH+--time%3D0%3A10%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+torchvision+--no-index%0A%0A%23+This+is+needed+to+initialize+pytorch%27s+RPC+module%2C+required+for+the+Pipe+class+which+we%27ll+use+for+Pipeline+Parallelism%0Aexport+MASTER_ADDR%3D%24%28hostname%29%0Aexport+MASTER_PORT%3D34567%0A+%0Aecho+%22starting+training...%22%0Atime+python+pytorch-modelpar-pipelined-rpc.py+--batch_size%3D512+--num_workers%3D0" />
<input type="hidden" name="filename" value="pytorch-modelpar-pipelined-rpc.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1</span>
<span class="c1">#SBATCH --gres=gpu:2 # request 2 GPUs</span>
<span class="c1">#SBATCH --tasks-per-node=1 </span>
<span class="c1">#SBATCH --cpus-per-task=1 # change this parameter to 2,4,6,... and increase &quot;--num_workers&quot; accordingly to see the effect on performance</span>
<span class="c1">#SBATCH --mem=8G      </span>
<span class="c1">#SBATCH --time=0:10:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>--no-index

<span class="c1"># This is needed to initialize pytorch&#39;s RPC module, required for the Pipe class which we&#39;ll use for Pipeline Parallelism</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MASTER_ADDR</span><span class="o">=</span><span class="k">$(</span>hostname<span class="k">)</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MASTER_PORT</span><span class="o">=</span><span class="m">34567</span>
<span class="w"> </span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;starting training...&quot;</span>
<span class="nb">time</span><span class="w"> </span>python<span class="w"> </span>pytorch-modelpar-pipelined-rpc.py<span class="w"> </span>--batch_size<span class="o">=</span><span class="m">512</span><span class="w"> </span>--num_workers<span class="o">=</span><span class="m">0</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-modelpar-pipelined-rpc.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+time%0A%0Aimport+torch%0Aimport+torch.nn+as+nn%0Aimport+torch.optim+as+optim%0Afrom+torch.distributed.pipeline.sync+import+Pipe%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+single+node+model+parallelism+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D512%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++%23+Convolutional+%2B+pooling+part+of+the+model%0A++++class+ConvPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28ConvPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A%0A++++++++++return+x%0A%0A++++%23+Dense+feedforward+part+of+the+model%0A++++class+MLPPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28MLPPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+self.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A%0A++++++++++return+x%0A%0A++++torch.distributed.rpc.init_rpc%28%27worker%27%2C+rank%3D0%2C+world_size%3D1%29+%23+initializing+RPC+is+required+by+Pipe+we+use+below%0A%0A++++part1+%3D+ConvPart%28%29.to%28%27cuda%3A0%27%29+%23+Load+part1+on+the+first+GPU%0A++++part2+%3D+MLPPart%28%29.to%28%27cuda%3A1%27%29+%23+Load+part2+on+the+second+GPU%0A%0A++++net+%3D+nn.Sequential%28part1%2Cpart2%29+%23+Pipe+requires+all+modules+be+wrapped+with+nn.Sequential%28%29%0A%0A++++net+%3D+Pipe%28net%2C+chunks%3D32%29+%23+Wrap+with+Pipe+to+perform+Pipeline+Parallelism%0A%0A++++criterion+%3D+nn.CrossEntropyLoss%28%29.to%28%27cuda%3A1%27%29+%23+Load+the+loss+function+on+the+last+GPU%0A++++optimizer+%3D+optim.SGD%28net.parameters%28%29%2C+lr%3Dargs.lr%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+num_workers%3Dargs.num_workers%29%0A%0A++++perf+%3D+%5B%5D%0A%0A++++total_start+%3D+time.time%28%29%0A%0A++++for+batch_idx%2C+%28inputs%2C+targets%29+in+enumerate%28train_loader%29%3A%0A%0A+++++++start+%3D+time.time%28%29%0A%0A+++++++inputs+%3D+inputs.to%28%27cuda%3A0%27%29%0A+++++++targets+%3D+targets.to%28%27cuda%3A1%27%29%0A%0A+++++++%23+Models+wrapped+with+Pipe%28%29+return+a+RRef+object.+Since+the+example+is+single+node%2C+all+values+are+local+to+the+node+and+we+can+grab+them%0A+++++++outputs+%3D+net%28inputs%29.local_value%28%29%0A+++++++loss+%3D+criterion%28outputs%2C+targets%29%0A%0A+++++++optimizer.zero_grad%28%29%0A+++++++loss.backward%28%29%0A+++++++optimizer.step%28%29%0A+++++++print%28f%22Loss%3A+%7Bloss.item%28%29%7D%22%29%0A%0A+++++++batch_time+%3D+time.time%28%29+-+start%0A%0A+++++++images_per_sec+%3D+args.batch_size%2Fbatch_time%0A%0A+++++++perf.append%28images_per_sec%29%0A%0A++++total_time+%3D+time.time%28%29+-+total_start%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="pytorch-modelpar-pipelined-rpc.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.distributed.pipeline.sync</span> <span class="kn">import</span> <span class="n">Pipe</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, single node model parallelism test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="c1"># Convolutional + pooling part of the model</span>
    <span class="k">class</span> <span class="nc">ConvPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">ConvPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="c1"># Dense feedforward part of the model</span>
    <span class="k">class</span> <span class="nc">MLPPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">MLPPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">rpc</span><span class="o">.</span><span class="n">init_rpc</span><span class="p">(</span><span class="s1">&#39;worker&#39;</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># initializing RPC is required by Pipe we use below</span>

    <span class="n">part1</span> <span class="o">=</span> <span class="n">ConvPart</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span> <span class="c1"># Load part1 on the first GPU</span>
    <span class="n">part2</span> <span class="o">=</span> <span class="n">MLPPart</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span> <span class="c1"># Load part2 on the second GPU</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">part1</span><span class="p">,</span><span class="n">part2</span><span class="p">)</span> <span class="c1"># Pipe requires all modules be wrapped with nn.Sequential()</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Pipe</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">chunks</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span> <span class="c1"># Wrap with Pipe to perform Pipeline Parallelism</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span> <span class="c1"># Load the loss function on the last GPU</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="n">perf</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">total_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

       <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

       <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
       <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span>

       <span class="c1"># Models wrapped with Pipe() return a RRef object. Since the example is single node, all values are local to the node and we can grab them</span>
       <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">local_value</span><span class="p">()</span>
       <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

       <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
       <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
       <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
       <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

       <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

       <span class="n">images_per_sec</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="o">/</span><span class="n">batch_time</span>

       <span class="n">perf</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">images_per_sec</span><span class="p">)</span>

    <span class="n">total_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">total_start</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h3><span id="Parall.C3.A9liser_mod.C3.A8le_et_donn.C3.A9es_avec_plusieurs_GPU"></span><span class="mw-headline" id="ParallÃ©liser_modÃ¨le_et_donnÃ©es_avec_plusieurs_GPU">ParallÃ©liser modÃ¨le et donnÃ©es avec plusieurs GPU</span></h3>
<p>Quand un modÃ¨le est trop grand pour Ãªtre contenu dans un seul GPU et que son entraÃ®nement doit se faire avec un trÃ¨s grand ensemble de donnÃ©es, le fait de combiner le parallÃ©lisme du modÃ¨le et celui des donnÃ©es permet dâ€™obtenir une bonne performance. Le principe est simple&#160;: le modÃ¨le est divisÃ© en portions chacune attribuÃ©e Ã  un GPU; le parallÃ©lisme des pipelines est fait avec les rÃ©sultats; puis des copies du processus sont faites et les copies du modÃ¨le sont entraÃ®nÃ©es en parallÃ¨le avec des sous-ensembles distincts de lâ€™ensemble de donnÃ©es dâ€™entraÃ®nement. 
Comme dÃ©crit <a class="mw-selflink-fragment" href="#ParallÃ©liser_les_donnÃ©es_avec_plusieurs_GPU">ci-dessus</a>, les gradients sont calculÃ©s indÃ©pendamment dans chacune des copies et agrÃ©gÃ©s pour modifier toutes les copies de faÃ§on synchrone ou asynchrone, dÃ©pendant de la mÃ©thode. La diffÃ©rence principale ici est que chaque copie du modÃ¨le se trouve sur plus dâ€™un GPU. 
</p><p><span id="Using_Torch_RPC_and_DDP"></span>
</p>
<h4><span class="mw-headline" id="Utiliser_Torch_RPC_et_DDP">Utiliser Torch RPC et DDP</span></h4>
<p>Toujours avec le mÃªme exemple, nous combinons maintenant Torch RPC et DistributedDataParallel pour sÃ©parer le modÃ¨le en deux portions et entraÃ®ner quatre copies du modÃ¨le distribuÃ©es en parallÃ¨le sur deux nÅ“uds. Autrement dit, nous avons deux copies sur deux GPU de chaque nÅ“ud. Cependant, un avertissement sâ€™impose&#160;: Ã  ce jour, Torch RPC prend en charge la division dâ€™un modÃ¨le dans un seul nÅ“ud. Pour entraÃ®ner un modÃ¨le qui dÃ©passe la quantitÃ© de mÃ©moire de tous les GPU dans un nÅ“ud de calcul, voyez <a class="mw-selflink-fragment" href="#DeepSpeed">la section DeepSpeed</a>.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-model-data-par.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+2%0A%23SBATCH+--gres%3Dgpu%3A4+%23+Request+4+GPUs+per+node%0A%23SBATCH+--tasks-per-node%3D2+%23+Request+one+task+per+MODEL+per+node%0A%23SBATCH+--cpus-per-task%3D1+%23+change+this+parameter+to+2%2C4%2C6%2C...+and+increase+%22--num_workers%22+accordingly+to+see+the+effect+on+performance%0A%23SBATCH+--mem%3D16G++++++%0A%23SBATCH+--time%3D0%3A10%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+StdEnv%2F2020+gcc%2F11.3.0%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%2C+python%2F3.10.2+works+with+this+demo%0Amodule+load+cuda%2F11.8.0%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+torchvision+--no-index%0A%0Aexport+MAIN_NODE%3D%24%28hostname%29%0A%0Aecho+%22starting+training...%22%0A%0Asrun+python+pytorch-model-data-par.py+--init_method+tcp%3A%2F%2F%24MAIN_NODE%3A3456+--world_size+%24SLURM_NTASKS++--batch_size+512" />
<input type="hidden" name="filename" value="pytorch-model-data-par.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 2</span>
<span class="c1">#SBATCH --gres=gpu:4 # Request 4 GPUs per node</span>
<span class="c1">#SBATCH --tasks-per-node=2 # Request one task per MODEL per node</span>
<span class="c1">#SBATCH --cpus-per-task=1 # change this parameter to 2,4,6,... and increase &quot;--num_workers&quot; accordingly to see the effect on performance</span>
<span class="c1">#SBATCH --mem=16G      </span>
<span class="c1">#SBATCH --time=0:10:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>StdEnv/2020<span class="w"> </span>gcc/11.3.0
module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application, python/3.10.2 works with this demo</span>
module<span class="w"> </span>load<span class="w"> </span>cuda/11.8.0
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>--no-index

<span class="nb">export</span><span class="w"> </span><span class="nv">MAIN_NODE</span><span class="o">=</span><span class="k">$(</span>hostname<span class="k">)</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;starting training...&quot;</span>

srun<span class="w"> </span>python<span class="w"> </span>pytorch-model-data-par.py<span class="w"> </span>--init_method<span class="w"> </span>tcp://<span class="nv">$MAIN_NODE</span>:3456<span class="w"> </span>--world_size<span class="w"> </span><span class="nv">$SLURM_NTASKS</span><span class="w">  </span>--batch_size<span class="w"> </span><span class="m">512</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-model-data-par.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+time%0Aimport+os%0A%0Aimport+torch%0Aimport+torch.nn+as+nn%0Aimport+torch.optim+as+optim%0Afrom+torch.distributed.pipeline.sync+import+Pipe%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+torch.distributed+as+dist%0Aimport+torch.utils.data.distributed%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+distributed+data+%26+model+parallel+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D768%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--max_epochs%27%2C+type%3Dint%2C+default%3D4%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0Aparser.add_argument%28%27--init_method%27%2C+default%3D%27tcp%3A%2F%2F127.0.0.1%3A3456%27%2C+type%3Dstr%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--dist-backend%27%2C+default%3D%27mpi%27%2C+type%3Dstr%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--world_size%27%2C+default%3D1%2C+type%3Dint%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--distributed%27%2C+action%3D%27store_true%27%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++%23+Convolutional+%2B+pooling+part+of+the+model%0A++++class+ConvPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28ConvPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A%0A++++++++++return+x%0A%0A++++%23+Dense+feedforward+part+of+the+model%0A++++class+MLPPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28MLPPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+self.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A%0A++++++++++return+x%0A%0A++++ngpus_per_node+%3D+torch.cuda.device_count%28%29%0A++++local_rank+%3D+int%28os.environ.get%28%22SLURM_LOCALID%22%29%29%0A++++rank+%3D+int%28os.environ.get%28%22SLURM_NODEID%22%29%29%2A%28ngpus_per_node%2F%2F2%29+%2B+local_rank++%23+Divide+ngpus_per_node+by+the+number+of+model+parts%0A%0A++++os.environ%5B%27MASTER_ADDR%27%5D+%3D+%27127.0.0.1%27+%23+Each+model+replica+will+run+its+own+RPC+server+to+run+pipeline+parallelism%0A++++os.environ%5B%27MASTER_PORT%27%5D+%3D+str%2834567+%2B+local_rank%29+%23+Make+sure+each+RPC+server+starts+on+a+different+port%0A++++torch.distributed.rpc.init_rpc%28%27worker%27%2C+rank%3D0%2C+world_size%3D1%29+%23+Different+replicas+won%27t+communicate+through+RPC%2C+but+through+DDP%0A%0A++++dist.init_process_group%28backend%3Dargs.dist_backend%2C+init_method%3Dargs.init_method%2C+world_size%3Dargs.world_size%2C+rank%3Drank%29+%23+Initialize+Data+Parallelism+communications%0A%0A++++part1+%3D+ConvPart%28%29.cuda%28local_rank%29+%23+First+part+of+the+model+goes+on+the+first+GPU+of+each+process%0A++++part2+%3D+MLPPart%28%29.cuda%28local_rank+%2B+1%29+%23+Second+part+goes+on+the+second+GPU+of+each+process%0A%0A++++net+%3D+nn.Sequential%28part1%2Cpart2%29%0A%0A++++net+%3D+Pipe%28net%2C+chunks%3D32%2C+checkpoint%3D%22never%22%29%0A%0A++++net+%3D+torch.nn.parallel.DistributedDataParallel%28net%29%0A%0A++++criterion+%3D+nn.CrossEntropyLoss%28%29.cuda%28local_rank+%2B+1%29+%23+Loss+function+goes+on+the+second+GPU+of+each+process%0A++++optimizer+%3D+optim.SGD%28net.parameters%28%29%2C+lr%3Dargs.lr%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_sampler+%3D+torch.utils.data.distributed.DistributedSampler%28dataset_train%29%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+shuffle%3D%28train_sampler+is+None%29%2C+num_workers%3Dargs.num_workers%2C+sampler%3Dtrain_sampler%29%0A%0A%0A++++for+epoch+in+range%28args.max_epochs%29%3A%0A%0A++++++++train_sampler.set_epoch%28epoch%29%0A%0A++++++++train%28epoch%2C+net%2C+criterion%2C+optimizer%2C+train_loader%2C+rank%2C+local_rank%29%0A%0Adef+train%28epoch%2C+net%2C+criterion%2C+optimizer%2C+train_loader%2C+train_rank%2C+model_rank%29%3A%0A%0A++++train_loss+%3D+0%0A++++correct+%3D+0%0A++++total+%3D+0%0A%0A++++epoch_start+%3D+time.time%28%29%0A%0A++++for+batch_idx%2C+%28inputs%2C+targets%29+in+enumerate%28train_loader%29%3A%0A%0A++++++++start+%3D+time.time%28%29%0A%0A++++++++inputs+%3D+inputs.cuda%28model_rank%29%0A++++++++targets+%3D+targets.cuda%28model_rank+%2B+1%29%0A%0A++++++++outputs+%3D+net%28inputs%29.local_value%28%29%0A++++++++loss+%3D+criterion%28outputs%2C+targets%29%0A%0A++++++++optimizer.zero_grad%28%29%0A++++++++loss.backward%28%29%0A++++++++optimizer.step%28%29%0A++++++++print%28f%22From+Rank+%7Btrain_rank%7D+-+Loss%3A+%7Bloss.item%28%29%7D%22%29%0A%0A++++++++batch_time+%3D+time.time%28%29+-+start%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="pytorch-model-data-par.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.distributed.pipeline.sync</span> <span class="kn">import</span> <span class="n">Pipe</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.utils.data.distributed</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, distributed data &amp; model parallel test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--init_method&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;tcp://127.0.0.1:3456&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--dist-backend&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;mpi&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--world_size&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--distributed&#39;</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s1">&#39;store_true&#39;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="c1"># Convolutional + pooling part of the model</span>
    <span class="k">class</span> <span class="nc">ConvPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">ConvPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="c1"># Dense feedforward part of the model</span>
    <span class="k">class</span> <span class="nc">MLPPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">MLPPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="n">ngpus_per_node</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
    <span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;SLURM_LOCALID&quot;</span><span class="p">))</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;SLURM_NODEID&quot;</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="n">ngpus_per_node</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">local_rank</span>  <span class="c1"># Divide ngpus_per_node by the number of model parts</span>

    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_ADDR&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;127.0.0.1&#39;</span> <span class="c1"># Each model replica will run its own RPC server to run pipeline parallelism</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_PORT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="mi">34567</span> <span class="o">+</span> <span class="n">local_rank</span><span class="p">)</span> <span class="c1"># Make sure each RPC server starts on a different port</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">rpc</span><span class="o">.</span><span class="n">init_rpc</span><span class="p">(</span><span class="s1">&#39;worker&#39;</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Different replicas won&#39;t communicate through RPC, but through DDP</span>

    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">dist_backend</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">init_method</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">)</span> <span class="c1"># Initialize Data Parallelism communications</span>

    <span class="n">part1</span> <span class="o">=</span> <span class="n">ConvPart</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span> <span class="c1"># First part of the model goes on the first GPU of each process</span>
    <span class="n">part2</span> <span class="o">=</span> <span class="n">MLPPart</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">local_rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># Second part goes on the second GPU of each process</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">part1</span><span class="p">,</span><span class="n">part2</span><span class="p">)</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Pipe</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">chunks</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">checkpoint</span><span class="o">=</span><span class="s2">&quot;never&quot;</span><span class="p">)</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">local_rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># Loss function goes on the second GPU of each process</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="p">(</span><span class="n">train_sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>


    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">):</span>

        <span class="n">train_sampler</span><span class="o">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>

        <span class="n">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">local_rank</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">train_rank</span><span class="p">,</span> <span class="n">model_rank</span><span class="p">):</span>

    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">epoch_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model_rank</span><span class="p">)</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model_rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">local_value</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;From Rank </span><span class="si">{</span><span class="n">train_rank</span><span class="si">}</span><span class="s2"> - Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h3><span class="mw-headline" id="DeepSpeed">DeepSpeed</span></h3>
<p>DeepSpeed est une bibliothÃ¨que qui permet d'optimiser lâ€™entraÃ®nement de lâ€™apprentissage de modÃ¨les ayant des milliards de paramÃ¨tres Ã  lâ€™Ã©chelle. Parfaitement compatible avec PyTorch, DeepSpeed offre des implÃ©mentations de nouvelles mÃ©thodes dâ€™entraÃ®nement distribuÃ© qui font un usage efficace de la mÃ©moire en appliquant le concept de <a rel="nofollow" class="external text" href="https://arxiv.org/abs/1910.02054">Zero Redundancy Optimizer (ZeRO)</a>. Avec ZeRO, DeepSpeed peut distribuer le stockage et le traitement  des divers Ã©lÃ©ments dâ€™une tÃ¢che dâ€™entraÃ®nement (Ã©tats de lâ€™optimiseur, poids du modÃ¨le, gradients et activations) sur plusieurs dispositifs comme les GPU, les CPU, les disques durs locaux et/ou les combinaisons de ces dispositifs. Cette mise en commun des ressources, surtout les ressources de stockage, permet lâ€™entraÃ®nement efficace sur plusieurs nÅ“uds de modÃ¨les ayant dâ€™Ã©normes quantitÃ©s de paramÃ¨tres sans avoir besoin dâ€™Ã©crire le code pour parallÃ©liser le modÃ¨le, les pipelines ou les donnÃ©es. Les exemples qui suivent montrent comment profiter de DeepSpeed et des diffÃ©rentes implÃ©mentations de ZeRO en utilisant lâ€™interface simple de  PyTorch Lightning.
</p><p><span id="ZeRO_on_GPU"></span>
</p>
<h4><span class="mw-headline" id="ZeRO_avec_GPU">ZeRO avec GPU</span></h4>
<p>Dans lâ€™exemple ci-dessous, nous utilisons ZeRO stage 3 pour entraÃ®ner un modÃ¨le qui utilise un groupe de 4 GPU. Le stage 3  rÃ©partit sur les 4 GPU les trois caractÃ©ristiques, soit les Ã©tats de lâ€™optimiseur, les paramÃ¨tres du modÃ¨le et les gradients du modÃ¨le. Ceci est plus efficace que le parallÃ©lisme pur des donnÃ©es oÃ¹ une copie complÃ¨te du modÃ¨le est chargÃ©e sur chacun des GPU. Avec lâ€™optimiseur DeepSpeed <code>FusedAdam</code> plutÃ´t quâ€™un optimiseur natif de PyTorch, la performance se compare au parallÃ©lisme pur des donnÃ©es. Les optimiseurs DeepSpeed sont compilÃ©s en temps rÃ©el Ã  lâ€™exÃ©cution et vous devez charger un module <code>cuda/&lt;version&gt;</code> oÃ¹ <i>version</i> correspond Ã  la version utilisÃ©e pour construire le paquet PyTorch que vous utilisez.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> deepspeed-stage3.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1+++++++++++++%0A%23SBATCH+--gres%3Dgpu%3A2++++++++++%23+Request+2+GPU+%22generic+resources%E2%80%9D.%0A%23SBATCH+--tasks-per-node%3D2++++%23+Request+1+process+per+GPU.+You+will+get+1+CPU+per+process+by+default.+Request+more+CPUs+with+the+%22cpus-per-task%22+parameter+to+enable+multiple+data-loader+workers+to+load+data+in+parallel.%0A%23SBATCH+--mem%3D32G++++++%0A%23SBATCH+--time%3D0-00%3A20%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+python+cuda+%23+CUDA+must+be+loaded+if+using+a+DeepSpeed+optimizer%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torchvision+pytorch-lightning+deepspeed+--no-index%0A%0Aexport+TORCH_NCCL_ASYNC_HANDLING%3D1%0A%0A%23+PyTorch+Lightning+will+query+the+environment+to+figure+out+if+it+is+running+inside+a+SLURM+batch+job%0A%23+If+it+is%2C+it+expects+the+user+to+have+requested+one+task+per+GPU.%0A%23+If+you+do+not+ask+for+1+task+per+GPU%2C+and+you+do+not+run+your+script+with+%22srun%22%2C+your+job+will+fail%21%0A%0Asrun+python+deepspeed-stage3.py++--batch_size+256" />
<input type="hidden" name="filename" value="deepspeed-stage3.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1             </span>
<span class="c1">#SBATCH --gres=gpu:2          # Request 2 GPU &quot;generic resourcesâ€.</span>
<span class="c1">#SBATCH --tasks-per-node=2    # Request 1 process per GPU. You will get 1 CPU per process by default. Request more CPUs with the &quot;cpus-per-task&quot; parameter to enable multiple data-loader workers to load data in parallel.</span>
<span class="c1">#SBATCH --mem=32G      </span>
<span class="c1">#SBATCH --time=0-00:20</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span>cuda<span class="w"> </span><span class="c1"># CUDA must be loaded if using a DeepSpeed optimizer</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torchvision<span class="w"> </span>pytorch-lightning<span class="w"> </span>deepspeed<span class="w"> </span>--no-index

<span class="nb">export</span><span class="w"> </span><span class="nv">TORCH_NCCL_ASYNC_HANDLING</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># PyTorch Lightning will query the environment to figure out if it is running inside a SLURM batch job</span>
<span class="c1"># If it is, it expects the user to have requested one task per GPU.</span>
<span class="c1"># If you do not ask for 1 task per GPU, and you do not run your script with &quot;srun&quot;, your job will fail!</span>

srun<span class="w"> </span>python<span class="w"> </span>deepspeed-stage3.py<span class="w">  </span>--batch_size<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> deepspeed-stage3.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+torch%0Afrom+torch+import+nn%0Aimport+torch.nn.functional+as+F%0A%0Aimport+pytorch_lightning+as+pl%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Afrom+deepspeed.ops.adam+import+FusedAdam%0Afrom+pytorch_lightning.strategies+import+DeepSpeedStrategy%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models+deep+seed+stage+3+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--max_epochs%27%2C+type%3Dint%2C+default%3D2%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D768%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A++++print%28%22Starting...%22%29%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++class+ConvPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28ConvPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A%0A++++++++++return+x%0A%0A++++%23+Dense+feedforward+part+of+the+model%0A++++class+MLPPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28MLPPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+self.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A%0A++++++++++return+x%0A%0A++++class+Net%28pl.LightningModule%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv_part+%3D+ConvPart%28%29%0A++++++++++self.mlp_part+%3D+MLPPart%28%29%0A%0A+++++++def+configure_sharded_model%28self%29%3A%0A%0A++++++++++self.block+%3D+nn.Sequential%28self.conv_part%2C+self.mlp_part%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.block%28x%29%0A%0A++++++++++return+x%0A%0A+++++++def+training_step%28self%2C+batch%2C+batch_idx%29%3A%0A++++++++++x%2C+y+%3D+batch%0A++++++++++y_hat+%3D+self%28x%29%0A++++++++++loss+%3D+F.cross_entropy%28y_hat%2C+y%29%0A++++++++++return+loss%0A%0A+++++++def+configure_optimizers%28self%29%3A%0A++++++++++return+FusedAdam%28self.parameters%28%29%29%0A%0A++++net+%3D+Net%28%29%0A%0A++++%22%22%22+Here+we+initialize+a+Trainer%28%29+explicitly+with+1+node+and+2+GPU.%0A++++++++To+make+this+script+more+generic%2C+you+can+use+torch.cuda.device_count%28%29+to+set+the+number+of+GPUs%0A++++++++and+you+can+use+int%28os.environ.get%28%22SLURM_JOB_NUM_NODES%22%29%29+to+set+the+number+of+nodes.+%0A++++++++We+also+set+progress_bar_refresh_rate%3D0+to+avoid+writing+a+progress+bar+to+the+logs%2C+%0A++++++++which+can+cause+issues+due+to+updating+logs+too+frequently.%22%22%22%0A%0A++++trainer+%3D+pl.Trainer%28accelerator%3D%22gpu%22%2C+devices%3D2%2C+num_nodes%3D1%2C+strategy%3D%22deepspeed_stage_3%22%2C+max_epochs+%3D+args.max_epochs%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+num_workers%3Dargs.num_workers%29%0A%0A++++trainer.fit%28net%2Ctrain_loader%29%0A%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="deepspeed-stage3.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">from</span> <span class="nn">deepspeed.ops.adam</span> <span class="kn">import</span> <span class="n">FusedAdam</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DeepSpeedStrategy</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models deep seed stage 3 test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting...&quot;</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="k">class</span> <span class="nc">ConvPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">ConvPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="c1"># Dense feedforward part of the model</span>
    <span class="k">class</span> <span class="nc">MLPPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">MLPPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv_part</span> <span class="o">=</span> <span class="n">ConvPart</span><span class="p">()</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">mlp_part</span> <span class="o">=</span> <span class="n">MLPPart</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">configure_sharded_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">block</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_part</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_part</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

       <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
          <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
          <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">loss</span>

       <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">FusedAdam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; Here we initialize a Trainer() explicitly with 1 node and 2 GPU.</span>
<span class="sd">        To make this script more generic, you can use torch.cuda.device_count() to set the number of GPUs</span>
<span class="sd">        and you can use int(os.environ.get(&quot;SLURM_JOB_NUM_NODES&quot;)) to set the number of nodes. </span>
<span class="sd">        We also set progress_bar_refresh_rate=0 to avoid writing a progress bar to the logs, </span>
<span class="sd">        which can cause issues due to updating logs too frequently.&quot;&quot;&quot;</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_nodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;deepspeed_stage_3&quot;</span><span class="p">,</span> <span class="n">max_epochs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">)</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">net</span><span class="p">,</span><span class="n">train_loader</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p><p><span id="ZeRO_with_offload_to_CPU"></span>
</p>
<h4><span class="mw-headline" id="ZeRO_avec_CPU">ZeRO avec CPU</span></h4>
<p>Dans cet autre exemple, nous utilisons aussi ZeRO stage 3, mais cette fois-ci nous utilisons le CPU pour les Ã©tats de lâ€™optimiseur et les paramÃ¨tres du modÃ¨le.  Ceci signifie que la mÃ©moire du nÅ“ud de calcul sera disponible pour stocker ces tenseurs quand ils ne sont pas requis par les calculs effectuÃ©s par le GPU et de plus, les pas de lâ€™optimiseur seront calculÃ©s sur le CPU. Pour des raisons pratiques, ce serait comme ajouter 32Go de mÃ©moire additionnelle Ã  la mÃ©moire du GPU. Comme la mÃ©moire du GPU est moins sollicitÃ©e, vous pouvez par exemple augmenter la taille de vos lots ou de votre modÃ¨le. Avec lâ€™optimiseur DeepSpeed <code>DeepSpeedCPUAdam</code> plutÃ´t quâ€™un optimiseur natif de PyTorch, la performance se compare au parallÃ©lisme pur des donnÃ©es. Les optimiseurs DeepSpeed sont compilÃ©s en temps rÃ©el Ã  lâ€™exÃ©cution et vous devez charger un module <code>cuda/&lt;version&gt;</code> oÃ¹ <i>version</i> correspond Ã  la version utilisÃ©e pour construire le paquet PyTorch que vous utilisez.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> deepspeed-stage3-offload-cpu.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1+++++++++++++%0A%23SBATCH+--gres%3Dgpu%3A2++++++++++%23+Request+2+GPU+%22generic+resources%E2%80%9D.%0A%23SBATCH+--tasks-per-node%3D2++++%23+Request+1+process+per+GPU.+You+will+get+1+CPU+per+process+by+default.+Request+more+CPUs+with+the+%22cpus-per-task%22+parameter+to+enable+multiple+data-loader+workers+to+load+data+in+parallel.%0A%23SBATCH+--mem%3D32G++++++%0A%23SBATCH+--time%3D0-00%3A20%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+python+cuda+%23+CUDA+must+be+loaded+if+using+ZeRO+offloading+to+CPU+or+NVMe.+Version+must+be+the+same+used+to+compile+PyTorch.+%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torchvision+pytorch-lightning+deepspeed+--no-index%0A%0Aexport+TORCH_NCCL_ASYNC_HANDLING%3D1%0A%0A%23+PyTorch+Lightning+will+query+the+environment+to+figure+out+if+it+is+running+inside+a+SLURM+batch+job%0A%23+If+it+is%2C+it+expects+the+user+to+have+requested+one+task+per+GPU.%0A%23+If+you+do+not+ask+for+1+task+per+GPU%2C+and+you+do+not+run+your+script+with+%22srun%22%2C+your+job+will+fail%21%0A%0Asrun+python+deepspeed-stage3-offload-cpu.py++--batch_size+256" />
<input type="hidden" name="filename" value="deepspeed-stage3-offload-cpu.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1             </span>
<span class="c1">#SBATCH --gres=gpu:2          # Request 2 GPU &quot;generic resourcesâ€.</span>
<span class="c1">#SBATCH --tasks-per-node=2    # Request 1 process per GPU. You will get 1 CPU per process by default. Request more CPUs with the &quot;cpus-per-task&quot; parameter to enable multiple data-loader workers to load data in parallel.</span>
<span class="c1">#SBATCH --mem=32G      </span>
<span class="c1">#SBATCH --time=0-00:20</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span>cuda<span class="w"> </span><span class="c1"># CUDA must be loaded if using ZeRO offloading to CPU or NVMe. Version must be the same used to compile PyTorch. </span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torchvision<span class="w"> </span>pytorch-lightning<span class="w"> </span>deepspeed<span class="w"> </span>--no-index

<span class="nb">export</span><span class="w"> </span><span class="nv">TORCH_NCCL_ASYNC_HANDLING</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># PyTorch Lightning will query the environment to figure out if it is running inside a SLURM batch job</span>
<span class="c1"># If it is, it expects the user to have requested one task per GPU.</span>
<span class="c1"># If you do not ask for 1 task per GPU, and you do not run your script with &quot;srun&quot;, your job will fail!</span>

srun<span class="w"> </span>python<span class="w"> </span>deepspeed-stage3-offload-cpu.py<span class="w">  </span>--batch_size<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> deepspeed-stage3-offload-cpu.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+torch%0Afrom+torch+import+nn%0Aimport+torch.nn.functional+as+F%0A%0Aimport+pytorch_lightning+as+pl%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Afrom+deepspeed.ops.adam+import+DeepSpeedCPUAdam%0Afrom+pytorch_lightning.strategies+import+DeepSpeedStrategy%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+deepspeed+offload+to+cpu+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--max_epochs%27%2C+type%3Dint%2C+default%3D2%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D768%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A++++print%28%22Starting...%22%29%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++class+ConvPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28ConvPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A%0A++++++++++return+x%0A%0A++++%23+Dense+feedforward+part+of+the+model%0A++++class+MLPPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28MLPPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+self.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A%0A++++++++++return+x%0A%0A++++class+Net%28pl.LightningModule%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv_part+%3D+ConvPart%28%29%0A++++++++++self.mlp_part+%3D+MLPPart%28%29%0A%0A+++++++def+configure_sharded_model%28self%29%3A%0A%0A++++++++++self.block+%3D+nn.Sequential%28self.conv_part%2C+self.mlp_part%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.block%28x%29%0A%0A++++++++++return+x%0A%0A+++++++def+training_step%28self%2C+batch%2C+batch_idx%29%3A%0A++++++++++x%2C+y+%3D+batch%0A++++++++++y_hat+%3D+self%28x%29%0A++++++++++loss+%3D+F.cross_entropy%28y_hat%2C+y%29%0A++++++++++return+loss%0A%0A+++++++def+configure_optimizers%28self%29%3A%0A++++++++++return+DeepSpeedCPUAdam%28self.parameters%28%29%29%0A%0A++++net+%3D+Net%28%29%0A%0A++++%22%22%22+Here+we+initialize+a+Trainer%28%29+explicitly+with+1+node+and+2+GPU.%0A++++++++To+make+this+script+more+generic%2C+you+can+use+torch.cuda.device_count%28%29+to+set+the+number+of+GPUs%0A++++++++and+you+can+use+int%28os.environ.get%28%22SLURM_JOB_NUM_NODES%22%29%29+to+set+the+number+of+nodes.+%0A++++++++We+also+set+progress_bar_refresh_rate%3D0+to+avoid+writing+a+progress+bar+to+the+logs%2C+%0A++++++++which+can+cause+issues+due+to+updating+logs+too+frequently.%22%22%22%0A%0A++++trainer+%3D+pl.Trainer%28accelerator%3D%22gpu%22%2C+devices%3D2%2C+num_nodes%3D1%2C+strategy%3DDeepSpeedStrategy%28%0A++++++++stage%3D3%2C%0A++++++++offload_optimizer%3DTrue%2C%0A++++++++offload_parameters%3DTrue%2C%0A++++++++%29%2C+max_epochs+%3D+args.max_epochs%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+num_workers%3Dargs.num_workers%29%0A%0A++++trainer.fit%28net%2Ctrain_loader%29%0A%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="deepspeed-stage3-offload-cpu.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">from</span> <span class="nn">deepspeed.ops.adam</span> <span class="kn">import</span> <span class="n">DeepSpeedCPUAdam</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DeepSpeedStrategy</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, deepspeed offload to cpu test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting...&quot;</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="k">class</span> <span class="nc">ConvPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">ConvPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="c1"># Dense feedforward part of the model</span>
    <span class="k">class</span> <span class="nc">MLPPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">MLPPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv_part</span> <span class="o">=</span> <span class="n">ConvPart</span><span class="p">()</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">mlp_part</span> <span class="o">=</span> <span class="n">MLPPart</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">configure_sharded_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">block</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_part</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_part</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

       <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
          <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
          <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">loss</span>

       <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">DeepSpeedCPUAdam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; Here we initialize a Trainer() explicitly with 1 node and 2 GPU.</span>
<span class="sd">        To make this script more generic, you can use torch.cuda.device_count() to set the number of GPUs</span>
<span class="sd">        and you can use int(os.environ.get(&quot;SLURM_JOB_NUM_NODES&quot;)) to set the number of nodes. </span>
<span class="sd">        We also set progress_bar_refresh_rate=0 to avoid writing a progress bar to the logs, </span>
<span class="sd">        which can cause issues due to updating logs too frequently.&quot;&quot;&quot;</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_nodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="n">DeepSpeedStrategy</span><span class="p">(</span>
        <span class="n">stage</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">offload_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">offload_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">),</span> <span class="n">max_epochs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">)</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">net</span><span class="p">,</span><span class="n">train_loader</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h4><span class="mw-headline" id="ZeRO_avec_utilisation_de_disques_NVMe">ZeRO avec utilisation de disques NVMe</span></h4>
<p>ZeRO stage 3 nous sert encore, cette fois-ci en utilisant le CPU pour les Ã©tats de lâ€™optimiseur et les paramÃ¨tres du modÃ¨le. Ceci signifie que lâ€™espace de stockage local du nÅ“ud de calcul sera disponible pour stocker ces tenseurs quand ils ne sont pas requis par les calculs effectuÃ©s par le GPU. Ici encore,  les pas de lâ€™optimiseur seront calculÃ©s sur le CPU. De mÃªme, pour des raisons pratiques, ce serait comme ajouter Ã  la mÃ©moire du GPU autant dâ€™espace de stockage que sur le disque local, mais par contre nous aurons une forte perte de performance. Cette approche peut Ãªtre utilisÃ©e avec tous les types de stockage, mais elle est Ã  privilÃ©gier avec les disques NVMe qui sont plus rapides et ont des temps de rÃ©ponse plus courts, ce qui compense pour la baisse de performance.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> deepspeed-stage3-offload-nvme.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1+++++++++++++%0A%23SBATCH+--gres%3Dgpu%3A2++++++++++%23+Request+2+GPU+%22generic+resources%E2%80%9D.+%0A%23SBATCH+--tasks-per-node%3D2++++%23+Request+1+process+per+GPU.+You+will+get+1+CPU+per+process+by+default.+Request+more+CPUs+with+the+%22cpus-per-task%22+parameter+to+enable+multiple+data-loader+workers+to+load+data+in+parallel.%0A%23SBATCH+--mem%3D32G++++++%0A%23SBATCH+--time%3D0-00%3A20%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+python+cuda+%23+CUDA+must+be+loaded+if+using+ZeRO+offloading+to+CPU+or+NVMe.+Version+must+be+the+same+used+to+compile+PyTorch.+%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torchvision+pytorch-lightning+deepspeed+--no-index%0A%0Aexport+TORCH_NCCL_ASYNC_HANDLING%3D1%0A%0A%23+PyTorch+Lightning+will+query+the+environment+to+figure+out+if+it+is+running+inside+a+SLURM+batch+job%0A%23+If+it+is%2C+it+expects+the+user+to+have+requested+one+task+per+GPU.%0A%23+If+you+do+not+ask+for+1+task+per+GPU%2C+and+you+do+not+run+your+script+with+%22srun%22%2C+your+job+will+fail%21%0A%0Asrun+python+deepspeed-stage3-offload-nvme.py++--batch_size+256" />
<input type="hidden" name="filename" value="deepspeed-stage3-offload-nvme.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1             </span>
<span class="c1">#SBATCH --gres=gpu:2          # Request 2 GPU &quot;generic resourcesâ€. </span>
<span class="c1">#SBATCH --tasks-per-node=2    # Request 1 process per GPU. You will get 1 CPU per process by default. Request more CPUs with the &quot;cpus-per-task&quot; parameter to enable multiple data-loader workers to load data in parallel.</span>
<span class="c1">#SBATCH --mem=32G      </span>
<span class="c1">#SBATCH --time=0-00:20</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span>cuda<span class="w"> </span><span class="c1"># CUDA must be loaded if using ZeRO offloading to CPU or NVMe. Version must be the same used to compile PyTorch. </span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torchvision<span class="w"> </span>pytorch-lightning<span class="w"> </span>deepspeed<span class="w"> </span>--no-index

<span class="nb">export</span><span class="w"> </span><span class="nv">TORCH_NCCL_ASYNC_HANDLING</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># PyTorch Lightning will query the environment to figure out if it is running inside a SLURM batch job</span>
<span class="c1"># If it is, it expects the user to have requested one task per GPU.</span>
<span class="c1"># If you do not ask for 1 task per GPU, and you do not run your script with &quot;srun&quot;, your job will fail!</span>

srun<span class="w"> </span>python<span class="w"> </span>deepspeed-stage3-offload-nvme.py<span class="w">  </span>--batch_size<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> deepspeed-stage3-offload-nvme.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+os%0A%0Aimport+torch%0Afrom+torch+import+nn%0Aimport+torch.nn.functional+as+F%0A%0Aimport+pytorch_lightning+as+pl%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Afrom+deepspeed.ops.adam+import+DeepSpeedCPUAdam%0Afrom+pytorch_lightning.strategies+import+DeepSpeedStrategy%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+deepspeed+offload+to+nvme+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--max_epochs%27%2C+type%3Dint%2C+default%3D2%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D768%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A++++print%28%22Starting...%22%29%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++class+ConvPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28ConvPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A%0A++++++++++return+x%0A%0A++++%23+Dense+feedforward+part+of+the+model%0A++++class+MLPPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28MLPPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+self.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A%0A++++++++++return+x%0A%0A++++class+Net%28pl.LightningModule%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv_part+%3D+ConvPart%28%29%0A++++++++++self.mlp_part+%3D+MLPPart%28%29%0A%0A+++++++def+configure_sharded_model%28self%29%3A%0A%0A++++++++++self.block+%3D+nn.Sequential%28self.conv_part%2C+self.mlp_part%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.block%28x%29%0A%0A++++++++++return+x%0A%0A+++++++def+training_step%28self%2C+batch%2C+batch_idx%29%3A%0A++++++++++x%2C+y+%3D+batch%0A++++++++++y_hat+%3D+self%28x%29%0A++++++++++loss+%3D+F.cross_entropy%28y_hat%2C+y%29%0A++++++++++return+loss%0A%0A+++++++def+configure_optimizers%28self%29%3A%0A++++++++++return+DeepSpeedCPUAdam%28self.parameters%28%29%29%0A%0A++++net+%3D+Net%28%29%0A%0A++++%22%22%22+Here+we+initialize+a+Trainer%28%29+explicitly+with+1+node+and+2+GPU.%0A++++++++To+make+this+script+more+generic%2C+you+can+use+torch.cuda.device_count%28%29+to+set+the+number+of+GPUs%0A++++++++and+you+can+use+int%28os.environ.get%28%22SLURM_JOB_NUM_NODES%22%29%29+to+set+the+number+of+nodes.+%0A++++++++We+also+set+progress_bar_refresh_rate%3D0+to+avoid+writing+a+progress+bar+to+the+logs%2C+%0A++++++++which+can+cause+issues+due+to+updating+logs+too+frequently.%22%22%22%0A%0A++++local_scratch+%3D+os.environ%5B%27SLURM_TMPDIR%27%5D+%23+Get+path+where+local+storage+is+mounted%0A%0A++++print%28f%27Offloading+to%3A+%7Blocal_scratch%7D%27%29%0A%0A++++trainer+%3D+pl.Trainer%28accelerator%3D%22gpu%22%2C+devices%3D2%2C+num_nodes%3D1%2C+strategy%3DDeepSpeedStrategy%28%0A++++++++stage%3D3%2C%0A++++++++offload_optimizer%3DTrue%2C%0A++++++++offload_parameters%3DTrue%2C%0A++++++++remote_device%3D%22nvme%22%2C%0A++++++++offload_params_device%3D%22nvme%22%2C%0A++++++++offload_optimizer_device%3D%22nvme%22%2C%0A++++++++nvme_path%3D%22local_scratch%22%2C%0A++++++++%29%2C+max_epochs+%3D+args.max_epochs%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+num_workers%3Dargs.num_workers%29%0A%0A++++trainer.fit%28net%2Ctrain_loader%29%0A%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="deepspeed-stage3-offload-nvme.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">from</span> <span class="nn">deepspeed.ops.adam</span> <span class="kn">import</span> <span class="n">DeepSpeedCPUAdam</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DeepSpeedStrategy</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, deepspeed offload to nvme test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting...&quot;</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="k">class</span> <span class="nc">ConvPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">ConvPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="c1"># Dense feedforward part of the model</span>
    <span class="k">class</span> <span class="nc">MLPPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">MLPPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv_part</span> <span class="o">=</span> <span class="n">ConvPart</span><span class="p">()</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">mlp_part</span> <span class="o">=</span> <span class="n">MLPPart</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">configure_sharded_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">block</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_part</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_part</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

       <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
          <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
          <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">loss</span>

       <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">DeepSpeedCPUAdam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; Here we initialize a Trainer() explicitly with 1 node and 2 GPU.</span>
<span class="sd">        To make this script more generic, you can use torch.cuda.device_count() to set the number of GPUs</span>
<span class="sd">        and you can use int(os.environ.get(&quot;SLURM_JOB_NUM_NODES&quot;)) to set the number of nodes. </span>
<span class="sd">        We also set progress_bar_refresh_rate=0 to avoid writing a progress bar to the logs, </span>
<span class="sd">        which can cause issues due to updating logs too frequently.&quot;&quot;&quot;</span>

    <span class="n">local_scratch</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;SLURM_TMPDIR&#39;</span><span class="p">]</span> <span class="c1"># Get path where local storage is mounted</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Offloading to: </span><span class="si">{</span><span class="n">local_scratch</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_nodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="n">DeepSpeedStrategy</span><span class="p">(</span>
        <span class="n">stage</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">offload_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">offload_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">remote_device</span><span class="o">=</span><span class="s2">&quot;nvme&quot;</span><span class="p">,</span>
        <span class="n">offload_params_device</span><span class="o">=</span><span class="s2">&quot;nvme&quot;</span><span class="p">,</span>
        <span class="n">offload_optimizer_device</span><span class="o">=</span><span class="s2">&quot;nvme&quot;</span><span class="p">,</span>
        <span class="n">nvme_path</span><span class="o">=</span><span class="s2">&quot;local_scratch&quot;</span><span class="p">,</span>
        <span class="p">),</span> <span class="n">max_epochs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">)</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">net</span><span class="p">,</span><span class="n">train_loader</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h1><span id="Cr.C3.A9er_des_points_de_contr.C3.B4le"></span><span class="mw-headline" id="CrÃ©er_des_points_de_contrÃ´le">CrÃ©er des points de contrÃ´le</span></h1>
<p>Peu importe si vous pensez que la durÃ©e d'exÃ©cution de votre code sera longue ou non, il est bon de prendre l'habitude de crÃ©er des points de contrÃ´le pendant l'entraÃ®nement. Un point de contrÃ´le est un portrait de votre modÃ¨le Ã  un moment prÃ©cis du processus d'entraÃ®nement (aprÃ¨s un certain nombre d'itÃ©rations ou un certain nombre d'Ã©poques) que vous pouvez sauvegarder sur disque et utiliser plus tard. C'est un moyen pratique de diviser les tÃ¢ches qui devraient Ãªtre de longue durÃ©e en de multiples petites tÃ¢ches auxquelles l'ordonnanceur peut allouer des ressources plus rapidement. C'est aussi une bonne faÃ§on de ne pas perdre le progrÃ¨s rÃ©alisÃ© au cas oÃ¹ des erreurs de code inattendues surviendraient ou que les nÅ“uds ne soient pas disponibles pour quelconque raison.
</p><p><span id="With_PyTorch_Lightning"></span>
</p>
<h2><span class="mw-headline" id="Avec_PyTorch_Lightning">Avec PyTorch Lightning</span></h2>
<p>Nous recommandons d'utiliser le paramÃ¨tre de rappels (<i>callbacks parameter</i>) de la classe <code>Trainer()</code>. Dans l'exemple suivant, on demande Ã  PyTorch de crÃ©er un point de contrÃ´le Ã  la fin de chacune des Ã©poques d'entraÃ®nement. VÃ©rifiez que le chemin oÃ¹ crÃ©er le point de contrÃ´le existe.
</p>
<pre>callbacks = [pl.callbacks.ModelCheckpoint(dirpath="./ckpt",every_n_epochs=1)]
trainer = pl.Trainer(callbacks=callbacks) 
trainer.fit(model)
</pre>
<p>Ce bout de code chargera un point de contrÃ´le de <code>./ckpt</code> (s'il en existe) et poursuivra l'entraÃ®nement Ã  partir de ce point. Pour plus d'information, consultez la <a rel="nofollow" class="external text" href="https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.callbacks.model_checkpoint.html">documentation PyTorch Lightning</a>.
</p><p><span id="With_custom_training_loops"></span>
</p>
<h2><span id="Avec_des_boucles_d.27entra.C3.AEnement_personnalis.C3.A9es"></span><span class="mw-headline" id="Avec_des_boucles_d'entraÃ®nement_personnalisÃ©es">Avec des boucles d'entraÃ®nement personnalisÃ©es</span></h2>
<p>Pour des exemples, consultez <a rel="nofollow" class="external text" href="https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html">la documentation PyTorch</a>.
</p><p><span id="During_distributed_training"></span>
</p>
<h2><span id="Pendant_l.E2.80.99entra.C3.AEnement_distribu.C3.A9"></span><span class="mw-headline" id="Pendant_lâ€™entraÃ®nement_distribuÃ©">Pendant lâ€™entraÃ®nement distribuÃ©</span></h2>
<p>Les points de contrÃ´le peuvent Ãªtre utilisÃ©s pendant lâ€™exÃ©cution dâ€™un programme dâ€™entraÃ®nement distribuÃ©. Avec PyTorch Lightning, aucun code supplÃ©mentaire nâ€™est requis, autre que dâ€™insÃ©rer le paramÃ¨tre de rappels (<i>callbacks parameter</i>) mentionnÃ© ci-dessus. Cependant, si vous utilisez  DistributedDataParallel ou Horovod, les points de contrÃ´le devront Ãªtre crÃ©Ã©s par un seul processus (<i>rank</i>) de votre programme puisque tous les processus auront le mÃªme Ã©tat aprÃ¨s chaque itÃ©ration. Dans cet exemple, le premier processus (<i>rank 0</i>) crÃ©e un point de contrÃ´le.
</p>
<pre>if global_rank == 0:
       torch.save(ddp_model.state_dict(), "./checkpoint_path")
</pre>
<p>Faites attention aux points de contrÃ´le ainsi crÃ©Ã©s. Si un processus tente de charger un point de contrÃ´le qui nâ€™a pas encore Ã©tÃ© sauvegardÃ© par un autre, des erreurs peuvent survenir ou de mauvais rÃ©sultats peuvent Ãªtre produits. Pour Ã©viter ceci, vous pouvez ajouter une barriÃ¨re Ã  votre code pour faire en sorte que le processus qui crÃ©e le point de contrÃ´le a terminÃ© son Ã©criture sur le disque avant que dâ€™autres processus tentent de le charger. Remarquez aussi que <code>torch.load</code> essaiera par dÃ©faut de charger les tenseurs sur le GPU sur lequel ils Ã©taient initialement sauvegardÃ©s, dans notre cas <code>cuda:0</code>. Pour Ã©viter les problÃ¨mes, passez <code>map_location</code> Ã  <code>torch.load</code> pour charger les tenseurs sur le GPU identifiÃ© par chaque processus.
</p>
<pre>torch.distributed.barrier()
map_location = f"cuda:{local_rank}"  
ddp_model.load_state_dict(
torch.load("./checkpoint_path", map_location=map_location))
</pre>
<p><br />
<span id="Troubleshooting"></span>
</p>
<h1><span id="D.C3.A9pannage"></span><span class="mw-headline" id="DÃ©pannage">DÃ©pannage</span></h1>
<h2><span id="Fuites_de_m.C3.A9moire"></span><span class="mw-headline" id="Fuites_de_mÃ©moire">Fuites de mÃ©moire</span></h2>
<p>Sur le matÃ©riel AVX512 (nÅ“uds V100, Skylake ou BÃ©luga), les versions PyTorch  antÃ©rieures Ã  v1.0.1 qui utilisent des bibliothÃ¨ques moins rÃ©centes (cuDNN &lt; v7.5 ou MAGMA &lt; v2.5) peuvent avoir des fuites de mÃ©moire importantes et crÃ©er des exceptions de mÃ©moire insuffisante et terminer vos tÃ¢ches. Pour contrer ceci, utilisez la plus rÃ©cente version de <code>torch</code>.
</p>
<h2><span class="mw-headline" id="c10::Error">c10::Error</span></h2>
<p>Dans certains cas, vous pouvez obtenir un erreur comme
</p>
<pre> terminate called after throwing an instance of 'c10::Error'
   what():  Given groups=1, weight of size [256, 1, 3, 3], expected input[16, 10, 16, 16] to have 1 channels, but got 10 channels instead
 Exception raised from check_shape_forward at /tmp/coulombc/pytorch_build_2021-11-09_14-57-01/avx2/python3.8/pytorch/aten/src/ATen/native/Convolution.cpp:496 (most recent call first):
 ...
</pre>
<p>Une exception C++ est Ã©mise plutÃ´t qu'une exception Python. Ceci peut se produire quand vous programmez en C++ avec libtorch, mais ne devrait pas se produire quand vous programmez en Python. Il n'est pas possible de suivre la trace des appels (<i>traceback</i>)du programme Python, ce qui ne permet pas d'identifier facilement la cause de l'erreur dans le script Python. Nous avons constatÃ© que le fait d'utiliser PyTorch 1.9.1 plutÃ´t que 1.10.x permet le <i>traceback</i> du programme Python.
</p><p><span id="CUDA_error:_no_kernel_image_is_available_for_execution_on_the_device"></span>
</p>
<h2><span class="mw-headline" id="Erreur_CUDA_:_no_kernel_image_is_available_for_execution_on_the_device">Erreur CUDA&#160;: no kernel image is available for execution on the device</span></h2>
<p>Cette exception signifie que l'installation courante de Torch ne prend pas en charge l'architecture de calcul ou le GPU utilisÃ©.
Vous pouvez installer une version plus rÃ©cente de <tt>torch</tt> ou demander un GPU compatible avec la version que vous utilisez.
</p>
<h1><span class="mw-headline" id="LibTorch">LibTorch</span></h1>
<p>LibTorch permet d'implÃ©menter Ã  PyTorch des extensions C++ et des <b>applications d'apprentissage machine en C++ pur</b>.  La distribution LibTorch possÃ¨de les en-tÃªtes, bibliothÃ¨ques et fichiers de configuration CMake nÃ©cessaires pour travailler avec PyTorch, tel que dÃ©crit dans la <a rel="nofollow" class="external text" href="https://pytorch.org/cppdocs/installing.html">documentation</a>.
</p><p><span id="How_to_use_LibTorch"></span>
</p>
<h3><span class="mw-headline" id="Utiliser_LibTorch">Utiliser LibTorch</span></h3>
<p><span id="Setting_up_the_environment"></span>
</p>
<h4><span id="Configurer_l.27environnement"></span><span class="mw-headline" id="Configurer_l'environnement">Configurer l'environnement</span></h4>
<p>Chargez les modules requis par LibTorch, puis installez PyTorch dans un environnement virtuel Python.
</p>
<form id="tabs-inputform" class="tabs tabs-inputform" action="#"></form><div class="tabs tabs-tabbox"><input type="radio" form="tabs-inputform" id="tabs-input-1-0" name="tabs-1" class="tabs-input tabs-input-0" checked="" /><input type="radio" form="tabs-inputform" id="tabs-input-1-1" name="tabs-1" class="tabs-input tabs-input-1" /><label class="tabs-label" for="tabs-input-1-1" data-tabpos="1">StdEnv/2023</label><wbr /><input type="radio" form="tabs-inputform" id="tabs-input-1-2" name="tabs-1" class="tabs-input tabs-input-2" /><label class="tabs-label" for="tabs-input-1-2" data-tabpos="2">StdEnv/2020</label><wbr /><div class="tabs-container" style="">
<div class="tabs-content tabs-content-1">
<pre>module load StdEnv/2023 gcc cuda/12.2 cmake protobuf cudnn python/3.11 abseil  cusparselt  opencv/4.8.1
virtualenv --no-download --clear ~/ENV &amp;&amp; source ~/ENV/bin/activate 
pip install --no-index torch numpy 
</pre>
<p>Vous devrez peut-Ãªtre ajuster les versions des modules abseil, cusparselt et opencv, dÃ©pendant du paquet torch que vous utilisez. Pour savoir quelle version d'un module a Ã©tÃ© utilisÃ©e pour compiler le wheel Python, lancez la commande
</p>
<div>
<div style="float:right; margin-left:8px">
<p><span typeof="mw:File"><a href="https://explainshell.com/explain?cmd=ldd+%24VIRTUAL_ENV%2Flib%2Fpython3.11%2Fsite-packages%2Ftorch%2Flib%2Flibtorch_cuda.so+%7C+sed+-n+%27s%26%5E.%2A%2F%5C%28%5C%28opencv%5C%7Cabseil%5C%7Ccusparselt%5C%29%2F%5B%5E%2F%5D%2A%5C%29.%2A%26%5C1%26p%27+%7C+sort+-u" rel="nofollow"><img src="/mediawiki/images/thumb/3/30/Question.png/40px-Question.png" decoding="async" width="40" height="40" class="mw-file-element" srcset="/mediawiki/images/thumb/3/30/Question.png/60px-Question.png 1.5x, /mediawiki/images/thumb/3/30/Question.png/80px-Question.png 2x" /></a></span>
</p>
</div>
<div class="command">
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span>$<span class="w"> </span>ldd<span class="w"> </span><span class="nv">$VIRTUAL_ENV</span>/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so<span class="w"> </span><span class="p">|</span><span class="w"> </span>sed<span class="w"> </span>-n<span class="w"> </span><span class="s1">&#39;s&amp;^.*/\(\(opencv\|abseil\|cusparselt\)/[^/]*\).*&amp;\1&amp;p&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>sort<span class="w"> </span>-u
abseil/20230125.3
cusparselt/0.5.0.1
opencv/4.8.1
</pre></div>
</div>
</div>
</div>
<div class="tabs-content tabs-content-2">
<pre>module load gcc cuda/11.4 cmake protobuf cudnn python/3.10
virtualenv --no-download --clear ~/ENV &amp;&amp; source ~/ENV/bin/activate 
pip install --no-index torch numpy 
</pre>
</div>
</div></div>
<p><span id="Compiling_a_minimal_example"></span>
</p>
<h4><span class="mw-headline" id="Compiler_un_exemple_simple">Compiler un exemple simple</span></h4>
<p>CrÃ©ez les deux fichiers suivants&#160;:
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> example.cpp</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23include+%3Ctorch%2Ftorch.h%3E%0A%23include+%3Ciostream%3E%0A%0Aint+main%28%29+%0A%7B%0A++++torch%3A%3ADevice+device%28torch%3A%3AkCPU%29%3B%0A++++if+%28torch%3A%3Acuda%3A%3Ais_available%28%29%29+%0A++++%7B%0A++++++++std%3A%3Acout+%3C%3C+%22CUDA+is+available%21+Using+GPU.%22+%3C%3C+std%3A%3Aendl%3B%0A++++++++device+%3D+torch%3A%3ADevice%28torch%3A%3AkCUDA%29%3B%0A++++%7D%0A%0A++++torch%3A%3ATensor+tensor+%3D+torch%3A%3Arand%28%7B2%2C+3%7D%29.to%28device%29%3B%0A++++std%3A%3Acout+%3C%3C+tensor+%3C%3C+std%3A%3Aendl%3B%0A%7D" />
<input type="hidden" name="filename" value="example.cpp" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-cpp mw-content-ltr" dir="ltr"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/torch.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;iostream&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span>
<span class="p">{</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Device</span><span class="w"> </span><span class="n">device</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kCPU</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">is_available</span><span class="p">())</span><span class="w"> </span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;CUDA is available! Using GPU.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">        </span><span class="n">device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">Device</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">rand</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">}).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">);</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> CMakeLists.txt</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="cmake_minimum_required%28VERSION+3.0+FATAL_ERROR%29%0Aproject%28example%29%0A%0Afind_package%28Torch+REQUIRED%29%0A%0Aadd_executable%28example+example.cpp%29%0Atarget_link_libraries%28example+%22%24%7BTORCH_LIBRARIES%7D%22%29%0Aset_property%28TARGET+example+PROPERTY+CXX_STANDARD+14%29" />
<input type="hidden" name="filename" value="CMakeLists.txt" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-content-ltr" dir="ltr"><pre>cmake_minimum_required(VERSION 3.0 FATAL_ERROR)
project(example)

find_package(Torch REQUIRED)

add_executable(example example.cpp)
target_link_libraries(example "${TORCH_LIBRARIES}")
set_property(TARGET example PROPERTY CXX_STANDARD 14)</pre></div>
</div>
<p><br />
</p><p>Activez l'environnement virtuel Python, configurez le projet et compilez le programme.
</p>
<div class="tabs tabs-tabbox"><input type="radio" form="tabs-inputform" id="tabs-input-2-0" name="tabs-2" class="tabs-input tabs-input-0" checked="" /><input type="radio" form="tabs-inputform" id="tabs-input-2-1" name="tabs-2" class="tabs-input tabs-input-1" /><label class="tabs-label" for="tabs-input-2-1" data-tabpos="1">StdEnv/2023</label><wbr /><input type="radio" form="tabs-inputform" id="tabs-input-2-2" name="tabs-2" class="tabs-input tabs-input-2" /><label class="tabs-label" for="tabs-input-2-2" data-tabpos="2">StdEnv/2020</label><wbr /><div class="tabs-container" style="">
<div class="tabs-content tabs-content-1">
<pre>cmake -B build -S . -DCMAKE_PREFIX_PATH=$VIRTUAL_ENV/lib/python3.11/site-packages \
                    -DCMAKE_EXE_LINKER_FLAGS=-Wl,-rpath=$VIRTUAL_ENV/lib/python3.11/site-packages/torch/lib,-L$EBROOTCUDA/extras/CUPTI/lib64 \
                    -DCMAKE_SKIP_RPATH=ON -DTORCH_CUDA_ARCH_LIST="6.0;7.0;7.5;8.0;9.0"
cmake --build build
</pre>
</div>
<div class="tabs-content tabs-content-2">
<pre>cmake -B build -S . -DCMAKE_PREFIX_PATH=$VIRTUAL_ENV/lib/python3.10/site-packages \
                    -DCMAKE_EXE_LINKER_FLAGS=-Wl,-rpath=$VIRTUAL_ENV/lib/python3.10/site-packages/torch/lib \
                    -DCMAKE_SKIP_RPATH=ON
cmake --build build 
</pre>
</div>
</div></div>
<p>Lancez le programme avec
</p>
<pre>build/example
</pre>
<p>Pour tester une application avec CUDA, demandez une <a href="/wiki/Running_jobs/fr#TÃ¢ches_interactives" title="Running jobs/fr">tÃ¢che interactive</a> avec <a href="/wiki/Using_GPUs_with_Slurm/fr" title="Using GPUs with Slurm/fr">GPU</a>.
</p><p><span id="Resources"></span>
</p>
<h1><span class="mw-headline" id="Ressources">Ressources</span></h1>
<p><a rel="nofollow" class="external free" href="https://pytorch.org/cppdocs/">https://pytorch.org/cppdocs/</a>
</p>
<!-- 
NewPP limit report
Cached time: 20250530235248
Cache expiry: 86400
Reduced expiry: false
Complications: [showâ€toc]
CPU time usage: 0.319 seconds
Real time usage: 3.841 seconds
Preprocessor visited node count: 1652/1000000
Postâ€expand include size: 70248/2097152 bytes
Template argument size: 100251/2097152 bytes
Highest expansion depth: 7/100
Expensive parser function count: 30/100
Unstrip recursion depth: 2/20
Unstrip postâ€expand size: 310342/5000000 bytes
ExtLoops count: 0/100
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00% 3635.117      1 -total
 85.31% 3100.989     26 Template:File
 14.32%  520.401      5 Template:Command
  0.02%    0.565      1 Template:Note
-->

<!-- Saved in parser cache with key ccwiki:pcache:idhash:4699-0!canonical and timestamp 20250530235248 and revision id 175858. Rendering was triggered because: page-view
 -->
</div>
<div class="printfooter" data-nosnippet="">Retrieved from "<a dir="ltr" href="https://docs.alliancecan.ca/mediawiki/index.php?title=PyTorch/fr&amp;oldid=175858">https://docs.alliancecan.ca/mediawiki/index.php?title=PyTorch/fr&amp;oldid=175858</a>"</div></div>
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>: <ul><li><a href="/mediawiki/index.php?title=Category:Pages_with_syntax_highlighting_errors&amp;action=edit&amp;redlink=1" class="new" title="Category:Pages with syntax highlighting errors (page does not exist)">Pages with syntax highlighting errors</a></li><li><a href="/wiki/Category:Software" title="Category:Software">Software</a></li><li><a href="/wiki/Category:AI_and_Machine_Learning" title="Category:AI and Machine Learning">AI and Machine Learning</a></li></ul></div></div>
	</div>
</div>

<div id="mw-navigation">
	<h2>Navigation menu</h2>
	<div id="mw-head">
		
<nav id="p-personal" class="mw-portlet mw-portlet-personal vector-user-menu-legacy vector-menu" aria-labelledby="p-personal-label"  >
	<h3
		id="p-personal-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Personal tools</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="pt-uls" class="mw-list-item active"><a class="uls-trigger" href="#"><span>English</span></a></li><li id="pt-login" class="mw-list-item"><a href="/mediawiki/index.php?title=Special:UserLogin&amp;returnto=PyTorch%2Ffr" title="You are encouraged to log in; however, it is not mandatory [o]" accesskey="o"><span>Log in</span></a></li>
		</ul>
		
	</div>
</nav>

		<div id="left-navigation">
			
<nav id="p-namespaces" class="mw-portlet mw-portlet-namespaces vector-menu-tabs vector-menu-tabs-legacy vector-menu" aria-labelledby="p-namespaces-label"  >
	<h3
		id="p-namespaces-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Namespaces</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-nstab-main" class="selected mw-list-item"><a href="/wiki/PyTorch/fr" title="View the content page [c]" accesskey="c"><span>Page</span></a></li><li id="ca-talk" class="new mw-list-item"><a href="/mediawiki/index.php?title=Talk:PyTorch/fr&amp;action=edit&amp;redlink=1" rel="discussion" class="new" title="Discussion about the content page (page does not exist) [t]" accesskey="t"><span>Discussion</span></a></li>
		</ul>
		
	</div>
</nav>

			
<nav id="p-variants" class="mw-portlet mw-portlet-variants emptyPortlet vector-menu-dropdown vector-menu" aria-labelledby="p-variants-label"  >
	<input type="checkbox"
		id="p-variants-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-p-variants"
		class="vector-menu-checkbox"
		aria-labelledby="p-variants-label"
	>
	<label
		id="p-variants-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">franÃ§ais</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</nav>

		</div>
		<div id="right-navigation">
			
<nav id="p-views" class="mw-portlet mw-portlet-views vector-menu-tabs vector-menu-tabs-legacy vector-menu" aria-labelledby="p-views-label"  >
	<h3
		id="p-views-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Views</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-view" class="selected mw-list-item"><a href="/wiki/PyTorch/fr"><span>Read</span></a></li><li id="ca-viewsource" class="mw-list-item"><a href="/mediawiki/index.php?title=PyTorch/fr&amp;action=edit" title="This page is protected.&#10;You can view its source [e]" accesskey="e"><span>View source</span></a></li><li id="ca-history" class="mw-list-item"><a href="/mediawiki/index.php?title=PyTorch/fr&amp;action=history" title="Past revisions of this page [h]" accesskey="h"><span>View history</span></a></li>
		</ul>
		
	</div>
</nav>

			
<nav id="p-cactions" class="mw-portlet mw-portlet-cactions emptyPortlet vector-menu-dropdown vector-menu" aria-labelledby="p-cactions-label"  title="More options" >
	<input type="checkbox"
		id="p-cactions-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-p-cactions"
		class="vector-menu-checkbox"
		aria-labelledby="p-cactions-label"
	>
	<label
		id="p-cactions-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">More</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</nav>

			
<div id="p-search" role="search" class="vector-search-box-vue  vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box">
	<h3 >Search</h3>
	<form action="/mediawiki/index.php" id="searchform" class="vector-search-box-form">
		<div id="simpleSearch"
			class="vector-search-box-inner"
			 data-search-loc="header-navigation">
			<input class="vector-search-box-input"
				 type="search" name="search" placeholder="Search Alliance Doc" aria-label="Search Alliance Doc" autocapitalize="sentences" title="Search Alliance Doc [f]" accesskey="f" id="searchInput"
			>
			<input type="hidden" name="title" value="Special:Search">
			<input id="mw-searchButton"
				 class="searchButton mw-fallbackSearchButton" type="submit" name="fulltext" title="Search the pages for this text" value="Search">
			<input id="searchButton"
				 class="searchButton" type="submit" name="go" title="Go to a page with this exact name if it exists" value="Go">
		</div>
	</form>
</div>

		</div>
	</div>
	
<div id="mw-panel" class="vector-legacy-sidebar">
	<div id="p-logo" role="banner">
		<a class="mw-wiki-logo" href="/wiki/Technical_documentation"
			title="Visit the main page"></a>
	</div>
	
<nav id="p-navigation" class="mw-portlet mw-portlet-navigation vector-menu-portal portal vector-menu" aria-labelledby="p-navigation-label"  >
	<h3
		id="p-navigation-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Navigation</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-sidebar-wiki-main-page" class="mw-list-item"><a href="/wiki/Technical_documentation"><span>Wiki Main Page</span></a></li>
		</ul>
		
	</div>
</nav>

	
<nav id="p-sidebar-support" class="mw-portlet mw-portlet-sidebar-support vector-menu-portal portal vector-menu" aria-labelledby="p-sidebar-support-label"  >
	<h3
		id="p-sidebar-support-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Support</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-sidebar-getting-started" class="mw-list-item"><a href="/wiki/Getting_started"><span>Getting started</span></a></li><li id="n-sidebar-technical-support" class="mw-list-item"><a href="/wiki/Technical_support"><span>Getting help</span></a></li><li id="n-sidebar-running-jobs" class="mw-list-item"><a href="/wiki/Running_jobs"><span>Running jobs</span></a></li><li id="n-sidebar-known-issues" class="mw-list-item"><a href="/wiki/Known_issues"><span>Known issues</span></a></li><li id="n-sidebar-system-status" class="mw-list-item"><a href="http://status.computecanada.ca" rel="nofollow"><span>System status</span></a></li>
		</ul>
		
	</div>
</nav>

<nav id="p-sidebar-resources" class="mw-portlet mw-portlet-sidebar-resources vector-menu-portal portal vector-menu" aria-labelledby="p-sidebar-resources-label"  >
	<h3
		id="p-sidebar-resources-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Resources</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-BÃ©luga" class="mw-list-item"><a href="/wiki/B%C3%A9luga/en"><span>BÃ©luga</span></a></li><li id="n-Cedar" class="mw-list-item"><a href="/wiki/Cedar"><span>Cedar</span></a></li><li id="n-Graham" class="mw-list-item"><a href="/wiki/Graham"><span>Graham</span></a></li><li id="n-Narval" class="mw-list-item"><a href="/wiki/Narval/en"><span>Narval</span></a></li><li id="n-Niagara" class="mw-list-item"><a href="/wiki/Niagara"><span>Niagara</span></a></li><li id="n-sidebar-cloud" class="mw-list-item"><a href="/wiki/CC-Cloud"><span>Cloud</span></a></li><li id="n-tamIA" class="mw-list-item"><a href="/wiki/TamIA/en"><span>tamIA</span></a></li><li id="n-Killarney" class="mw-list-item"><a href="/wiki/Killarney"><span>Killarney</span></a></li><li id="n-sidebar-quantum-computing" class="mw-list-item"><a href="/wiki/Services_d%27informatique_quantique/en"><span>Quantum computing</span></a></li><li id="n-sidebar-available-software" class="mw-list-item"><a href="/wiki/Available_software"><span>Available software</span></a></li>
		</ul>
		
	</div>
</nav>

<nav id="p-sidebar-alliance" class="mw-portlet mw-portlet-sidebar-alliance vector-menu-portal portal vector-menu" aria-labelledby="p-sidebar-alliance-label"  >
	<h3
		id="p-sidebar-alliance-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">The Alliance</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-sidebar-alliance-main-page" class="mw-list-item"><a href="https://alliancecan.ca/en" rel="nofollow"><span>Alliance main page</span></a></li><li id="n-sidebar-ccdb" class="mw-list-item"><a href="https://ccdb.computecanada.ca/security/login" rel="nofollow"><span>CCDB</span></a></li><li id="n-sidebar-getting-an-account" class="mw-list-item"><a href="https://alliancecan.ca/en/services/advanced-research-computing/account-management/apply-account" rel="nofollow"><span>Getting An Account</span></a></li><li id="n-sidebar-acknowledging-alliance" class="mw-list-item"><a href="https://alliancecan.ca/en/services/advanced-research-computing/research-portal/acknowledging-alliance" rel="nofollow"><span>Acknowledging the Alliance</span></a></li><li id="n-sidebar-aup" class="mw-list-item"><a href="https://alliancecan.ca/en/services/advanced-research-computing/account-management/policies" rel="nofollow"><span>Acceptable Use Policy</span></a></li>
		</ul>
		
	</div>
</nav>

<nav id="p-sidebar-authoring" class="mw-portlet mw-portlet-sidebar-authoring vector-menu-portal portal vector-menu" aria-labelledby="p-sidebar-authoring-label"  >
	<h3
		id="p-sidebar-authoring-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Authoring</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-sidebar-guidelines" class="mw-list-item"><a href="/wiki/Authoring_guidelines"><span>Guidelines</span></a></li><li id="n-sidebar-mediawiki-help" class="mw-list-item"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/Help:Contents"><span>MediaWiki Help</span></a></li><li id="n-recentchanges" class="mw-list-item"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r"><span>Recent changes</span></a></li>
		</ul>
		
	</div>
</nav>

<nav id="p-tb" class="mw-portlet mw-portlet-tb vector-menu-portal portal vector-menu" aria-labelledby="p-tb-label"  >
	<h3
		id="p-tb-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Tools</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="t-whatlinkshere" class="mw-list-item"><a href="/wiki/Special:WhatLinksHere/PyTorch/fr" title="A list of all wiki pages that link here [j]" accesskey="j"><span>What links here</span></a></li><li id="t-recentchangeslinked" class="mw-list-item"><a href="/wiki/Special:RecentChangesLinked/PyTorch/fr" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k"><span>Related changes</span></a></li><li id="t-specialpages" class="mw-list-item"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q"><span>Special pages</span></a></li><li id="t-print" class="mw-list-item"><a href="javascript:print();" rel="alternate" title="Printable version of this page [p]" accesskey="p"><span>Printable version</span></a></li><li id="t-permalink" class="mw-list-item"><a href="/mediawiki/index.php?title=PyTorch/fr&amp;oldid=175858" title="Permanent link to this revision of this page"><span>Permanent link</span></a></li><li id="t-info" class="mw-list-item"><a href="/mediawiki/index.php?title=PyTorch/fr&amp;action=info" title="More information about this page"><span>Page information</span></a></li>
		</ul>
		
	</div>
</nav>

	
</div>

</div>

<footer id="footer" class="mw-footer" >
	<ul id="footer-info">
	<li id="footer-info-lastmod"> This page was last edited on 11 April 2025, at 09:27.</li>
</ul>

	<ul id="footer-places">
	<li id="footer-places-privacy"><a href="/wiki/CCWiki:Privacy_policy">Privacy policy</a></li>
	<li id="footer-places-about"><a href="/wiki/CCWiki:About">About Alliance Doc</a></li>
	<li id="footer-places-disclaimers"><a href="/wiki/CCWiki:General_disclaimer">Disclaimers</a></li>
	<li id="footer-places-mobileview"><a href="https://docs.alliancecan.ca/mediawiki/index.php?title=PyTorch/fr&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
</ul>

	<ul id="footer-icons" class="noprint">
	<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/" class="cdx-button cdx-button--fake-button cdx-button--size-large cdx-button--fake-button--enabled"><img src="/mediawiki/resources/assets/poweredby_mediawiki.svg" alt="Powered by MediaWiki" width="88" height="31" loading="lazy"></a></li>
</ul>

</footer>

<script>(RLQ=window.RLQ||[]).push(function(){mw.log.warn("This page is using the deprecated ResourceLoader module \"codex-search-styles\".\n[1.43] Use a CodexModule with codexComponents to set your specific components used: https://www.mediawiki.org/wiki/Codex#Using_a_limited_subset_of_components");});</script>
<script src="https://www.googletagmanager.com/gtag/js?id=G-TVBPRD78K4" async=""></script><script>
window.dataLayer = window.dataLayer || [];

function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-TVBPRD78K4', {});
</script>

<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":3969,"wgPageParseReport":{"limitreport":{"cputime":"0.319","walltime":"3.841","ppvisitednodes":{"value":1652,"limit":1000000},"postexpandincludesize":{"value":70248,"limit":2097152},"templateargumentsize":{"value":100251,"limit":2097152},"expansiondepth":{"value":7,"limit":100},"expensivefunctioncount":{"value":30,"limit":100},"unstrip-depth":{"value":2,"limit":20},"unstrip-size":{"value":310342,"limit":5000000},"timingprofile":["100.00% 3635.117      1 -total"," 85.31% 3100.989     26 Template:File"," 14.32%  520.401      5 Template:Command","  0.02%    0.565      1 Template:Note"]},"loops":{"limitreport-count-limited":{"value":0,"limit":100}},"cachereport":{"timestamp":"20250530235248","ttl":86400,"transientcontent":false}}});});</script>
</body>
</html>